{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"climate_models/","title":"Climate Models","text":"<p>Information on how to run simulations using the climate models we use in the lab.</p>"},{"location":"climate_models/cesm/","title":"CESM","text":"<p>This overview goes through how to install CESM version 2.1.3 on Archer2, before outlining the  basics of CESM and how to run a simple simulation.</p> <p>There is a tutorial  which much of this is based on, and provides a lot further information.</p>"},{"location":"climate_models/cesm/basics/","title":"Basics","text":"<p>This page just gives some useful information regarding the Community Earth System Model.</p>"},{"location":"climate_models/cesm/basics/#resources","title":"Resources","text":"<ul> <li>Discussion Forum</li> <li>Tutorial (particularly useful)</li> <li>Practical</li> <li>Analysis Example</li> <li>My code to help with analysis </li> </ul>"},{"location":"climate_models/cesm/basics/#paths-on-archer2","title":"Paths on ARCHER2","text":"<p>Paths pointing to different parts of the CESM model are given below.</p> <p>The paths that are not case specific should be available to use in ARCHER2 after loading the CESM module through <code>module load CESM2/2.1.3</code>.</p> <p></p> <ul> <li><code>$CESM_ROOT = /work/$GROUP/$GROUP/$USER/cesm/CESM2.1.3</code>  This is the overall directory containing all CESM stuff (for me, <code>$GROUP=n02</code> and <code>$USER=jamd</code>, giving: <code>$CESM_ROOT = /work/n02/n02/jamd/cesm/CESM2.1.3</code>)</li> <li><code>$CESMDATA = $CESM_ROOT/cesm_inputdata</code>Path to input data.</li> <li><code>$SRCROOT = $CESM_ROOT/my_cesm_sandbox</code>Path to CESM source code. <code>$CESM_LOC</code> will aslo refer to this directory.</li> <li><code>$CIMEROOT = $CESM_ROOT/my_cesm_sandbox/cime</code>Path to the  Common Infrastructure for Modeling Earth part of the source code.</li> <li><code>$CASEROOT = $CESM_ROOT/runs/$CASE/</code>Path to a particular experiment, as indicated by <code>$CASE</code>.</li> <li><code>$EXEROOT = $CESM_ROOT/runs/$CASE/bld/</code>Path to the build directories of a particular experiment.</li> <li><code>$RUNDIR = $CESM_ROOT/runs/$CASE/run/</code>Path to the run directories of a particular experiment.</li> <li><code>$DOUT_S_ROOT = $CESM_ROOT/archive/$CASE/</code>Path to the archive3d model output.</li> </ul>"},{"location":"climate_models/cesm/basics/#code-components","title":"Code Components","text":"<p>CESM consists of several sub models listed below.  Output data is saved in a different location for each.</p> <p></p>"},{"location":"climate_models/cesm/basics/#workflow","title":"Workflow","text":"<p>Here, I go through the general steps  for running an CESM experiment on ARCHER2.</p> <p>At any point, the file <code>$CESM_ROOT/runs/$CASE/CaseStatus</code> records commands run and whether each step has been successful.</p>"},{"location":"climate_models/cesm/basics/#step-1-login","title":"Step 1 - Login","text":"<p>First, you need to login to ARCHER2 using ssh.</p>"},{"location":"climate_models/cesm/basics/#step-2-load-modules","title":"Step 2 - Load modules","text":"<p>Each time you login to ARCHER2, you need to load the python and CESM modules:</p> <pre><code>module load cray-python\nmodule load CESM2/2.1.3\n</code></pre>"},{"location":"climate_models/cesm/basics/#step-3-create-a-case","title":"Step 3 - Create a case","text":"<p>Create a new case using: <pre><code>create_newcase --case $CESM_ROOT/runs/CASE --compset COMPSET --res RES --project PROJECT\n</code></pre></p> <p>where for me, <code>PROJECT=n02-GLOBALEX</code>.  You may need to add <code>--run-unsupported</code> to the end of this command if you get the following error: This compset and grid combination is untested in CESM.</p> Casename convection <p>Experiment casenames have a convention:</p> <pre><code>`&lt;compset char&gt;.&lt;code base&gt;.&lt;compset shortname&gt;.&lt;res shortname&gt;[.opt_desc_string].&lt;nnn&gt;[opt_char]`\n</code></pre> <p>An example <code>$CASE = e.e20.ETEST.f19_g17.test</code>.</p> Compsets <p>Compsets are listed here and the names are  explained here:</p> <p></p> Resolution <p>Resolutions are listed here and the names are  explained here:</p> <p></p>"},{"location":"climate_models/cesm/basics/#step-4-setup","title":"Step 4 - Setup","text":"<p>Navigate to <code>$CASEROOT</code> and then invoke using <code>case.setup</code>, e.g. for <code>$CASE = e.e20.ETEST.f19_g17.test</code>: <pre><code>cd $CESM_ROOT/runs/e.e20.ETEST.f19_g17.test\n./case.setup\n</code></pre></p>"},{"location":"climate_models/cesm/basics/#step-5-customize-namelists","title":"Step 5 - Customize namelists","text":"<p>At this stage, you need to specify the details of the experiment by modifying the namelists and/or  customizing the output.</p>"},{"location":"climate_models/cesm/basics/#step-6-build","title":"Step 6 - Build","text":"<p>Next, the executable should be built through <code>case.build</code>: <pre><code>./case.build\n</code></pre> Again, this should be run from <code>$CASEROOT</code>.</p>"},{"location":"climate_models/cesm/basics/#step-7-download-input-data","title":"Step 7 - Download input data","text":"<p>Next, the required input data, from which to start the model, should be downloaded: <pre><code>./check_input_data --download\n</code></pre> Again, this should be run from <code>$CASEROOT</code>.</p>"},{"location":"climate_models/cesm/basics/#step-8-run-model","title":"Step 8 - Run model","text":"<p>Finally, you can run the model with <code>case.submit</code>: <pre><code>./case.submit\n</code></pre> Again, this should be run from <code>$CASEROOT</code>. Details of the experiment may need to be changed  before submission using <code>xmlchange</code>.</p>"},{"location":"climate_models/cesm/basics/#model-output","title":"Model Output","text":"<p>If the model run is successful, the CESM netcdf output  history files are automatically moved to the short term archive, located at <code>$DOUT_S_ROOT</code>. Otherwise, they are in <code>$RUNDIR</code>.</p> <p>Output files should be moved somewhere else for more long term storage. This is likely to be JASMIN, and the  files can be transferred with globus.</p> <p>Timing information  is saved as <code>$CASEROOT/timing/cesm_timing.$CASE.$date</code>. The model throughput is the estimated number  of model years that you can run in a wallclock day.</p> <p>The <code>cpl.log</code> file at <code>$CESM_ROOT/archive/$CASE/logs</code> indicates whether successful.  It should end with  <code>SUCCESSFUL TERMINATION OF CPL7-cesm</code>.</p>"},{"location":"climate_models/cesm/basics/#xml-modifications","title":"XML Modifications","text":"<p>Some details of the experiment such as how long to run it, and on which queue to submit to, are  specified with xml variables.  These can be checked with <code>xmlquery</code> and modified with <code>xmlchange</code> from the  <code>$CASEROOT</code> directory. This should be done just before running the experiment.</p> <p>You can check  all variables containing the word <code>VAR</code> using <code>-p</code> for a partial match:</p> <pre><code>./xmlquery -p VAR\n</code></pre> <p>You can change the value of variable <code>VAR</code> to the new value of <code>new_val</code> using:</p> <pre><code>./xmlchange VAR=new_val\n</code></pre> Useful Variables <ul> <li><code>JOB_WALLCLOCK_TIME</code> - Max time to run job for. Must be less than max walltime of chosen <code>JOB_QUEUE</code>. This is listed on ARCHER2 website.</li> <li><code>STOP_N</code> - Experiment will end after <code>STOP_N</code> <code>STOP_OPTION</code>.</li> <li><code>STOP_OPTION</code> - Unit of simulation time to indicate how long to run for e.g. <code>nmonths</code>, <code>nyears</code> or <code>ndays</code>.</li> <li><code>JOB_QUEUE</code> - Which queue to submit to. Most common on ARCHER2  are <code>standard</code> or <code>short</code>.</li> <li><code>CONTINUE_RUN</code> - <code>TRUE</code> to continue run from last restart file.</li> <li><code>RESUBMIT</code> - Need to use  if experiment takes longer than max job time on partition being used.</li> </ul>"},{"location":"climate_models/cesm/basics/#restarting","title":"Restarting","text":"<p>Restart files  are written according to the <code>REST_OPTION</code> and <code>REST_N</code> settings.  By default, this is set to be the same as <code>$STOP_OPTION</code> and <code>$STOP_N</code> i.e. one restart file per run.</p> <p>The restart files are saved as <code>$DOUT_S_ROOT/rest/yyyy-mm-dd-ssss/</code>.</p> <p>To carry on running a model  from a restart file, you need to set <code>CONTINUE_RUN=TRUE</code> using <code>xmlchange</code>. By default, it is <code>FALSE</code>, in which  case the experiment would just be run from the beginning again.</p> <p>Rather than conitnuing a run from when the last job finished, if you want to  restart from a specific point,  you can move the restart file into the <code>$RUNDIR</code>.</p>"},{"location":"climate_models/cesm/basics/#namelists","title":"Namelists","text":"<p>Namelists  can be modified through the <code>user_nl_xxx</code> files in <code>$CASEROOT</code>:</p> <p></p> <p>This is where you modify details of the simulation e.g. \\(CO_2\\) concentration.</p>"},{"location":"climate_models/cesm/basics/#customizing-output","title":"Customizing Output","text":"<p>By default, the simulation will just output  the monthly average of default variables.</p> <p>Within the <code>user_nl_xxx</code> files, there are three namelist variables which allow you to  change output frequency  (<code>nhtfrq</code>) e.g. to daily average, as well as add extra variables or history files  (<code>fincl</code>).</p> <p>The <code>print_ds_var_list</code> function is quite useful  for checking which variables are in the CESM default output, and thus to decide which to output at a different  frequency.</p> Example <p>Below I go through how to run an experiment called <code>e.e20.ETEST.f19_g17.test_daily_output</code> for 40 days while outputting the daily average of the following in <code>h1</code> history files which contain 10 days each:</p> <ul> <li><code>T</code>: Temperature</li> <li><code>TS</code>: Surface temperature</li> <li><code>Q</code>: Specific humidity</li> <li><code>Z3</code>: Geopotential height</li> <li><code>LHFLX</code>: Surface latent heat flux</li> <li><code>SHFLX</code>: Surface sensible heat flux</li> <li><code>FSNS</code>: Net solar flux at the surface</li> <li><code>FLNS</code>: Net longwave flux at the surface</li> <li><code>U10</code>: 10m wind speed</li> </ul> <p>Go through up to step 5 as normal: <pre><code>$CIMEROOT/scripts/create_newcase --case $CESM_ROOT/runs/e.e20.ETEST.f19_g17.test_daily_output --compset ETEST --res f19_g17 --project n02-GLOBALEX --run-unsupported\ncd $CESM_ROOT/runs/e.e20.ETEST.f19_g17.test_daily_output\n./case.setup\n</code></pre></p> <p>Now customize <code>$CASE_ROOT/user_nl_cam</code> to include the variables: <pre><code>! Users should add all user specific namelist changes below in the form of \n! namelist_var = new_namelist_value \n\nnhtfrq = 0, -24         ! Monthly average for default h0 file, daily average for h1 file\nmfilt = 1, 10           ! 1 file per month for h0, 1 file per 10 days for h1 file\nfincl2 = 'T', 'Q', 'Z3', 'TS', 'LHFLX', 'SHFLX', 'FSNS', 'FLNS', 'U10' ! Variables to save daily in h1\n</code></pre></p> <p>Continue the rest of the pipeline as normal: <pre><code>./case.build\n./check_input_data --download\n./xmlchange STOP_N=40\n./xmlchange STOP_OPTION=ndays\n./case.submit\n</code></pre></p> <p>This produces files such as <code>e.e20.ETEST.f19_g17.test_daily_output.cam.h1.0001-01-01-00000.nc</code>  in <code>$DOUT_S_ROOT</code> with the <code>h1</code> history file indicator containing the output for daily data.</p> <p>This example only changes atmospheric variables, but you can do similar things  for the other components.</p> <p>The namelist files should be edited after setup but before build. Note that the  <code>_in</code> files only appear in <code>$CASEROOT</code> after <code>./case.build</code> and these should not be edited.</p> <p>Optionally, can run <code>./preview_namelists</code> from <code>$CASEROOT</code> after editing namelists, but this is done anyway in  <code>./case.build</code>. But if you have changed the namelists and then want to continue the same run, you can just run <code>./preview_namelists</code> followed by <code>./case.submit</code> with <code>CONTINUE_RUN=TRUE</code> to continue an experiment  with modified namelists.</p> <p>Warning</p> <p>Note that you cannot change history options (i.e. customize output) on a restart and instead must do a branch run.</p>"},{"location":"climate_models/cesm/basics/#branch-run","title":"Branch Run","text":"<p>This section describes how to do a branch run.  This will take an experiment you have already run at a particular point  in time as a starting condition, then modify the experiment somehow e.g. change \\(CO_2\\) concentration, and continue the run.</p> <p>The workflow for this is exactly the same as you carried out for the experiment you want to branch off  from up until Step 5, except the case name should be different. E.g. if the initial experiment was called <code>e.e20.E1850TEST.f09_g17.test</code>, the branched experiment may be called <code>e.e20.E1850TEST.f09_g17.branch</code>.</p> <p>Step 5 is where you make this experiment different from the experiment you have already run, e.g. change \\(CO_2\\) concentration through the variable <code>co2vmr</code> or <code>co2_vmr_rad</code> in the <code>user_nml_cam</code> namelist file.</p> <p>Next, you should build the executable as usual, but you can skip downloading the input data because you are starting from a restart file, not input data.</p>"},{"location":"climate_models/cesm/basics/#branch-point","title":"Branch Point","text":"<p>At this stage, you need to  specify the branch point  by moving the relevant restart files into <code>$RUNDIR</code>.</p> <p>To restart from the date <code>yyyy-mm-dd-ssss</code>, all the files in the directory <code>$DOUT_S_ROOT/rest/yyyy-mm-dd-ssss/</code> of the initial experiment should be moved to <code>$RUNDIR</code> of the new experiment</p> Example <p>If I want to create a branch called <code>e.e20.E1850TEST.f09_g17.branch</code> from 1st January Year 11 of experiment called <code>e.e20.E1850TEST.f09_g17.test</code>, then I would move all the files in the directory  <code>$CESM_ROOT/archive/e.e20.E1850TEST.f09_g17.test/rest/0011-01-01-00000/</code> to <code>$RUNDIR = $CESM_ROOT/runs/e.e20.E1850TEST.f09_g17.branch/run/</code>.</p> <p>The <code>$RUNDIR</code> before and after this transfer is shown below. Afterwards, there are .nc and rcpointer files in the <code>$RUNDIR</code>.</p> BeforeAfter <p></p> <p></p> <p>Once the restart files have been transferred, <code>xmlchange</code> must be used to indicate that this experiment is a branch run. If we are branching off from an experiment with casename <code>old_case</code> at the date <code>yyyy-mm-dd</code>, then you should run:</p> <pre><code>./xmlchange RUN_TYPE=branch\n./xmlchange RUN_REFCASE=old_case\n./xmlchange RUN_REFDATE=yyyy-mm-dd\n./xmlchange GET_REFCASE=FALSE\n</code></pre> Example <p>Continuing from our previous example, you would run:</p> <pre><code>./xmlchange RUN_TYPE=branch\n./xmlchange RUN_REFCASE=e.e20.E1850TEST.f09_g17.test\n./xmlchange RUN_REFDATE=0011-01-01\n./xmlchange GET_REFCASE=FALSE\n</code></pre> <p>After this, the branch job can be submitted as normal, remembering to  specify the run duration etc.</p>"},{"location":"climate_models/cesm/first_run/","title":"First Run","text":"<p>Here I go through a step by step example of running a slab ocean experiment, with present day initialization. I  use the <code>ETEST</code> compset with <code>f19_g17</code> resolution.</p> Details of <code>ETEST</code> and <code>f19_g17</code> ETESTf19_g17 <p></p> <p></p>"},{"location":"climate_models/cesm/first_run/#step-1-login","title":"Step 1 - Login","text":"<p>After logging in to ARCHER2, you should get a welcome message.</p> Terminal Output <pre><code>Last login: Tue Nov  5 15:59:18 2024 from 2.98.194.178\n#######################################################################################\n\n        @@@@@@@@@\n     @@@         @@@            _      ____     ____   _   _   _____   ____    ____\n   @@@    @@@@@    @@@         / \\    |  _ \\   / ___| | | | | | ____| |  _ \\  |___ \\\n  @@@   @@     @@   @@@       / _ \\   | |_) | | |     | |_| | |  _|   | |_) |   __) |\n  @@   @@  @@@  @@   @@      / ___ \\  |  _ &lt;  | |___  |  _  | | |___  |  _ &lt;   / __/\n  @@   @@  @@@  @@   @@     /_/   \\_\\ |_| \\_\\  \\____| |_| |_| |_____| |_| \\_\\ |_____|\n  @@@   @@     @@   @@@\n   @@@    @@@@@    @@@       https://www.archer2.ac.uk/support-access/\n     @@@         @@@\n        @@@@@@@@@\n\n -         U K R I         -        E P C C        -         H P E   C r a y         -\n\nHostname:     ln02\nDistribution: SLES 15.4 4\nCPUS:         256\nMemory:       515.3GB\nConfigured:   2024-07-04\n\n######################################################################################\n---------------------------------Welcome to ARCHER2-----------------------------------\n######################################################################################\n\n/usr/bin/manpath: can't set the locale; make sure $LC_* and $LANG are correct\n</code></pre>"},{"location":"climate_models/cesm/first_run/#step-2-load-modules","title":"Step 2 - Load modules","text":"<p>After successfully loading the modules, there are a few messages printed to terminal.</p> Terminal Output <pre><code>jamd@ln02:~&gt; module load cray-python\njamd@ln02:~&gt; module load CESM2/2.1.3\n\nLmod is automatically replacing \"cce/15.0.0\" with \"gcc/11.2.0\".\n\n\nLmod is automatically replacing \"PrgEnv-cray/8.3.3\" with \"PrgEnv-gnu/8.3.3\".\n\n\nDue to MODULEPATH changes, the following have been reloaded:\n  1) cray-mpich/8.1.23\n</code></pre> <p>After this, I make <code>$CESM_ROOT</code> the current directory:</p> <pre><code>jamd@ln02:~&gt; cd $CESM_ROOT\njamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3&gt; \n</code></pre>"},{"location":"climate_models/cesm/first_run/#step-3-create-a-case","title":"Step 3 - Create a case","text":"<p>To create the case with <code>ETEST</code> compset and <code>f19_g17</code> resolution, I run:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3&gt; $CIMEROOT/scripts/create_newcase --case $CESM_ROOT/runs/e.e20.ETEST.f19_g17.test --compset ETEST --res f19_g17 --project n02-GLOBALEX --run-unsupported\n</code></pre> <p>where I have used the case name of <code>e.e20.ETEST.f19_g17.test</code> following the  convection with the descriptive string <code>test</code>.</p> Terminal Output <p>Following this command, a bunch of stuff will be printed to terminal, the last of which should be: <pre><code>Pes comments: none\n Compset is: 2000_CAM60_CLM50%SP_CICE_DOCN%SOM_MOSART_SGLC_SWAV_TEST \n Grid is: a%1.9x2.5_l%1.9x2.5_oi%gx1v7_r%r05_g%null_w%null_m%gx1v7 \n Components in compset are: ['cam', 'clm', 'cice', 'docn', 'mosart', 'sglc', 'swav', 'sesp', 'drv', 'dart'] \nNo charge_account info available, using value from PROJECT\nNo project info available\ncesm model version found: cesm2.1.3-rc.01\nBatch_system_type is slurm\njob is case.run USER_REQUESTED_WALLTIME None USER_REQUESTED_QUEUE None WALLTIME_FORMAT %H:%M:%S\njob is case.st_archive USER_REQUESTED_WALLTIME None USER_REQUESTED_QUEUE None WALLTIME_FORMAT %H:%M:%S\n Creating Case directory /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test\n</code></pre></p>"},{"location":"climate_models/cesm/first_run/#step-4-setup","title":"Step 4 - Setup","text":"<p>Now the experiment has been created, we need to make <code>$CESM_ROOT/runs/e.e20.ETEST.f19_g17.test</code> the current directory, before running <code>case.setup</code>:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3&gt; cd $CESM_ROOT/runs/e.e20.ETEST.f19_g17.test\njamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./case.setup\n</code></pre> Terminal Output <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./case.setup\nSetting resource.RLIMIT_STACK to -1 from (8388608, -1)\n/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/env_mach_specific.xml already exists, delete to replace\njob is case.run USER_REQUESTED_WALLTIME None USER_REQUESTED_QUEUE None WALLTIME_FORMAT %H:%M:%S\nCreating batch scripts\nWriting case.run script from input template /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/cime/config/cesm/machines/template.case.run\nCreating file .case.run\nWriting case.st_archive script from input template /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/cime/config/cesm/machines/template.st_archive\nCreating file case.st_archive\nCreating user_nl_xxx files for components and cpl\nIf an old case build already exists, might want to run 'case.build --clean' before building\nYou can now run './preview_run' to get more info on how your case will be run\n</code></pre>"},{"location":"climate_models/cesm/first_run/#step-5-customize-namelists","title":"Step 5 - Customize namelists","text":"<p>For this example, we keep the default experiment parameters so don't need to  change the namelist files.</p>"},{"location":"climate_models/cesm/first_run/#step-6-build","title":"Step 6 - Build","text":"<p>Now we build the executable:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./case.build\n</code></pre> <p>This step takes a while (about 5 minutes for this example), but should end with the following message:</p> <pre><code>Time spent not building: 6.407066 sec\nTime spent building: 253.041738 sec\nMODEL BUILD HAS FINISHED SUCCESSFULLY\n</code></pre> Terminal Output <p>A lot is printed to terminal at this stage, the last of which is shown below.</p> <pre><code>Building atm with output to /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/bld/atm.bldlog.241105-175344\nBuilding ice with output to /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/bld/ice.bldlog.241105-175344\nBuilding ocn with output to /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/bld/ocn.bldlog.241105-175344\nBuilding rof with output to /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/bld/rof.bldlog.241105-175344\nBuilding glc with output to /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/bld/glc.bldlog.241105-175344\nBuilding wav with output to /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/bld/wav.bldlog.241105-175344\nBuilding esp with output to /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/bld/esp.bldlog.241105-175344\nsesp built in 4.198223 seconds\nsglc built in 4.202698 seconds\nswav built in 4.218285 seconds\ndocn built in 5.041561 seconds\nComponent rof build complete with 4 warnings\nmosart built in 10.101223 seconds\nComponent ice build complete with 10 warnings\ncice built in 25.621810 seconds\nComponent atm build complete with 109 warnings\ncam built in 101.564669 seconds\nBuilding cesm with output to /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/bld/cesm.bldlog.241105-175344 \nTime spent not building: 6.407066 sec\nTime spent building: 253.041738 sec\nMODEL BUILD HAS FINISHED SUCCESSFULLY\n</code></pre>"},{"location":"climate_models/cesm/first_run/#step-7-download-input-data","title":"Step 7 - Download Input data","text":"<p>The input data is downloaded with the command:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./check_input_data --download\n</code></pre> Terminal Output <p>In this step, you will get a lot of messages of the form  <code>Model missing file...Trying to download file...using WGET protocol...SUCCESS</code>. </p> <p>An few examples are given below:</p> <pre><code>  Model cam missing file srf_emis_specifier for SOAG = '/work/n02/n02/jamd/cesm/CESM2.1.3/cesm_inputdata/atm/cam/chem/emis/CMIP6_emissions_2000climo/emissions-cmip6_SOAGx1.5_anthro_surface_2000climo_0.9x1.25_c20170608.nc'\nTrying to download file: 'atm/cam/chem/emis/CMIP6_emissions_2000climo/emissions-cmip6_SOAGx1.5_anthro_surface_2000climo_0.9x1.25_c20170608.nc' to path '/work/n02/n02/jamd/cesm/CESM2.1.3/cesm_inputdata/atm/cam/chem/emis/CMIP6_emissions_2000climo/emissions-cmip6_SOAGx1.5_anthro_surface_2000climo_0.9x1.25_c20170608.nc' using WGET protocol.\nSUCCESS\n\n  Model cam missing file srf_emis_specifier for SOAG = '/work/n02/n02/jamd/cesm/CESM2.1.3/cesm_inputdata/atm/cam/chem/emis/CMIP6_emissions_2000climo/emissions-cmip6_SOAGx1.5_bb_surface_2000climo_0.9x1.25_c20170322.nc'\nTrying to download file: 'atm/cam/chem/emis/CMIP6_emissions_2000climo/emissions-cmip6_SOAGx1.5_bb_surface_2000climo_0.9x1.25_c20170322.nc' to path '/work/n02/n02/jamd/cesm/CESM2.1.3/cesm_inputdata/atm/cam/chem/emis/CMIP6_emissions_2000climo/emissions-cmip6_SOAGx1.5_bb_surface_2000climo_0.9x1.25_c20170322.nc' using WGET protocol.\nSUCCESS\n\n  Model cam missing file srf_emis_specifier for SOAG = '/work/n02/n02/jamd/cesm/CESM2.1.3/cesm_inputdata/atm/cam/chem/emis/CMIP6_emissions_2000climo/emissions-cmip6_SOAGx1.5_biogenic_surface_2000climo_0.9x1.25_c20170322.nc'\nTrying to download file: 'atm/cam/chem/emis/CMIP6_emissions_2000climo/emissions-cmip6_SOAGx1.5_biogenic_surface_2000climo_0.9x1.25_c20170322.nc' to path '/work/n02/n02/jamd/cesm/CESM2.1.3/cesm_inputdata/atm/cam/chem/emis/CMIP6_emissions_2000climo/emissions-cmip6_SOAGx1.5_biogenic_surface_2000climo_0.9x1.25_c20170322.nc' using WGET protocol.\nSUCCESS\n</code></pre> <p>This stage will take a while if no input data already exists, on the order of 30 minutes for 1 degree resolution. If the input data was downloaded successfully, running <code>./check_input_data</code> should show the following:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./check_input_data           \nSetting resource.RLIMIT_STACK to -1 from (8388608, -1)\nLoading input file list: 'Buildconf/cice.input_data_list'\nLoading input file list: 'Buildconf/mosart.input_data_list'\nLoading input file list: 'Buildconf/docn.input_data_list'\nLoading input file list: 'Buildconf/cpl.input_data_list'\nLoading input file list: 'Buildconf/clm.input_data_list'\nLoading input file list: 'Buildconf/cam.input_data_list'\n</code></pre>"},{"location":"climate_models/cesm/first_run/#step-8-run-model","title":"Step 8 - Run model","text":"<p>Before running the model, you need change  the duration of the simulation (default is 5 days),  and specify which partition to submit the job to (default is <code>standard</code>).  Here, I change it to 1 month on the <code>short</code> partition which has a max walltime of 20 minutes:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./xmlchange JOB_QUEUE=short      \njamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./xmlchange JOB_WALLCLOCK_TIME=20:00   \njamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./xmlchange STOP_N=1       \njamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./xmlchange STOP_OPTION=nmonths\n</code></pre> Checking using <code>xmlquery</code> <p>After the above changes, you can use <code>./xmlquery -p JOB</code> to check the <code>JOB_QUEUE</code> and <code>JOB_WALLCLOCK_TIME</code> have  changed: <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./xmlquery -p JOB\n\nResults in group case.run\n        JOB_QUEUE: short\n        JOB_WALLCLOCK_TIME: 20:00\n\nResults in group case.st_archive\n        JOB_QUEUE: short\n        JOB_WALLCLOCK_TIME: 20:00\n\nResults in group run_begin_stop_restart\n        JOB_IDS: \n        JOB_PRIORITY: regular\n</code></pre></p> <p>and <code>./xmlquery -p STOP</code> to check <code>STOP_N</code> and <code>STOP_OPTION</code>:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./xmlquery -p STOP\n\nResults in group run_begin_stop_restart\n        STOP_DATE: -999\n        STOP_N: 1\n        STOP_OPTION: nmonths\n</code></pre> <p>Now we can run the job:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./case.submit \n</code></pre> <p>If successful, you should get information printed to terminal about the job id:</p> <pre><code>Submitted job id is 7977850\nSubmitted job case.run with id 7977849\nSubmitted job case.st_archive with id 7977850\n</code></pre> Terminal Output <p>The full terminal output is:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; ./case.submit \nSetting resource.RLIMIT_STACK to -1 from (8388608, -1)\nCreating component namelists\n   Calling /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/components/cam//cime_config/buildnml\nCAM namelist copy: file1 /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/Buildconf/camconf/atm_in file2 /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/run/atm_in \n   Calling /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/components/clm//cime_config/buildnml\n   Calling /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/components/cice//cime_config/buildnml\n   Calling /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/cime/src/components/data_comps/docn/cime_config/buildnml\n   Calling /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/components/mosart//cime_config/buildnml\n   Calling /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/cime/src/components/stub_comps/sglc/cime_config/buildnml\n   Calling /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/cime/src/components/stub_comps/swav/cime_config/buildnml\n   Calling /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/cime/src/components/stub_comps/sesp/cime_config/buildnml\n   Calling /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/cime/src/drivers/mct/cime_config/buildnml\nFinished creating component namelists\nChecking that inputdata is available as part of case submission\nSetting resource.RLIMIT_STACK to -1 from (-1, -1)\nLoading input file list: 'Buildconf/cice.input_data_list'\nLoading input file list: 'Buildconf/mosart.input_data_list'\nLoading input file list: 'Buildconf/docn.input_data_list'\nLoading input file list: 'Buildconf/cpl.input_data_list'\nLoading input file list: 'Buildconf/clm.input_data_list'\nLoading input file list: 'Buildconf/cam.input_data_list'\nCheck case OK\nsubmit_jobs case.run\nSubmit job case.run\nSubmitting job script sbatch --time 20:00 -q short --account n02-GLOBALEX --export=ALL /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/.case.run --resubmit\nSubmitted job id is 7977849\nSubmit job case.st_archive\nSubmitting job script sbatch --time 20:00 -q short --account n02-GLOBALEX --export=ALL  --dependency=afterok:7977849 /mnt/lustre/a2fs-work2/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/case.st_archive --resubmit\nSubmitted job id is 7977850\nSubmitted job case.run with id 7977849\nSubmitted job case.st_archive with id 7977850\n</code></pre> <p>The progress of the job can be monitored and managed with the usual slurm commands:</p> <pre><code>jamd@ln02:/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test&gt; squeue -u jamd\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           7977850  standard st_archi     jamd PD       0:00      1 (Dependency)\n           7977849  standard run.e.e2     jamd  R       1:52      6 nid[006831,006833,006836,006844-006845,006849]\n</code></pre>"},{"location":"climate_models/cesm/first_run/#model-output","title":"Model Output","text":"<p>The model output should be located in  <code>$DOUT_S_ROOT = /work/n02/n02/jamd/cesm/CESM2.1.3/archive/e.e20.ETEST.f19_g17.test/</code> if successful with files  corresponding to each of the model components, as well as restart and log files:</p> <p></p>"},{"location":"climate_models/cesm/first_run/#timing","title":"Timing","text":"<p>Timing information can be found at <code>$CASEROOT/timing/cesm_timing.$CASE.$date</code>. For this experiment,  the file is: </p> <p><code>/work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/timing/cesm_timing.e.e20.ETEST.f19_g17.test.7977849.241105-182147</code></p> Timing File <p>The timing file is quite long, the first section is shown below.  The most useful information is in the Overall Metrics section.</p> <pre><code>---------------- TIMING PROFILE ---------------------\n  Case        : e.e20.ETEST.f19_g17.test\n  LID         : 7977849.241105-182147\n  Machine     : archer2\n  Caseroot    : /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test\n  Timeroot    : /work/n02/n02/jamd/cesm/CESM2.1.3/runs/e.e20.ETEST.f19_g17.test/Tools\n  User        : jamd\n  Curr Date   : Tue Nov  5 18:31:13 2024\n  grid        : a%1.9x2.5_l%1.9x2.5_oi%gx1v7_r%r05_g%null_w%null_m%gx1v7\n  compset     : 2000_CAM60_CLM50%SP_CICE_DOCN%SOM_MOSART_SGLC_SWAV_TEST\n  run_type    : startup, continue_run = FALSE (inittype = TRUE)\n  stop_option : nmonths, stop_n = 1\n  run_length  : 31 days (30.979166666666668 for ocean)\n\n  component       comp_pes    root_pe   tasks  x threads instances (stride) \n  ---------        ------     -------   ------   ------  ---------  ------  \n  cpl = cpl        512         0        512    x 1       1      (1     ) \n  atm = cam        512         0        512    x 1       1      (1     ) \n  lnd = clm        256         0        256    x 1       1      (1     ) \n  ice = cice       256         256      256    x 1       1      (1     ) \n  ocn = docn       256         512      256    x 1       1      (1     ) \n  rof = mosart     256         0        256    x 1       1      (1     ) \n  glc = sglc       128         0        128    x 1       1      (1     ) \n  wav = swav       128         0        128    x 1       1      (1     ) \n  esp = sesp       1           0        1      x 1       1      (1     ) \n\n  total pes active           : 768 \n  mpi tasks per node               : 128 \n  pe count for cost estimate : 768 \n\n  Overall Metrics: \n    Model Cost:             798.38   pe-hrs/simulated_year \n    Model Throughput:        23.09   simulated_years/day \n\n    Init Time   :     234.999 seconds \n    Run Time    :     317.848 seconds       10.253 seconds/day \n    Final Time  :       0.004 seconds \n\n    Actual Ocn Init Wait Time     :     308.436 seconds \n    Estimated Ocn Init Run Time   :       0.001 seconds \n    Estimated Run Time Correction :       0.000 seconds \n      (This correction has been applied to the ocean and total run times)\n</code></pre>"},{"location":"climate_models/cesm/first_run/#log","title":"Log","text":"<p>The <code>cpl.log</code> file can be found at <code>$CESM_ROOT/archive/$CASE/logs</code>. For this example, the path is:</p> <p><code>/work/n02/n02/jamd/cesm/CESM2.1.3/archive/e.e20.ETEST.f19_g17.test/logs/cpl.log.7977849.241105-182147</code></p> <p>This should end with <code>SUCCESSFUL TERMINATION OF CPL7-cesm.</code> if the simulation ran correctly.</p> Log File <p>The file is very long, but the last SUCCESSFUL part is given below for this experiment:</p> <pre><code>(seq_mct_drv): ===============          SUCCESSFUL TERMINATION OF CPL7-cesm ===============\n(seq_mct_drv): ===============        at YMD,TOD =   00010201       0         ===============\n(seq_mct_drv): ===============  # simulated days (this run) =       31.000  ===============\n(seq_mct_drv): ===============  compute time (hrs)          =        0.088  ===============\n(seq_mct_drv): ===============  # simulated years / cmp-day =       23.087  ===============\n(seq_mct_drv): ===============  pes min memory highwater  (MB)     890.039  ===============\n(seq_mct_drv): ===============  pes max memory highwater  (MB)    7045.794  ===============\n(seq_mct_drv): ===============  pes min memory last usage (MB)      62.108  ===============\n(seq_mct_drv): ===============  pes max memory last usage (MB)     660.740  ===============\n</code></pre>"},{"location":"climate_models/cesm/first_run/#first-plot","title":"First Plot","text":"<p>After transferring the output data in <code>$DOUT_S_ROOT</code> to JASMIN, the <code>load_dataset</code> function can be used to load the dataset, and  the <code>print_ds_var_list</code> function to find out  what variables are contained in the output data.</p> <p>Below, I load the atmospheric dataset, find the temperature-related variables in the Dataset and plot the  surface temperature averaged over the first month.</p> CodeOutputPlot <pre><code>import sys\nsys.path.append('/home/users/$USER/')\nimport climdyn_tools\n# var_keep = ['TS', 'FSNT', 'FLNT', 'gw', 'LHFLX', 'SHFLX', 'FLNS', 'FSNS', 'PRECSC', 'PRECSL']\nexp_name = 'e.e20.ETEST.f19_g17.test'\nds = climdyn_tools.cesm.load_dataset(exp_name, decode_times=False).load()\nclimdyn_tools.utils.print_ds_var_list(ds, 'temp')\nds.TS.plot()\n</code></pre> <pre><code>RTPTHLP_CLUBB: Temp. Moist. Covariance\nSTEND_CLUBB: Temperature tendency\nT: Temperature\nTHLP2_CLUBB: Temperature Variance\nTREFHT: Reference height temperature\nTS: Surface temperature (radiative)\nTSMN: Minimum surface temperature over output period\nTSMX: Maximum surface temperature over output period\n</code></pre> <p></p> <p>Note that for this to work, the <code>climdyn_tools</code> directory has the path <code>/home/users/$USER/climdyn_tools</code> on JASMIN.</p>"},{"location":"climate_models/cesm/installation/","title":"Installation on Archer2","text":"<p>Here, I go through the procedure for installing CESM version 2.1.3 on ARCHER2. These are mainly based on the instructions on the ARCHER2 website  but correct them, so it actually works.</p>"},{"location":"climate_models/cesm/installation/#step-1-archer2-account","title":"Step 1 - ARCHER2 Account","text":"<p>First, you need to setup an ARCHER2 account and  then connect using ssh.</p>"},{"location":"climate_models/cesm/installation/#step-2-downloading-cesm-213-and-setting-up-the-directory-structure","title":"Step 2 - Downloading CESM 2.1.3 And Setting Up The Directory Structure","text":"<p>This step is the same as on the ARCHER2 website, i.e. you need to run:</p> <pre><code>module load cray-python\nsource /work/n02/shared/CESM2/setup_cesm213.sh\n</code></pre> <p>This script will create a directory at <code>$CESM_ROOT</code>, defaulting to <code>/work/$GROUP/$GROUP/$USER/cesm/CESM2.1.3</code>, so for me  with <code>$GROUP=n02</code>, and <code>$USER=jamd</code>, <code>$CESM_ROOT = /work/n02/n02/jamd/cesm/CESM2.1.3/</code>. </p> <p>Note <code>$CESM_LOC = $CESM_ROOT/my_cesm_sandbox</code> in what follows.</p>"},{"location":"climate_models/cesm/installation/#step-3-changes-to-externals-configuration","title":"Step 3 - Changes to Externals Configuration","text":"<p>A couple of sections in the file <code>$CESM_ROOT/my_cesm_sandbox/Externals.cfg</code> need modifying.  First, change the CIME section from:</p> Editing files on ARCHER2 <p>You can open or edit a file using the nano.</p> <ul> <li><code>nano index.html</code> opens the <code>index.html</code> file, which can then be edited. </li> <li><code>Ctrl + O</code> to save the file, and confirm with <code>Enter</code></li> <li><code>Ctrl + X</code> to close <code>nano</code></li> </ul> <p>Alternatively, globus can be used to transfer the file to your local laptop, where you can make the necessary edits, before sending it back to overwrite the file on ARCHER2.</p> <pre><code>[cime]\ntag = cime5.6.32\nprotocol = git\nrepo_url = https://github.com/ESMCI/cime\nlocal_path = cime\nrequired = True\n</code></pre> <p>to (note when I did this, I did not include the <code>externals = Externals_cime.cfg</code> addition):</p> <pre><code>[cime]\nbranch = maint-5.6\nprotocol = git\nrepo_url = https://github.com/ESMCI/cime\nlocal_path = cime\nexternals = Externals_cime.cfg\nrequired = True\n</code></pre> <p>The CAM section in the same file also needs changing. This is the step that was missing from the ARCHER2 website,  but I think it is there  now. This needs changing from:</p> <pre><code>[cam]\ntag = cam_cesm2_1_rel_41\nprotocol = git\nrepo_url = https://github.com/ESCOMP/CAM\nlocal_path = components/cam\nexternals = Externals_CAM.cfg\nrequired = True\n</code></pre> <p>to:</p> <pre><code>[cam]\ntag = cam_cesm2_1_rel\nprotocol = git\nrepo_url = https://github.com/ESCOMP/CAM\nlocal_path = components/cam\nexternals = Externals_CAM.cfg\nrequired = True\n</code></pre>"},{"location":"climate_models/cesm/installation/#step-4-downloading-components","title":"Step 4 - Downloading Components","text":"<p>Now you can download the external components by running the following commands:</p> <pre><code>cd $CESM_ROOT\n./manage_externals/checkout_externals\n</code></pre> Confirming successful download <p>To confirm a successful download of all components, you can run checkout_externals with the status flag to show the  status of the externals:</p> <pre><code>./manage_externals/checkout_externals -S\n</code></pre> <p>This should show a clean status for all externals, with no characters in the first two columns of output,  as in this example:</p> <p></p> <p>This comes from the CESM website  (but the screenshot is from my installation).</p>"},{"location":"climate_models/cesm/installation/#step-5-building-cprnc","title":"Step 5 - Building cprnc","text":"<p>cprnc is a generic tool for analyzing a netcdf file or comparing two netcdf files. It is used in various places by CESM  and the source is included with cime. My procedure for building it differs slightly from that on the  ARCHER2 website.</p> <p>First, load <code>CESM2/2.1.3</code> and navigate to <code>$CIMEROOT/tools/cprnc</code>:</p> <pre><code>module load CESM2/2.1.3\ncd $CIMEROOT/tools/\n</code></pre> <p>where <code>$CIMEROOT = $CESM_ROOT/my_cesm_sandbox/cime</code>.  For me, I am in the directory <code>/work/n02/n02/jamd/cesm/CESM2.1.3/my_cesm_sandbox/cime/tools</code> at this stage.</p> <p>If the directory <code>$CIMEROOT/tools/cprnc</code> does not exist (it did not for me) then create and enter it:</p> <pre><code>cd $CIMEROOT/tools/\nmkdir cprnc\ncd ./cprnc\n</code></pre> <p>From the <code>$CIMEROOT/tools/cprnc</code> directory, run the following three commands (one after the other):</p> <pre><code>../configure --macros-format=Makefile--mpilib=mpi-serial\nsed -i '/}}/d' .env_mach_specific.sh\nsource ./.env_mach_specific.sh &amp;&amp; make\n</code></pre> <p>I received the following message after the last of these commands, which is expected:</p> <pre><code>The following dependent module(s) are not currently loaded: cray-hdf5-parallel (required by: CESM2/2.1.3), cray-netcdf-hdf5parallel (required by: CESM2/2.1.3), cray-parallel-netodf (required by: CESM2/2.1.3)\n</code></pre> <p>Note that this step is not essential, so you can proceed to the next step if you encountered some issues here.</p>"},{"location":"climate_models/cesm/installation/#step-6-changing-input-data-configuration","title":"Step 6 - Changing Input Data Configuration","text":"<p>When I first installed CESM on ARCHER2, I kept getting errors when trying to  download input data. These errors arose due to a problem with globus.</p> <p>I managed to fix it by changing the order so the <code>wget</code> protocol is used instead. This is done by modifying the  <code>$CIMEROOT/config/cesm/config_inputdata.xml</code> file as indicated below:</p> BeforeAfter <pre><code>&lt;?xml version=\"1.0\"?&gt;\n\n&lt;inputdata&gt;\n  &lt;!-- server precidence is order in this file.  Highest preference at top --&gt;\n  &lt;!-- If the client doesn't have the protocol it will be skipped --&gt;\n  &lt;!-- chksum verification of inputfiles is possible.  If a file with name --&gt;\n  &lt;!-- inputdata_chksum.dat is found on the server in the directory above inputdata --&gt;\n  &lt;!-- it will be searched for filename and chksum of each downloaded file.  --&gt;\n  &lt;!-- see the file ftp://ftp.cgd.ucar.edu/cesm/inputdata_chksum.dat for proper format. --&gt;\n  &lt;server&gt;\n    &lt;comment&gt;grid ftp requires the globus-url-copy tool on the client side &lt;/comment&gt;\n    &lt;protocol&gt;gftp&lt;/protocol&gt;\n    &lt;address&gt;ftp://gridanon.cgd.ucar.edu:2811/cesm/inputdata/&lt;/address&gt;\n    &lt;checksum&gt;../inputdata_checksum.dat&lt;/checksum&gt;\n  &lt;/server&gt;\n\n  &lt;server&gt;\n    &lt;protocol&gt;wget&lt;/protocol&gt;\n    &lt;address&gt;ftp://ftp.cgd.ucar.edu/cesm/inputdata/&lt;/address&gt;\n    &lt;user&gt;anonymous&lt;/user&gt;\n    &lt;password&gt;user@example.edu&lt;/password&gt;\n    &lt;checksum&gt;../inputdata_checksum.dat&lt;/checksum&gt;\n  &lt;/server&gt;\n\n  &lt;server&gt;\n    &lt;comment&gt; ftp requires the python package ftplib &lt;/comment&gt;\n    &lt;protocol&gt;ftp&lt;/protocol&gt;\n    &lt;address&gt;ftp.cgd.ucar.edu/cesm/inputdata&lt;/address&gt;\n    &lt;user&gt;anonymous&lt;/user&gt;\n    &lt;password&gt;user@example.edu&lt;/password&gt;\n    &lt;checksum&gt;../inputdata_checksum.dat&lt;/checksum&gt;\n  &lt;/server&gt;\n\n  &lt;server&gt;\n    &lt;protocol&gt;svn&lt;/protocol&gt;\n    &lt;address&gt;https://svn-ccsm-inputdata.cgd.ucar.edu/trunk/inputdata&lt;/address&gt;\n  &lt;/server&gt;\n\n&lt;/inputdata&gt;\n</code></pre> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n\n&lt;inputdata&gt;\n  &lt;!-- server precidence is order in this file.  Highest preference at top --&gt;\n  &lt;!-- If the client doesn't have the protocol it will be skipped --&gt;\n  &lt;!-- chksum verification of inputfiles is possible.  If a file with name --&gt;\n  &lt;!-- inputdata_chksum.dat is found on the server in the directory above inputdata --&gt;\n  &lt;!-- it will be searched for filename and chksum of each downloaded file.  --&gt;\n  &lt;!-- see the file ftp://ftp.cgd.ucar.edu/cesm/inputdata_chksum.dat for proper format. --&gt;\n  &lt;server&gt;\n    &lt;protocol&gt;wget&lt;/protocol&gt;\n    &lt;address&gt;ftp://ftp.cgd.ucar.edu/cesm/inputdata/&lt;/address&gt;\n    &lt;user&gt;anonymous&lt;/user&gt;\n    &lt;password&gt;user@example.edu&lt;/password&gt;\n    &lt;checksum&gt;../inputdata_checksum.dat&lt;/checksum&gt;\n  &lt;/server&gt;\n\n  &lt;server&gt;\n    &lt;comment&gt; ftp requires the python package ftplib &lt;/comment&gt;\n    &lt;protocol&gt;ftp&lt;/protocol&gt;\n    &lt;address&gt;ftp.cgd.ucar.edu/cesm/inputdata&lt;/address&gt;\n    &lt;user&gt;anonymous&lt;/user&gt;\n    &lt;password&gt;user@example.edu&lt;/password&gt;\n    &lt;checksum&gt;../inputdata_checksum.dat&lt;/checksum&gt;\n  &lt;/server&gt;\n\n  &lt;server&gt;\n    &lt;protocol&gt;svn&lt;/protocol&gt;\n    &lt;address&gt;https://svn-ccsm-inputdata.cgd.ucar.edu/trunk/inputdata&lt;/address&gt;\n  &lt;/server&gt;\n\n  &lt;server&gt;\n    &lt;comment&gt;grid ftp requires the globus-url-copy tool on the client side &lt;/comment&gt;\n    &lt;protocol&gt;gftp&lt;/protocol&gt;\n    &lt;address&gt;ftp://gridanon.cgd.ucar.edu:2811/cesm/inputdata/&lt;/address&gt;\n    &lt;checksum&gt;../inputdata_checksum.dat&lt;/checksum&gt;\n  &lt;/server&gt;\n\n&lt;/inputdata&gt;\n</code></pre> <p>Once this step has been completed, you are ready to run a simple test case.</p>"},{"location":"climate_models/isca/","title":"Isca","text":"<p>The Isca climate model is an idealized model which is easily customizable and quick to run.</p>"},{"location":"climate_models/isca/#resources","title":"Resources","text":"<ul> <li>Running the Isca climate model on hypatia \ud83d\udd17</li> <li>Official documentation \ud83d\udd17</li> </ul>"},{"location":"code/","title":"Climate Dynamics Tools","text":"<p>This is a python package containing useful code for  using, obtaining and analysing climate data.</p>"},{"location":"code/ceda_esgf/base/","title":"Base","text":""},{"location":"code/ceda_esgf/base/#climdyn_tools.ceda_esgf.base.checkCEDA","title":"<code>checkCEDA(source_id, activity_id, experiment_id, M2I, variableList, table_id, member_id='*')</code>","text":"<p>For a given source_id (model), check which variables have data on CEDA, and return a DataFrame similar to intake-esm format, combining paths for identical metadata.</p> <p>Parameters:</p> Name Type Description Default <code>source_id</code> <code>str</code> <p>source_id (e.g. \"MPI-ESM1-2-HR\")</p> required <code>activity_id</code> <code>str</code> <p>CMIP6 activity (e.g. \"CMIP\", \"ScenarioMIP\")</p> required <code>experiment_id</code> <code>str</code> <p>Experiment (e.g. \"historical\", \"ssp585\")</p> required <code>M2I</code> <code>dict</code> <p>Mapping source_id -&gt; institution_id (from getModel_to_inst)</p> required <code>variableList</code> <code>List[str]</code> <p>Variables to check (e.g. <code>['tas','rlut']</code>)</p> required <code>table_id</code> <code>str</code> <p>Corresponding CMIP6 table_id e.g. 'Amon'</p> required <code>member_id</code> <code>str</code> <p>The member Id specifically looked for - can be a wild car when searching for all members</p> <code>'*'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with the following columns: <code>['project', 'mip_era', 'activity_id', 'institution_id',       'source_id', 'experiment_id', 'member_id', 'table_id',       'variable_id', 'grid_label', 'version', 'id']</code> Note that <code>'id'</code> contains a list of file paths for each unique combination of metadata.</p> Source code in <code>climdyn_tools/ceda_esgf/base.py</code> <pre><code>def checkCEDA(source_id: str, activity_id: str, experiment_id: str, M2I: dict, variableList: List[str],\n              table_id: str, member_id: str = '*') -&gt; pd.DataFrame:\n    \"\"\"\n    For a given source_id (model), check which variables have data on CEDA,\n    and return a DataFrame similar to intake-esm format, combining paths for identical metadata.\n\n    Args:\n        source_id: source_id (e.g. \"MPI-ESM1-2-HR\")\n        activity_id: CMIP6 activity (e.g. \"CMIP\", \"ScenarioMIP\")\n        experiment_id: Experiment (e.g. \"historical\", \"ssp585\")\n        M2I: Mapping source_id -&gt; institution_id (from getModel_to_inst)\n        variableList: Variables to check (e.g. `['tas','rlut']`)\n        table_id: Corresponding CMIP6 table_id e.g. 'Amon'\n        member_id: The member Id specifically looked for - can be a wild car when searching for all members\n\n    Returns:\n        DataFrame with the following columns:&lt;/br&gt;\n            `['project', 'mip_era', 'activity_id', 'institution_id',\n                  'source_id', 'experiment_id', 'member_id', 'table_id',\n                  'variable_id', 'grid_label', 'version', 'id']`&lt;/br&gt;\n            Note that `'id'` contains a list of file paths for each unique combination of metadata.\n    \"\"\"\n    inst = M2I.get(source_id)\n    if inst is None:\n        raise ValueError(f\"No institution_id mapping found for source_id={SID}\")\n\n    records = []\n    for variable in variableList:\n        search_pattern = (\n            f\"/badc/cmip6/data/CMIP6/{activity_id}/\"\n            f\"{inst}/{source_id}/{experiment_id}/{member_id}/\"\n            f\"{table_id}/{variable}/*/latest/*.nc\"\n        )\n        paths = glob.glob(search_pattern)\n        if not paths:\n            continue\n\n        # Extract variant info\n        variant_df = parse_variant_labels(paths)\n\n        # Build record for each file\n        for idx, row in variant_df.iterrows():\n            records.append({\n                \"project\": \"CMIP6\",\n                \"mip_era\": \"CMIP6\",\n                \"activity_id\": activity_id,\n                \"institution_id\": inst,\n                \"source_id\": source_id,\n                \"experiment_id\": experiment_id,\n                \"member_id\": row[\"member_id\"],\n                \"table_id\": table_id,\n                \"variable_id\": variable,\n                \"grid_label\": \"gn\",       # assuming native grid; could parse if needed\n                \"version\": \"latest\",      # placeholder\n                \"id\": row[\"path\"]\n            })\n\n    df = pd.DataFrame(records)\n\n    if not df.empty:\n        # Combine rows with identical metadata except 'id' into a single row with list of ids\n        group_cols = ['project', 'mip_era', 'activity_id', 'institution_id',\n                      'source_id', 'experiment_id', 'member_id', 'table_id',\n                      'variable_id', 'grid_label', 'version']\n        df = df.groupby(group_cols, as_index=False)['id'].agg(list)\n\n    return df\n</code></pre>"},{"location":"code/ceda_esgf/base/#climdyn_tools.ceda_esgf.base.checkESGF","title":"<code>checkESGF(source_id, activity_id, experiment_id, M2I, variableList, table_id, member_id='*')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>source_id</code> <code>str</code> <p>source_id (e.g. \"MPI-ESM1-2-HR\")</p> required <code>activity_id</code> <code>str</code> <p>CMIP6 activity (e.g. \"CMIP\", \"ScenarioMIP\")</p> required <code>experiment_id</code> <code>str</code> <p>Experiment (e.g. \"historical\", \"ssp585\")</p> required <code>M2I</code> <code>dict</code> <p>Mapping source_id -&gt; institution_id (from getModel_to_inst)</p> required <code>variableList</code> <code>List[str]</code> <p>Variables to check (e.g. <code>['tas','rlut']</code>)</p> required <code>table_id</code> <code>str</code> <p>Corresponding CMIP6 table_id e.g. 'Amon'</p> required <code>member_id</code> <code>str</code> <p>The member Id specifically looked for - can be a wild car when searching for all members</p> <code>'*'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with the following columns: <code>['project', 'mip_era', 'activity_id', 'institution_id',       'source_id', 'experiment_id', 'member_id', 'table_id',       'variable_id', 'grid_label', 'version', 'id']</code> Note that <code>'id'</code> contains a list of file paths for each unique combination of metadata.</p> Source code in <code>climdyn_tools/ceda_esgf/base.py</code> <pre><code>def checkESGF(source_id: str, activity_id: str, experiment_id: str, M2I: dict, variableList: List[str], table_id: str,\n              member_id: str = '*') -&gt; pd.DataFrame:\n    \"\"\"\n\n    Args:\n        source_id: source_id (e.g. \"MPI-ESM1-2-HR\")\n        activity_id: CMIP6 activity (e.g. \"CMIP\", \"ScenarioMIP\")\n        experiment_id: Experiment (e.g. \"historical\", \"ssp585\")\n        M2I: Mapping source_id -&gt; institution_id (from getModel_to_inst)\n        variableList: Variables to check (e.g. `['tas','rlut']`)\n        table_id: Corresponding CMIP6 table_id e.g. 'Amon'\n        member_id: The member Id specifically looked for - can be a wild car when searching for all members\n\n    Returns:\n        DataFrame with the following columns:&lt;/br&gt;\n            `['project', 'mip_era', 'activity_id', 'institution_id',\n                  'source_id', 'experiment_id', 'member_id', 'table_id',\n                  'variable_id', 'grid_label', 'version', 'id']`&lt;/br&gt;\n            Note that `'id'` contains a list of file paths for each unique combination of metadata.\n    \"\"\"\n    cat = initializeCat()\n    if member_id == '*':\n        cat.search(\n            source_id=[source_id],\n            activity_drs=[activity_id],\n            experiment_id=[experiment_id],\n            variable_id=variableList,\n            table_id=table_id\n        )\n    else: \n        cat.search(\n            source_id=[source_id],\n            activity_drs=[activity_id],\n            variant_label = [member_id],\n            experiment_id=[experiment_id],\n            variable_id=variableList,\n            table_id=table_id\n        )\n    return cat\n</code></pre>"},{"location":"code/ceda_esgf/base/#climdyn_tools.ceda_esgf.base.compare_cat_res_pivot","title":"<code>compare_cat_res_pivot(cat_df, res_df)</code>","text":"<p>Compare two CMIP6 DataFrames (cat_df and res_df) and return a pivot table showing where each (member_id, variable_id) combination exists: 'cat only', 'res only', or 'both'.</p> <p>If res_df is empty or missing required columns, bypass comparison and return a pivot table indicating all entries are from cat_df only.</p> <p>Parameters:</p> Name Type Description Default <code>cat_df</code> <code>DataFrame</code> <p>Catalog dataframe with columns <code>['member_id', 'variable_id']</code></p> required <code>res_df</code> <code>DataFrame</code> <p>Result dataframe with columns <code>['member_id', 'variable_id']</code></p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Pivot table: <code>index=member_id, columns=variable_id, values='cat only'/'res only'/'both'</code></p> Source code in <code>climdyn_tools/ceda_esgf/base.py</code> <pre><code>def compare_cat_res_pivot(cat_df: pd.DataFrame, res_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Compare two CMIP6 DataFrames (cat_df and res_df) and return a pivot table\n    showing where each (member_id, variable_id) combination exists:\n    'cat only', 'res only', or 'both'.\n\n    If res_df is empty or missing required columns, bypass comparison and return\n    a pivot table indicating all entries are from cat_df only.\n\n    Args:\n        cat_df: Catalog dataframe with columns `['member_id', 'variable_id']`\n        res_df: Result dataframe with columns `['member_id', 'variable_id']`\n\n    Returns:\n        Pivot table: `index=member_id, columns=variable_id, values='cat only'/'res only'/'both'`\n    \"\"\"\n    required_cols = {'member_id', 'variable_id'}\n\n    # Check columns for cat_df\n    missing_cat_cols = required_cols - set(cat_df.columns)\n    if missing_cat_cols:\n        raise KeyError(f\"cat_df is missing columns: {missing_cat_cols}\")\n\n    # Bypass if res_df is empty or missing required columns\n    missing_res_cols = required_cols - set(res_df.columns)\n    if res_df.empty or missing_res_cols:\n        print(\"Warning: res_df is empty or missing required columns. Marking all entries as 'ESGF_ONLY'\")\n        cat_set = set(cat_df[['member_id', 'variable_id']].itertuples(index=False, name=None))\n        records = [{'member_id': k[0], 'variable_id': k[1], 'status': 'ESGF_ONLY'} for k in cat_set]\n        df = pd.DataFrame(records)\n        return df.pivot(index='member_id', columns='variable_id', values='status')\n\n    # Proceed with normal comparison\n    cat_set = set(cat_df[['member_id', 'variable_id']].itertuples(index=False, name=None))\n    res_set = set(res_df[['member_id', 'variable_id']].itertuples(index=False, name=None))\n    all_keys = cat_set | res_set\n\n    records = []\n    for key in all_keys:\n        if key in cat_set and key in res_set:\n            status = 'CEDA_CHOICE'\n        elif key in cat_set:\n            status = 'ESGF_ONLY'\n        else:\n            status = 'CEDA_ONLY'\n        records.append({'member_id': key[0], 'variable_id': key[1], 'status': status})\n\n    df = pd.DataFrame(records)\n    return df.pivot(index='member_id', columns='variable_id', values='status')\n</code></pre>"},{"location":"code/ceda_esgf/base/#climdyn_tools.ceda_esgf.base.extract_r","title":"<code>extract_r(member_id)</code>","text":"<p>Extract <code>r&lt;number&gt;</code> from CMIP6 member_id string like <code>r1i1p1f1</code>.</p> <p>Parameters:</p> Name Type Description Default <code>member_id</code> <code>str</code> <p>The member id string e.g. <code>r1i1p1f1</code></p> required <p>Returns:</p> Type Description <code>int</code> <p>The <code>number</code> where the <code>member_id</code> is in <code>r&lt;number&gt;</code> form.</p> Source code in <code>climdyn_tools/ceda_esgf/base.py</code> <pre><code>def extract_r(member_id: str) -&gt; int:\n    \"\"\"\n    Extract `r&lt;number&gt;` from CMIP6 member_id string like `r1i1p1f1`.\n\n    Args:\n        member_id: The member id string e.g. `r1i1p1f1`\n\n    Returns:\n        The `number` where the `member_id` is in `r&lt;number&gt;` form.\n    \"\"\"\n    m = re.match(r\"r(\\d+)i\\d+p\\d+f\\d+\", member_id)\n    if not m:\n        raise ValueError(f\"Could not parse r-number from member_id: {member_id}\")\n    return int(m.group(1))\n</code></pre>"},{"location":"code/ceda_esgf/base/#climdyn_tools.ceda_esgf.base.getCombinedData","title":"<code>getCombinedData(source_id, activity_id, experiment_id, M2I, CEDA_vars, ESGF_vars, table_id, member_id, doReadOut=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>source_id</code> <code>str</code> <p>CMIP6 model/source ID (e.g., 'NorESM2-LM').</p> required <code>activity_id</code> <code>str</code> <p>CMIP6 activity (e.g., 'DAMIP').</p> required <code>experiment_id</code> <code>str</code> <p>CMIP6 experiment ID (e.g., 'hist-aer').</p> required <code>M2I</code> <code>dict</code> <p>Mapping from model variable names to CMOR variable names.</p> required <code>CEDA_vars</code> <code>List[str]</code> <p>Variables to retrieve from CEDA.</p> required <code>ESGF_vars</code> <code>List[str]</code> <p>Variables to retrieve from ESGF.</p> required <code>table_id</code> <code>str</code> <p>CMIP6 table ID (e.g., 'Amon', 'Lmon').</p> required <code>member_id</code> <code>str</code> <p>Variant label for the ensemble member (e.g., 'r1i1p1f1').</p> required <code>doReadOut</code> <code>bool</code> <p>Whether to print while loading or not</p> <code>False</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Combined xarray Dataset containing requested variables from CEDA and ESGF.</p> <ul> <li>If only CEDA_vars exist, returns CEDA dataset.</li> <li>If only ESGF_vars exist, returns ESGF dataset.</li> <li>If both exist, returns merged dataset.</li> </ul> Source code in <code>climdyn_tools/ceda_esgf/base.py</code> <pre><code>def getCombinedData(source_id: str, activity_id: str, experiment_id: str, M2I: dict,\n                    CEDA_vars: List[str], ESGF_vars: List[str], table_id: str, member_id: str,\n                    doReadOut: bool = False) -&gt; xr.Dataset:\n    \"\"\"\n\n    Args:\n        source_id: CMIP6 model/source ID (e.g., 'NorESM2-LM').\n        activity_id: CMIP6 activity (e.g., 'DAMIP').\n        experiment_id: CMIP6 experiment ID (e.g., 'hist-aer').\n        M2I: Mapping from model variable names to CMOR variable names.\n        CEDA_vars: Variables to retrieve from CEDA.\n        ESGF_vars: Variables to retrieve from ESGF.\n        table_id: CMIP6 table ID (e.g., 'Amon', 'Lmon').\n        member_id: Variant label for the ensemble member (e.g., 'r1i1p1f1').\n        doReadOut: Whether to print while loading or not\n\n    Returns:\n        Combined xarray Dataset containing requested variables from CEDA and ESGF.\n\n            - If only CEDA_vars exist, returns CEDA dataset.\n            - If only ESGF_vars exist, returns ESGF dataset.\n            - If both exist, returns merged dataset.\n    \"\"\"\n    ds = None  # placeholder\n\n    if len(CEDA_vars) != 0:\n        if doReadOut:\n            print('loading CEDA Variables')\n        CEDA_paths = checkCEDA(\n            source_id, activity_id, experiment_id, M2I,\n            CEDA_vars, table_id, member_id=member_id\n        )\n        CEDA_paths = sum(CEDA_paths[\"id\"].tolist(), [])\n        CEDA_ds = xr.open_mfdataset(CEDA_paths, combine='by_coords')\n        ds = CEDA_ds\n\n    if len(ESGF_vars) != 0:\n        if doReadOut:\n            print('loading ESGF Variables')\n        ESGF_paths = checkESGF(\n            source_id, activity_id, experiment_id, M2I,\n            ESGF_vars, table_id, member_id=member_id\n        )\n        ESGF_ds = ESGF_paths.to_dataset_dict(add_measures=False, prefer_streaming=False)\n        ESGF_ds = xr.merge([d for d in ESGF_ds.values()])\n        ds = ESGF_ds if ds is None else xr.merge([ds, ESGF_ds], compat=\"override\")\n\n    return ds\n</code></pre>"},{"location":"code/ceda_esgf/base/#climdyn_tools.ceda_esgf.base.getModel_to_inst","title":"<code>getModel_to_inst(CMIP6Meta)</code>","text":"<p>Build a mapping from CMIP6 source_id (model name) to its first listed institution_id.</p> <p>Parameters:</p> Name Type Description Default <code>CMIP6Meta</code> <code>dict</code> <p>Parsed JSON from CMIP6_source_id.json</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of the form <code>{source_id: institution_id}</code></p> Source code in <code>climdyn_tools/ceda_esgf/base.py</code> <pre><code>def getModel_to_inst(CMIP6Meta: dict) -&gt; dict:\n    \"\"\"\n    Build a mapping from CMIP6 source_id (model name)\n    to its first listed institution_id.\n\n    Args:\n        CMIP6Meta: Parsed JSON from CMIP6_source_id.json\n\n    Returns:\n        Dictionary of the form `{source_id: institution_id}`\n    \"\"\"\n    return {\n        source_id: (attrs.get(\"institution_id\", [None])[0])\n        for source_id, attrs in CMIP6Meta.get(\"source_id\", {}).items()\n    }\n</code></pre>"},{"location":"code/ceda_esgf/base/#climdyn_tools.ceda_esgf.base.initializeCat","title":"<code>initializeCat()</code>","text":"<p>Attempt to initialize an ESGFCatalog object with multiple retries.</p> <p>The function tries up to 20 times to create an ESGFCatalog instance, waiting a random interval between 15 and 60 seconds between attempts if initialization fails. This helps handle transient network or server issues.</p> <p>Returns:</p> Type Description <code>Optional[ESGFCatalog]</code> <p>Successfully initialized ESGFCatalog object.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the catalog cannot be initialized after 20 attempts.</p> Notes <p>Each attempt logs its progress, and the exception is encountered if it fails.</p> Source code in <code>climdyn_tools/ceda_esgf/base.py</code> <pre><code>def initializeCat() -&gt; Optional[ESGFCatalog]:\n    \"\"\"\n    Attempt to initialize an ESGFCatalog object with multiple retries.\n\n    The function tries up to 20 times to create an ESGFCatalog instance,\n    waiting a random interval between 15 and 60 seconds between attempts\n    if initialization fails. This helps handle transient network or server issues.\n\n    Returns:\n        Successfully initialized ESGFCatalog object.\n\n    Raises:\n        RuntimeError: If the catalog cannot be initialized after 20 attempts.\n\n    Notes:\n        Each attempt logs its progress, and the exception is encountered if it fails.\n    \"\"\"\n    import random\n    import time\n    for attempt in range(1, 21):\n        try:\n            print(f\"Attempt {attempt} to initialize ESGFCatalog...\")\n            cat = ESGFCatalog()\n            print(\"ESGFCatalog successfully initialized.\")\n            return cat  # success!\n        except Exception as e:\n            print(f\"[Attempt {attempt}] ESGFCatalog failed to load: {e}\")\n            if attempt &lt; 20:\n                wait_time = random.randint(15, 60)\n                print(f\"Retrying in {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                raise RuntimeError(\"Failed to initialize ESGFCatalog after multiple attempts.\") from e\n</code></pre>"},{"location":"code/ceda_esgf/base/#climdyn_tools.ceda_esgf.base.load_cmip6_source_id","title":"<code>load_cmip6_source_id(local_path=None)</code>","text":"<p>Load CMIP6 source_id controlled vocabulary JSON. Tries local path first, then GitHub if not found.</p> <p>Parameters:</p> Name Type Description Default <code>local_path</code> <code>Optional[str]</code> <p>Path to local CMIP6_source_id.json</p> <code>None</code> <p>Returns:</p> Name Type Description <code>data</code> <code>dict</code> <p>Parsed JSON content</p> Source code in <code>climdyn_tools/ceda_esgf/base.py</code> <pre><code>def load_cmip6_source_id(local_path: Optional[str]=None) -&gt; dict:\n    \"\"\"\n    Load CMIP6 source_id controlled vocabulary JSON. Tries local path first, then GitHub if not found.\n\n    Args:\n        local_path: Path to local CMIP6_source_id.json\n\n    Returns:\n        data: Parsed JSON content\n    \"\"\"\n    url = \"https://raw.githubusercontent.com/WCRP-CMIP/CMIP6_CVs/main/CMIP6_source_id.json\"\n\n    # Try local file first\n    if local_path is not None and os.path.exists(local_path):\n        with open(local_path) as f:\n            data = json.load(f)\n    else:\n        with urllib.request.urlopen(url) as f:\n            data = json.load(f)\n\n    return data\n</code></pre>"},{"location":"code/ceda_esgf/base/#climdyn_tools.ceda_esgf.base.parse_variant_labels","title":"<code>parse_variant_labels(paths)</code>","text":"<p>Given a list of CMIP6 file paths or variant_labels, extract r, i, p, f values into a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>List[str]</code> <p>CMIP6 file paths or strings containing a variant_label.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns <code>['member_id', 'r', 'i', 'p', 'f', 'path']</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any path/label does not contain a valid variant_id.</p> Source code in <code>climdyn_tools/ceda_esgf/base.py</code> <pre><code>def parse_variant_labels(paths: List[str]) -&gt; pd.DataFrame:\n    \"\"\"\n    Given a list of CMIP6 file paths or variant_labels,\n    extract r, i, p, f values into a DataFrame.\n\n    Args:\n        paths: CMIP6 file paths or strings containing a variant_label.\n\n    Returns:\n        DataFrame with columns `['member_id', 'r', 'i', 'p', 'f', 'path']`.\n\n    Raises:\n        ValueError: If any path/label does not contain a valid variant_id.\n    \"\"\"\n    records = []\n    for path in paths:\n        match = re.search(r\"(r\\d+i\\d+p\\d+f\\d+)\", path)\n        if not match:\n            raise ValueError(f\"No valid variant_label found in: {path}\")\n        variant = match.group(1)\n        r, i, p, f = re.match(r\"r(\\d+)i(\\d+)p(\\d+)f(\\d+)\", variant).groups()\n        records.append({\"member_id\": variant, \"r\": int(r), \"i\": int(i), \"p\": int(p), \"f\": int(f), \"path\": path})\n    return pd.DataFrame(records)\n</code></pre>"},{"location":"code/ceda_esgf/base/#climdyn_tools.ceda_esgf.base.rank_members_with_vars","title":"<code>rank_members_with_vars(pivot)</code>","text":"<p>Rank member_ids by maximizing CEDA_CHOICE, minimizing ESGF_ONLY, then by r-number. Also attach the list of variable_ids available via CEDA and those only via ESGF.</p> <p>Parameters:</p> Name Type Description Default <code>pivot</code> <code>DataFrame</code> <p>Pivot table where rows correspond to <code>member_id</code> and columns correspond to <code>variable_id</code>. Each cell contains a status string indicating the availability of that variable for the member. Possible values include:</p> <ul> <li><code>\"CEDA_CHOICE\"</code>: Variable available via CEDA and preferred.</li> <li><code>\"ESGF_ONLY\"</code>: Variable only available via ESGF.</li> <li><code>\"CEDA_ONLY\"</code>: Variable only available via CEDA.</li> </ul> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with one row per member containing (the dataframe is sorted descending by <code>CEDA_CHOICE_count</code>,</p> <code>DataFrame</code> <p>ascending by <code>ESGF_ONLY_count</code>, and ascending by <code>r</code>):</p> <ul> <li><code>member_id</code>: The member identifier.</li> <li><code>CEDA_CHOICE_count</code>: Number of variables with status CEDA_CHOICE.</li> <li><code>ESGF_ONLY_count</code>: Number of variables with status ESGF_ONLY.</li> <li><code>CEDA_ONLY_count</code>: Number of variables with status CEDA_ONLY.</li> <li><code>CEDA_vars</code>: List of variable_ids available via CEDA (CEDA_CHOICE or CEDA_ONLY).</li> <li><code>ESGF_vars</code>: List of variable_ids available only via ESGF.</li> <li><code>r</code>: Extracted r-number from the member_id.</li> </ul> Source code in <code>climdyn_tools/ceda_esgf/base.py</code> <pre><code>def rank_members_with_vars(pivot: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Rank member_ids by maximizing CEDA_CHOICE, minimizing ESGF_ONLY,\n    then by r-number. Also attach the list of variable_ids available\n    via CEDA and those only via ESGF.\n\n    Args:\n        pivot: Pivot table where rows correspond to `member_id` and columns\n            correspond to `variable_id`. Each cell contains a status string indicating\n            the availability of that variable for the member. Possible values include:\n\n            - `\"CEDA_CHOICE\"`: Variable available via CEDA and preferred.\n            - `\"ESGF_ONLY\"`: Variable only available via ESGF.\n            - `\"CEDA_ONLY\"`: Variable only available via CEDA.\n\n    Returns:\n        A dataframe with one row per member containing (the dataframe is sorted descending by `CEDA_CHOICE_count`,\n        ascending by `ESGF_ONLY_count`, and ascending by `r`):\n\n            - `member_id`: The member identifier.\n            - `CEDA_CHOICE_count`: Number of variables with status CEDA_CHOICE.\n            - `ESGF_ONLY_count`: Number of variables with status ESGF_ONLY.\n            - `CEDA_ONLY_count`: Number of variables with status CEDA_ONLY.\n            - `CEDA_vars`: List of variable_ids available via CEDA (CEDA_CHOICE or CEDA_ONLY).\n            - `ESGF_vars`: List of variable_ids available only via ESGF.\n            - `r`: Extracted r-number from the member_id.\n    \"\"\"\n    records = []\n    for member_id, row in pivot.iterrows():\n        # row is a Series: index = variable_id, value = status\n        ceda_vars = row[row == \"CEDA_CHOICE\"].index.tolist()\n        esgf_vars = row[row == \"ESGF_ONLY\"].index.tolist()\n        ceda_only_vars = row[row == \"CEDA_ONLY\"].index.tolist()\n\n        records.append({\n            \"member_id\": member_id,\n            \"CEDA_CHOICE_count\": len(ceda_vars),\n            \"ESGF_ONLY_count\": len(esgf_vars),\n            \"CEDA_ONLY_count\": len(ceda_only_vars),\n            \"CEDA_vars\": ceda_vars,\n            \"ESGF_vars\": esgf_vars,\n            \"r\": extract_r(member_id),\n        })\n\n    df = pd.DataFrame(records)\n\n    # Sort by: CEDA_CHOICE descending, ESGF_ONLY ascending, then r ascending\n    df_sorted = df.sort_values(\n        by=[\"CEDA_CHOICE_count\", \"ESGF_ONLY_count\", \"r\"],\n        ascending=[False, True, True]\n    ).reset_index(drop=True)\n\n    return df_sorted\n</code></pre>"},{"location":"code/ceda_esgf/base/#climdyn_tools.ceda_esgf.base.source_id_in_activity","title":"<code>source_id_in_activity(activity_id, CMIP6Meta)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>activity_id</code> <code>str</code> <p>e.g. \"CMIP\", \"ScenarioMIP\"</p> required <code>CMIP6Meta</code> <code>dict</code> <p>Parsed JSON from CMIP6_source_id.json</p> required <p>Returns:</p> Name Type Description <code>sil</code> <code>List[str]</code> <p>List of source_id names that participate in the activity.</p> Source code in <code>climdyn_tools/ceda_esgf/base.py</code> <pre><code>def source_id_in_activity(activity_id: str, CMIP6Meta: dict) -&gt; List[str]:\n    \"\"\"\n\n    Args:\n        activity_id: e.g. \"CMIP\", \"ScenarioMIP\"\n        CMIP6Meta: Parsed JSON from CMIP6_source_id.json\n\n    Returns:\n        sil: List of source_id names that participate in the activity.\n    \"\"\"\n    sil = []\n    for source_id, meta in CMIP6Meta.get(\"source_id\", {}).items():\n        activities = meta.get(\"activity_participation\", [])\n        if activity_id in activities:\n            sil.append(source_id)\n    return sil\n</code></pre>"},{"location":"code/cesm/load/","title":"Load","text":""},{"location":"code/cesm/load/#climdyn_tools.cesm.load.ds_month_shift","title":"<code>ds_month_shift(ds, decode_times=True)</code>","text":"<p>When loading CESM data, for some reason the first month is marked as February, so this function shifts the time variable to correct it to January.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Dataset to apply the shift to. It should have been loaded with <code>decode_times=False</code>.</p> required <code>decode_times</code> <code>bool</code> <p>If <code>True</code>, will convert time to actual date.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset with first months shifted by -1 so now first month is January.</p> Source code in <code>climdyn_tools/cesm/load.py</code> <pre><code>def ds_month_shift(ds: xr.Dataset, decode_times: bool = True) -&gt; xr.Dataset:\n    \"\"\"\n    When loading CESM data, for some reason the first month is marked as February, so this function\n    shifts the time variable to correct it to January.\n\n    Args:\n        ds: Dataset to apply the shift to.\n            It should have been loaded with `decode_times=False`.\n        decode_times: If `True`, will convert time to actual date.\n\n    Returns:\n        Dataset with first months shifted by -1 so now first month is January.\n    \"\"\"\n    n_day_month = np.asarray(\n        [cftime.DatetimeNoLeap(1, i + 1, 1).daysinmonth for i in range(12)])  # number of days in each month\n    months_in_ds = np.arange(1, 13)                             # TODO may have to get months from ds e.g. ds.time.dt.month\n    n_day_month = n_day_month[np.asarray(months_in_ds) - 1]\n    n_months_in_ds = ds.time.size\n    n_years_in_ds = int(np.floor(n_months_in_ds / 12))\n    month_shift_array = np.concatenate((np.tile(n_day_month, n_years_in_ds),\n                                        n_day_month[:n_months_in_ds % 12]))\n    ds_new = ds.assign_coords({'time': ('time', ds.time.values - month_shift_array, ds.time.attrs)})\n    if decode_times:\n        ds_new = xr.decode_cf(ds_new)\n    return ds_new\n</code></pre>"},{"location":"code/cesm/load/#climdyn_tools.cesm.load.get_exp_dir","title":"<code>get_exp_dir(exp_name, comp='atm', archive_dir=jasmin_archive_dir)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>exp_name</code> <code>str</code> <p>Name of folder in <code>archive_dir</code> where data for this experiment was saved.</p> required <code>comp</code> <code>str</code> <p>Component of CESM to load data from. Options are:</p> <ul> <li><code>atm</code>: atmosphere</li> <li><code>ice</code>: ice</li> <li><code>lnd</code>: land</li> <li><code>rof</code>: river</li> </ul> <code>'atm'</code> <code>archive_dir</code> <code>str</code> <p>Directory where CESM archive data saved.</p> <code>jasmin_archive_dir</code> <p>Returns:</p> Source code in <code>climdyn_tools/cesm/load.py</code> <pre><code>def get_exp_dir(exp_name: str, comp: str = 'atm', archive_dir: str = jasmin_archive_dir):\n    \"\"\"\n\n    Args:\n        exp_name: Name of folder in `archive_dir` where data for this experiment was saved.\n        comp: Component of CESM to load data from.&lt;/br&gt;\n            Options are:\n\n            * `atm`: atmosphere\n            * `ice`: ice\n            * `lnd`: land\n            * `rof`: river\n        archive_dir: Directory where CESM archive data saved.\n\n    Returns:\n\n    \"\"\"\n    # LHS of comp_id_dict is name of directory containing hist files for the component\n    # RHS of comp_id_dict is the string indicating the component in the individual .nc files within this directory\n    comp_id_dict = {'atm': 'cam',  # atmosphere\n                    'ice': 'cice',  # ice\n                    'lnd': 'clm2',  # land\n                    'rof': 'mosart'}  # river\n    if comp not in comp_id_dict:\n        # Generate inverse dict to comp_id_dict\n        comp_id_dict_reverse = {key: list(comp_id_dict.keys())[i] for i, key in enumerate(comp_id_dict.values())}\n        if comp in comp_id_dict_reverse:\n            # Deal with case where give comp_file not comp_dir i.e. 'cam' rather than 'atm'\n            comp_dir = os.path.join(archive_dir, exp_name, comp_id_dict_reverse[comp])\n            comp_id = comp\n        else:\n            raise ValueError(f'comp must be one of {list(comp_id_dict.keys())} but got {comp}')\n    else:\n        comp_dir = os.path.join(archive_dir, exp_name, comp)\n        comp_id = comp_id_dict[comp]\n    return os.path.join(comp_dir, 'hist'), comp_id\n</code></pre>"},{"location":"code/cesm/load/#climdyn_tools.cesm.load.get_exp_file_dates","title":"<code>get_exp_file_dates(exp_name, comp='atm', archive_dir=jasmin_archive_dir, hist_file=0)</code>","text":"<p>Get dates indicated in file names of a particular experiment.</p> <p>Parameters:</p> Name Type Description Default <code>exp_name</code> <code>str</code> <p>Name of folder in <code>archive_dir</code> where data for this experiment was saved.</p> required <code>comp</code> <code>str</code> <p>Component of CESM to load data from. Options are:</p> <ul> <li><code>atm</code>: atmosphere</li> <li><code>ice</code>: ice</li> <li><code>lnd</code>: land</li> <li><code>rof</code>: river</li> </ul> <code>'atm'</code> <code>archive_dir</code> <code>str</code> <p>Directory where CESM archive data saved.</p> <code>jasmin_archive_dir</code> <code>hist_file</code> <code>int</code> <p>Which history file to load, <code>0</code> is the default monthly averaged data set.</p> <code>0</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>DataArray of dates indicated in file names of <code>exp_name</code>.</p> Source code in <code>climdyn_tools/cesm/load.py</code> <pre><code>def get_exp_file_dates(exp_name: str, comp: str = 'atm', archive_dir: str = jasmin_archive_dir,\n                       hist_file: int = 0) -&gt; xr.DataArray:\n    \"\"\"\n    Get dates indicated in file names of a particular experiment.\n\n    Args:\n        exp_name: Name of folder in `archive_dir` where data for this experiment was saved.\n        comp: Component of CESM to load data from.&lt;/br&gt;\n            Options are:\n\n            * `atm`: atmosphere\n            * `ice`: ice\n            * `lnd`: land\n            * `rof`: river\n        archive_dir: Directory where CESM archive data saved.\n        hist_file: Which history file to load, `0` is the default monthly averaged data set.\n\n    Returns:\n        DataArray of dates indicated in file names of `exp_name`.\n    \"\"\"\n    exp_dir, comp_id = get_exp_dir(exp_name, comp, archive_dir)\n    # Only load in specific years and/or months\n    data_files_all = os.listdir(exp_dir)\n    # only keep files of correct format\n    file_dates = []\n    for file in data_files_all:\n        date = re.search(rf'h{hist_file}\\.(.*?)\\.nc', file)\n        if not date:\n            continue\n        file_dates.append(parse_cesm_datetime(date.group(1)))\n    try:\n        return xr.DataArray(np.array(file_dates, dtype='datetime64[D]'), dims=\"time\", name=\"time\")\n    except OutOfBoundsDatetime as e:\n        warnings.warn(f\"Got out of bounds error, re-trying with NoLeap Calendar and cftime\\n{e}\")\n        cftime_dates = [cftime.DatetimeNoLeap(dt.year, dt.month, dt.day) for dt in file_dates]\n        return xr.DataArray(CFTimeIndex(cftime_dates, calendar=\"noleap\"), dims=\"time\", name=\"time\")\n</code></pre>"},{"location":"code/cesm/load/#climdyn_tools.cesm.load.load_dataset","title":"<code>load_dataset(exp_name, comp='atm', archive_dir=jasmin_archive_dir, hist_file=0, chunks=None, combine='nested', concat_dim='time', decode_times=True, parallel=False, preprocess=None, year_files=None, month_files=None, apply_month_shift_fix=True, logger=None)</code>","text":"<p>This loads a dataset of a given component produced by CESM.</p> <p>Parameters:</p> Name Type Description Default <code>exp_name</code> <code>str</code> <p>Name of folder in <code>archive_dir</code> where data for this experiment was saved.</p> required <code>comp</code> <code>str</code> <p>Component of CESM to load data from. Options are:</p> <ul> <li><code>atm</code>: atmosphere</li> <li><code>ice</code>: ice</li> <li><code>lnd</code>: land</li> <li><code>rof</code>: river</li> </ul> <code>'atm'</code> <code>archive_dir</code> <code>str</code> <p>Directory where CESM archive data saved.</p> <code>jasmin_archive_dir</code> <code>hist_file</code> <code>int</code> <p>Which history file to load, <code>0</code> is the default monthly averaged data set.</p> <code>0</code> <code>chunks</code> <code>Optional[Union[dict, Literal['auto'], int]]</code> <p>Dictionary with keys given by dimension names and values given by chunk sizes e.g. <code>{\"time\": 365, \"lat\": 50, \"lon\": 100}</code>. Has big impact on memory usage. If <code>None</code>, no chunking is performed.</p> <code>None</code> <code>combine</code> <code>Literal['by_coords', 'nested']</code> <p>Whether <code>xarray.combine_by_coords</code> or <code>xarray.combine_nested</code> is used to combine all the data.</p> <code>'nested'</code> <code>concat_dim</code> <code>str</code> <p>Dimensions to concatenate files along. You only need to provide this argument if combine='nested'.</p> <code>'time'</code> <code>parallel</code> <code>bool</code> <p>Whether parallel loading is performed.</p> <code>False</code> <code>preprocess</code> <code>Optional[Callable]</code> <p>Function to preprocess the data before loading.</p> <code>None</code> <code>decode_times</code> <code>bool</code> <p>If <code>True</code>, will convert time to actual date.</p> <code>True</code> <code>year_files</code> <code>Optional[Union[int, List, str]]</code> <p>Only files with these years in their name will be loaded. Leave as <code>None</code> to load all years. As well as integer or list of integers, there are three string options:</p> <ul> <li><code>'1975:1979'</code> will load in all years between 1975 and 1979 inclusive.</li> <li><code>first5</code> will load in the first 5 years.</li> <li><code>last5</code> will load in the last 5 years.</li> </ul> <code>None</code> <code>month_files</code> <code>Optional[Union[int, List, str]]</code> <p>Only files with these months (1 is Jan) in their names will be loaded. Leave as <code>None</code> to load all months. As well as integer or list of integers, there are three is a single string option: <code>'2:5'</code> will load in all months between 2 and 5 inclusive.</p> <code>None</code> <code>apply_month_shift_fix</code> <code>bool</code> <p>If <code>True</code>, will apply <code>ds_month_shift</code> before returning dataset. Only used for monthly averaged data i.e. <code>hist_file=0</code>.</p> <code>True</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional logger.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset containing all diagnostics specified for the experiment.</p> Source code in <code>climdyn_tools/cesm/load.py</code> <pre><code>def load_dataset(exp_name: str, comp: str = 'atm',\n                 archive_dir: str = jasmin_archive_dir,\n                 hist_file: int = 0,\n                 chunks: Optional[Union[dict, Literal[\"auto\"], int]] = None,\n                 combine: Literal[\"by_coords\", \"nested\"] = 'nested',\n                 concat_dim: str = 'time',\n                 decode_times: bool = True,\n                 parallel: bool = False,\n                 preprocess: Optional[Callable] = None,\n                 year_files: Optional[Union[int, List, str]] = None,\n                 month_files: Optional[Union[int, List, str]] = None,\n                 apply_month_shift_fix: bool = True,\n                 logger: Optional[logging.Logger] = None) -&gt; xr.Dataset:\n    \"\"\"\n    This loads a dataset of a given component produced by CESM.\n\n    Args:\n        exp_name: Name of folder in `archive_dir` where data for this experiment was saved.\n        comp: Component of CESM to load data from.&lt;/br&gt;\n            Options are:\n\n            * `atm`: atmosphere\n            * `ice`: ice\n            * `lnd`: land\n            * `rof`: river\n        archive_dir: Directory where CESM archive data saved.\n        hist_file: Which history file to load, `0` is the default monthly averaged data set.\n        chunks: Dictionary with keys given by dimension names and values given by chunk sizes\n            e.g. `{\"time\": 365, \"lat\": 50, \"lon\": 100}`.&lt;/br&gt;\n            Has big impact on memory usage. If `None`, no chunking is performed.\n        combine: Whether `xarray.combine_by_coords` or `xarray.combine_nested` is used to combine all the data.\n        concat_dim: Dimensions to concatenate files along.\n            You only need to provide this argument if combine='nested'.\n        parallel: Whether parallel loading is performed.\n        preprocess: Function to preprocess the data before loading.\n        decode_times: If `True`, will convert time to actual date.\n        year_files: Only files with these years in their name will be loaded. Leave as `None` to load all years.&lt;/br&gt;\n            As well as integer or list of integers, there are three string options:\n\n            * `'1975:1979'` will load in all years between 1975 and 1979 inclusive.\n            * `first5` will load in the first 5 years.\n            * `last5` will load in the last 5 years.\n        month_files: Only files with these months (1 is Jan) in their names will be loaded.\n            Leave as `None` to load all months.&lt;/br&gt;\n            As well as integer or list of integers, there are three is a single string option:\n            `'2:5'` will load in all months between 2 and 5 inclusive.\n        apply_month_shift_fix: If `True`, will apply `ds_month_shift` before returning dataset.&lt;/br&gt;\n            Only used for monthly averaged data i.e. `hist_file=0`.\n        logger: Optional logger.\n\n    Returns:\n        Dataset containing all diagnostics specified for the experiment.\n    \"\"\"\n    exp_dir, comp_id = get_exp_dir(exp_name, comp, archive_dir)\n    if year_files is None and month_files is None:\n        # Load all data in folder\n        # * indicates where date index info is, so we combine all datasets\n        data_files_load = os.path.join(exp_dir, f'{exp_name}.{comp_id}.h{hist_file}.*.nc')\n    else:\n        if hist_file != 0 and month_files is not None:\n            warnings.warn(f'If h{hist_file} files not saved monthly then will not have a file for each month so '\n                          f'using months_keep={month_files} will miss out different days in different years.')\n        file_dates = get_exp_file_dates(exp_name, comp, archive_dir, hist_file)\n        year_files_all = np.unique(file_dates.dt.year).tolist()\n        if year_files is None:\n            year_files = year_files_all         # all possible years\n        else:\n            year_files = parse_int_list(year_files, format_func=lambda x: int(x), all_values=year_files_all)\n            years_request_missing = [x for x in year_files if x not in year_files_all]\n            if len(years_request_missing) &gt; 0:\n                warnings.warn(f'The requested years = {years_request_missing}\\n'\n                              f'are missing from the available years = {year_files_all}')\n        month_files_all = np.unique(file_dates.dt.month).tolist()\n        if month_files is None:\n            month_files = month_files_all       # all possible months\n        else:\n            month_files = parse_int_list(month_files, format_func= lambda x: int(x))\n            month_request_missing = [x for x in month_files if x not in month_files_all]\n            if len(month_request_missing) &gt; 0:\n                warnings.warn(f'The requested months = {month_request_missing}\\n'\n                              f'are missing from the available months = {month_files_all}')\n\n        file_ind_keep = [i for i in range(file_dates.size) if (file_dates.dt.year[i] in year_files)\n                         and (file_dates.dt.month[i] in month_files)]\n        if len(file_ind_keep) == 0:\n            raise ValueError(f'No files with requested years and months in file name\\n'\n                             f'Available years: {np.unique(file_dates.dt.year).tolist()}\\n'\n                             f'Available months: {np.unique(file_dates.dt.month).tolist()}\\n'\n                             f'Requested years: {year_files}\\n'\n                             f'Requested months: {month_files}\\n')\n\n        # Only load in specific years and/or months\n        data_files_all = os.listdir(exp_dir)\n        # only keep files of correct format\n        data_files_all = [file for file in data_files_all if\n                          fnmatch.fnmatch(file, f'{exp_name}.{comp_id}.h{hist_file}.*.nc')]\n        # only keep files with requested years and months in file name\n        data_files_load = [os.path.join(exp_dir, file) for i, file in enumerate(data_files_all) if\n                           i in file_ind_keep]\n    if logger:\n        if isinstance(data_files_load, str):\n            logger.info(f'Loading data from all files: {data_files_load}')\n        else:\n            files_str = \"\\n\".join(data_files_load)\n            logger.info(f'Loading data from {len(data_files_load)} files:\\n{files_str}')\n    if apply_month_shift_fix and hist_file == 0:\n        ds = xr.open_mfdataset(data_files_load, decode_times=False, concat_dim=concat_dim,\n                               combine=combine, chunks=chunks, parallel=parallel, preprocess=preprocess)\n        return ds_month_shift(ds, decode_times)\n    else:\n        return xr.open_mfdataset(data_files_load, decode_times=decode_times,\n                                 concat_dim=concat_dim, combine=combine, chunks=chunks, parallel=parallel,\n                                 preprocess=preprocess)\n</code></pre>"},{"location":"code/cesm/load/#climdyn_tools.cesm.load.load_z2m","title":"<code>load_z2m(surf_geopotential_file=jasmin_surf_geopotential_file, var_reindex_like=None)</code>","text":"<p>Returns 2m geopotential height for CESM simulation.</p> <p>Parameters:</p> Name Type Description Default <code>surf_geopotential_file</code> <code>str</code> <p>File location of input data containing the geopotential at the surface: <code>PHIS</code>.</p> <code>jasmin_surf_geopotential_file</code> <code>var_reindex_like</code> <code>Optional[DataArray]</code> <p>Can provide a variable so <code>z2m</code> will have the same lat-lon as this variable.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>2m geopotential height in units of meters.</p> Source code in <code>climdyn_tools/cesm/load.py</code> <pre><code>def load_z2m(surf_geopotential_file: str = jasmin_surf_geopotential_file,\n             var_reindex_like: Optional[xr.DataArray] = None) -&gt; xr.DataArray:\n    \"\"\"\n    Returns 2m geopotential height for CESM simulation.\n\n    Args:\n        surf_geopotential_file: File location of input data containing the geopotential at the surface: `PHIS`.\n        var_reindex_like: Can provide a variable so `z2m` will have the same lat-lon as this variable.\n\n    Returns:\n        2m geopotential height in units of meters.\n    \"\"\"\n    # PHIS is the geopotential at the surface, so to get Z at reference height, divide by g and add 2\n    ds_z2m = xr.open_dataset(surf_geopotential_file)[['PHIS']]\n    z_refht = 2   # reference height is at 2m\n    ds_z2m['ZREFHT'] = ds_z2m['PHIS'] / g + z_refht               # PHIS is geopotential in m2/s2 so need to convert\n    del ds_z2m['PHIS']\n    if var_reindex_like is not None:\n        ds_z2m = ds_z2m.reindex_like(var_reindex_like, method=\"nearest\", tolerance=0.01)\n    ds_z2m = set_attrs(ds_z2m.ZREFHT, long_name='Geopotential height at reference height (2m)', units='m')\n    return ds_z2m\n</code></pre>"},{"location":"code/cesm/load/#climdyn_tools.cesm.load.parse_cesm_datetime","title":"<code>parse_cesm_datetime(time_str)</code>","text":"<p>Given a time string in the form either 'YYYY-MM' or 'YYYY-MM-DD-sssss' where <code>sssss</code> are the seconds since midnight, this will return the datetime object corresponding to that time.</p> <p>Parameters:</p> Name Type Description Default <code>time_str</code> <code>str</code> <p>String to convert to datetime object.</p> required <p>Returns:</p> Type Description <code>datetime</code> <p>Datetime object corresponding to <code>time_str</code>.</p> Source code in <code>climdyn_tools/cesm/load.py</code> <pre><code>def parse_cesm_datetime(time_str: str) -&gt; datetime:\n    \"\"\"\n    Given a time string in the form either 'YYYY-MM' or 'YYYY-MM-DD-sssss' where `sssss` are the seconds since midnight,\n    this will return the datetime object corresponding to that time.\n\n    Args:\n        time_str: String to convert to datetime object.\n\n    Returns:\n        Datetime object corresponding to `time_str`.\n    \"\"\"\n    date_part = time_str[:10]               # 'YYYY-MM-DD'\n    if len(date_part) == 7:\n        return datetime.strptime(date_part, '%Y-%m')\n    else:\n        seconds_since_midnight = int(time_str[11:])  # e.g., 00000 \u2192 0 seconds, 04320 \u2192 01:12:00 (1 hour, 12 minutes)\n        base_date = datetime.strptime(date_part, '%Y-%m-%d')\n        return base_date + timedelta(seconds=seconds_since_midnight)\n</code></pre>"},{"location":"code/cesm/load/#climdyn_tools.cesm.load.select_months","title":"<code>select_months(ds, month_nh, month_sh=None)</code>","text":"<p>In dataset, keep only <code>month_nh</code> months in the northern hemisphere, and <code>month_sh</code> in the southern hemisphere.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Dataset to select months from.</p> required <code>month_nh</code> <code>Union[ndarray, List[int]]</code> <p>List of months to keep in northern hemisphere.</p> required <code>month_sh</code> <code>Optional[Union[ndarray, List[int]]]</code> <p>List of months to keep in southern hemisphere. If <code>None</code>, will be the same as <code>month_nh</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset with months selected.</p> Source code in <code>climdyn_tools/cesm/load.py</code> <pre><code>def select_months(ds: xr.Dataset, month_nh: Union[np.ndarray, List[int]],\n                  month_sh: Optional[Union[np.ndarray, List[int]]] = None) -&gt; xr.Dataset:\n    \"\"\"\n    In dataset, keep only `month_nh` months in the northern hemisphere, and `month_sh` in the southern hemisphere.\n\n    Args:\n        ds: Dataset to select months from.\n        month_nh: List of months to keep in northern hemisphere.\n        month_sh: List of months to keep in southern hemisphere. If `None`, will be the same as `month_nh`.\n\n    Returns:\n        Dataset with months selected.\n    \"\"\"\n    # Select months for NH\n    if month_sh is None:\n        mask = ds.time.dt.month.isin(month_nh)\n    else:\n        mask_nh = (ds.lat &gt;= 0) &amp; (ds.time.dt.month.isin(month_nh))\n        mask_sh = (ds.lat &lt; 0) &amp; (ds.time.dt.month.isin(month_sh))\n        mask = mask_nh | mask_sh\n    return ds.where(mask)\n</code></pre>"},{"location":"code/era5/get_jasmin_era5/","title":"get_jasmin_era5","text":""},{"location":"code/era5/get_jasmin_era5/#modifications","title":"Modifications","text":"<p>This is a modification of someone else's package. </p> <p>The modifications are:</p> <ul> <li>Model level data has a different file name format for years 2000-2006, so if you try and load it normally, you get the following error:     <pre><code>ERA5 Model level data - 2000-2006: WARNING\n==========================================\nERA5 data for 2000-2006 was found to suffer from statospheric cold biases.\nAs such users should make use of the ERA5.1 data for this period.\nPlease see the associated dataset catalgoue records in the CEDA Data Catalogue for links to the ERA5.1 datasets.\n</code></pre>     To fix this, you can select the <code>archive</code> i.e. for years 2000-2006, you can call <code>era5=Find_era5(archive=1)</code>.</li> <li>Now accepts the variable <code>sp</code> (surface pressure) and computes by taking exponential of <code>lnsp</code>.</li> <li>Changed <code>H</code> to <code>h</code> in date stuff to avoid annoying warning:  <code>FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead</code></li> <li>Gives warning messages if the variable selected does not exist.</li> </ul>"},{"location":"code/era5/get_jasmin_era5/#initial-readme","title":"Initial README","text":"<p>A small python package to find and load ECMWF ERA5 data from the BADC archive on jasmin</p> <p>Requirements: <code>numpy</code>, <code>pandas</code> and <code>xarray</code></p> <p>Installation: <pre><code>git clone https://github.com/w-k-jones/get_jasmin_era5.git\npip install get_jasmin_era5/\n</code></pre></p> <p>Example usage:</p> <p>Import and initialise era5 object:</p> <p><pre><code>from get_jasmin_era5 import Find_era5\nera5 = Find_era5()\n</code></pre> Load temperature (t) for one time step:</p> <p><code>era5[\"t\", \"2020-06-01-12:00:00\"]</code></p> <pre><code>OUT:\n&lt;xarray.Dataset&gt;\nDimensions:    (longitude: 1440, latitude: 721, level: 137, time: 1)\nCoordinates:\n  * longitude  (longitude) float32 0.0 0.25 0.5 0.75 ... 359.0 359.2 359.5 359.8\n  * latitude   (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0\n  * level      (level) int32 1 2 3 4 5 6 7 8 ... 130 131 132 133 134 135 136 137\n  * time       (time) datetime64[ns] 2020-06-01T12:00:00\nData variables:\n    t          (time, level, latitude, longitude) float32 dask.array&lt;chunksize=(1, 137, 721, 1440), meta=np.ndarray&gt;\nAttributes:\n    Conventions:  CF-1.6\n    history:      2020-12-18 00:24:54 GMT by grib_to_netcdf-2.19.1: grib_to_n...\n</code></pre> <p>Load temperature (t) for all time steps on 2020/6/1:</p> <p><code>era5[\"t\", \"2020-06-01\":\"2020-06-02\"]</code></p> <pre><code>OUT: \n&lt;xarray.Dataset&gt;\nDimensions:    (longitude: 1440, latitude: 721, level: 137, time: 24)\nCoordinates:\n  * longitude  (longitude) float32 0.0 0.25 0.5 0.75 ... 359.0 359.2 359.5 359.8\n  * latitude   (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0\n  * level      (level) int32 1 2 3 4 5 6 7 8 ... 130 131 132 133 134 135 136 137\n  * time       (time) datetime64[ns] 2020-06-01 ... 2020-06-01T23:00:00\nData variables:\n    t          (time, level, latitude, longitude) float32 dask.array&lt;chunksize=(1, 137, 721, 1440), meta=np.ndarray&gt;\nAttributes:\n    Conventions:  CF-1.6\n    history:      2020-12-18 00:18:51 GMT by grib_to_netcdf-2.19.1: grib_to_n...\n</code></pre> <p>Slicing across the prime meridian and using ascending latitude values is also enabled:</p> <p><code>era5[\"t\", \"2020-06-01\":\"2020-06-02\", None, -90:90, -60:60]</code></p> <pre><code>OUT:\n&lt;xarray.Dataset&gt;\nDimensions:    (longitude: 721, latitude: 481, level: 137, time: 24)\nCoordinates:\n  * longitude  (longitude) float32 270.0 270.2 270.5 270.8 ... 89.5 89.75 90.0\n  * latitude   (latitude) float32 60.0 59.75 59.5 59.25 ... -59.5 -59.75 -60.0\n  * level      (level) int32 1 2 3 4 5 6 7 8 ... 130 131 132 133 134 135 136 137\n  * time       (time) datetime64[ns] 2020-06-01 ... 2020-06-01T23:00:00\nData variables:\n    t          (time, level, latitude, longitude) float32 dask.array&lt;chunksize=(1, 137, 481, 721), meta=np.ndarray&gt;\nAttributes:\n    Conventions:  CF-1.6\n    history:      2020-12-18 00:18:51 GMT by grib_to_netcdf-2.19.1: grib_to_n...\n</code></pre> <p>Load temperature (t), specific humidity (q), 2m temperature (2t) and surface height (z) for every three hours on 2020/6/1, for the 100th level downward, and between 90-270 degrees longtiude and -60-60 degrees latitude:</p> <p><code>era5[(\"t\", \"q\", \"2t\", \"z\"), \"2020-06-01\":\"2020-06-02\":\"3H\", 100:, 90:270, -60:60]</code></p> <pre><code>OUT:\n&lt;xarray.Dataset&gt;\nDimensions:    (longitude: 721, latitude: 481, level: 38, time: 8)\nCoordinates:\n  * longitude  (longitude) float32 90.0 90.25 90.5 90.75 ... 269.5 269.8 270.0\n  * latitude   (latitude) float32 60.0 59.75 59.5 59.25 ... -59.5 -59.75 -60.0\n  * level      (level) int32 100 101 102 103 104 105 ... 132 133 134 135 136 137\n  * time       (time) datetime64[ns] 2020-06-01 ... 2020-06-01T21:00:00\nData variables:\n    q          (time, level, latitude, longitude) float32 dask.array&lt;chunksize=(1, 38, 481, 721), meta=np.ndarray&gt;\n    t          (time, level, latitude, longitude) float32 dask.array&lt;chunksize=(1, 38, 481, 721), meta=np.ndarray&gt;\n    t2m        (time, latitude, longitude) float32 dask.array&lt;chunksize=(1, 481, 721), meta=np.ndarray&gt;\n    z          (latitude, longitude) float32 dask.array&lt;chunksize=(481, 721), meta=np.ndarray&gt;\nAttributes:\n    Conventions:  CF-1.6\n    history:      2020-12-18 00:18:45 GMT by grib_to_netcdf-2.19.1: grib_to_n...\n</code></pre> <p>Calculate pressure levels for the previous example:</p> <p><code>era5.pl[\"2020-06-01\":\"2020-06-02\":\"3H\", 100:, 90:270, -60:60]</code></p> <pre><code>OUT:\n&lt;xarray.DataArray (time: 8, level: 38, latitude: 481, longitude: 721)&gt;\narray([[[[ 57872.48474194,  57851.04773663,  57737.57761903, ...,\n           59308.89351633,  59309.34814012,  59312.7428857 ],\n         [ 57819.39331036,  57858.96549122,  57897.17048227, ...,\n           59314.18639922,  59313.68863594,  59313.68863594],\n         [ 57719.0044118 ,  57721.79188617,  57792.87911958, ...,\n           59317.08669994,  59320.93275089,  59319.48260053],\n         ...,\n...\n         [ 97330.26117977,  97328.03725574,  97325.90697063, ...,\n          100171.16415324, 100144.3366066 , 100115.30854566],\n         [ 97374.73185699,  97376.86994535,  97377.99361223, ...,\n          100127.62206184, 100100.7945152 , 100078.54747172],\n         [ 97401.85592684,  97413.84170684,  97422.48769918, ...,\n          100085.22704703, 100067.38103215, 100052.87480492]]]])\nCoordinates:\n  * level      (level) int64 100 101 102 103 104 105 ... 132 133 134 135 136 137\n  * time       (time) datetime64[ns] 2020-06-01 ... 2020-06-01T21:00:00\n  * longitude  (longitude) float32 90.0 90.25 90.5 90.75 ... 269.5 269.8 270.0\n  * latitude   (latitude) float32 60.0 59.75 59.5 59.25 ... -59.5 -59.75 -60.0\n</code></pre> <p>Calculate geopotential heights for the same example:</p> <p><code>era5.gz[\"2020-06-01\":\"2020-06-02\":\"3H\", 100:, 90:270, -60:60]</code></p> <pre><code>OUT:\n&lt;xarray.DataArray (time: 8, level: 38, latitude: 481, longitude: 721)&gt;\narray([[[[5.11737957e+04, 5.20996878e+04, 5.44298830e+04, ...,\n          4.13218602e+04, 4.12940971e+04, 4.12325377e+04],\n         [5.19596141e+04, 5.17204796e+04, 5.12408262e+04, ...,\n          4.13251723e+04, 4.13314302e+04, 4.13199145e+04],\n         [5.43623733e+04, 5.43935385e+04, 5.32772366e+04, ...,\n          4.13783058e+04, 4.12982295e+04, 4.12765517e+04],\n         ...,\n...\n         [1.01424387e+02, 1.01402244e+02, 9.25518078e+01, ...,\n          7.53837186e+01, 7.54326293e+01, 8.43107208e+01],\n         [9.25974757e+01, 9.25697266e+01, 9.25488523e+01, ...,\n          8.42433633e+01, 1.19629061e+02, 1.19669439e+02],\n         [1.01434764e+02, 1.01407126e+02, 9.25514372e+01, ...,\n          1.19581624e+02, 9.31415615e+01, 6.66779947e+01]]]])\nCoordinates:\n  * level      (level) int64 100 101 102 103 104 105 ... 132 133 134 135 136 137\n  * time       (time) datetime64[ns] 2020-06-01 ... 2020-06-01T21:00:00\n  * longitude  (longitude) float32 90.0 90.25 90.5 90.75 ... 269.5 269.8 270.0\n  * latitude   (latitude) float32 60.0 59.75 59.5 59.25 ... -59.5 -59.75 -60.0\n</code></pre> <p>It will also load ensemble datasets. By default, if you pass the \"enda\" argument it will load the ensemble mean:</p> <p><code>era5[(\"2t\",\"2d\"), \"2020-06-01\", None, None, None, \"enda\"]</code></p> <pre><code>OUT:\n&lt;xarray.Dataset&gt;\nDimensions:    (longitude: 1440, latitude: 721, time: 1)\nCoordinates:\n  * longitude  (longitude) float32 0.0 0.25 0.5 0.75 ... 359.0 359.2 359.5 359.8\n  * latitude   (latitude) float32 90.0 89.75 89.5 89.25 ... -89.5 -89.75 -90.0\n  * time       (time) datetime64[ns] 2020-06-01\nData variables:\n    d2m        (time, latitude, longitude) float32 dask.array&lt;chunksize=(1, 721, 1440), meta=np.ndarray&gt;\n    t2m        (time, latitude, longitude) float32 dask.array&lt;chunksize=(1, 721, 1440), meta=np.ndarray&gt;\nAttributes:\n    Conventions:  CF-1.6\n    history:      2020-12-22 18:23:11 GMT by grib_to_netcdf-2.19.1: grib_to_n...\n</code></pre> <p>However you can also get the ensemble members using <code>.enda</code>:</p> <p><code>era5.enda[(\"2t\", \"2d\"), \"2020-06-01\":\"2020-06-02\":\"3H\", 100:, 90:270, -60:60]</code></p> <pre><code>OUT:\n&lt;xarray.Dataset&gt;\nDimensions:          (longitude: 721, latitude: 481, time: 8,\n                      ensemble_member: 10)\nCoordinates:\n  * longitude        (longitude) float32 90.0 90.25 90.5 ... 269.5 269.8 270.0\n  * latitude         (latitude) float32 60.0 59.75 59.5 ... -59.5 -59.75 -60.0\n  * time             (time) datetime64[ns] 2020-06-01 ... 2020-06-01T21:00:00\n  * ensemble_member  (ensemble_member) int64 1 2 3 4 5 6 7 8 9 10\nData variables:\n    t2m              (ensemble_member, time, latitude, longitude) float32 dask.array&lt;chunksize=(1, 1, 481, 721), meta=np.ndarray&gt;\n    d2m              (ensemble_member, time, latitude, longitude) float32 dask.array&lt;chunksize=(1, 1, 481, 721), meta=np.ndarray&gt;\nAttributes:\n    Conventions:  CF-1.6\n    history:      2021-01-15 04:09:35 GMT by grib_to_netcdf-2.19.1: grib_to_n...\n</code></pre>"},{"location":"code/era5/get_jasmin_era5/core/","title":"Core","text":""},{"location":"code/era5/get_jasmin_era5/core/#climdyn_tools.era5.get_jasmin_era5.core.Find_era5","title":"<code>Find_era5</code>","text":"Source code in <code>climdyn_tools/era5/get_jasmin_era5/core.py</code> <pre><code>class Find_era5:\n    \"\"\"\n\n    \"\"\"\n    def __init__(self, archive: Literal[None, 1, 't'] = None):\n        \"\"\"\n        Initialise object to load ERA5 data from JASMIN.\n\n        Args:\n            archive: There are three types of ERA5 archives:\n\n                * `None` to use default ERA5 archive at `/badc/ecmwf-era5`\n                * `1` to use ERA5.1 at `/badc/ecmwf-era51`,\n                    which is suggested for model level data in years 2000-2006 inclusive.\n                * `t` to use Preliminary at `/badc/ecmwf-era5t`, near real-time data\n        \"\"\"\n        self._init_vars(archive)\n        self.pl = Pressure_levels_era5(archive)\n        self.gz = Geopotential_levels_era5(archive)\n        self.enda = Ensemble_era5(archive)\n\n    def _init_vars(self, archive: Literal[None, 1, 't'] = None):\n        self.archive = '' if archive is None else str(archive)\n        self.path = pathlib.Path(f\"/badc/ecmwf-era5{self.archive}/data/\")\n        self._INVARIANTS = [\n            \"anor\",\n            \"cl\",\n            \"cvh\",\n            \"cvl\",\n            \"dl\",\n            \"isor\",\n            \"lsm\",\n            \"sdfor\",\n            \"sdor\",\n            \"slor\",\n            \"slt\",\n            \"tvh\",\n            \"tvl\",\n            \"z\",\n        ]\n        self._INVARIANT_DATE = datetime(2000, 1, 1)\n\n        self._ML_VARS = ['sp', 'lnsp', 'o3', 'q', 't', 'u', 'v', 'vo', 'z']       # variables on model levels\n        self._SURF_VARS = ['10u', '10v', '2d', '2t', 'asn', 'cape', 'ci',   # variables on surface level\n                           'msl', 'sd', 'skt', 'sst', 'tcc', 'tcwv']\n        self._ML_WARNING_YEARS = np.arange(2000, 2007).tolist()  # in these years model level data suffer from statospheric cold biases - should use ERA5.1\n\n    def __getitem__(self, args):\n        var = args[0]\n        date = args[1]\n        sel = {}\n        if len(args) &gt; 2 and args[2] is not None:\n            sel[\"level\"] = args[2]\n        if len(args) &gt; 3 and args[3] is not None:\n            sel[\"longitude\"] = args[3]\n        if len(args) &gt; 4 and args[4] is not None:\n            sel[\"latitude\"] = args[4]\n\n        if len(args) &gt; 5:\n            model = args[5]\n        else:\n            model = \"oper\"\n\n        if isinstance(date, slice):\n            if date.step is None:\n                freq = \"1h\"\n            else:\n                freq = date.step\n            dates = (\n                pd.date_range(\n                    pd.to_datetime(date.start),\n                    pd.to_datetime(date.stop),\n                    inclusive=\"left\",\n                    freq=freq,\n                )\n                .to_pydatetime()\n                .tolist()\n            )\n        else:\n            dates = [pd.to_datetime(date).to_pydatetime()]\n\n        if isinstance(var, str):\n            var = [var]\n\n        # If requested surface pressure, must get log of surface pressure first and convert later\n        # Record info here\n        sp_info = {'in_var': 'sp' in var}\n        if sp_info['in_var']:\n            var.remove('sp')\n            if 'lnsp' not in var:\n                # Requested just sp\n                var.append('lnsp')\n                sp_info['delete_lnsp'] = True\n            else:\n                # Requested lnsp and sp\n                sp_info['delete_lnsp'] = False\n\n        self.warn_missing_years(var, dates)\n\n        files = sum(\n            [\n                self.find_files(v, dates, model=model)\n                for v in var\n                if v not in self._INVARIANTS\n            ],\n            [],\n        )\n        ds = None\n        if len(files) &gt; 0:\n            ds = xr.open_mfdataset(files, combine=\"by_coords\")\n            ds = sel_era5(ds, sel)\n        invar_files = sum(\n            [\n                self.find_files(v, dates, model=model)\n                for v in var\n                if v in self._INVARIANTS\n            ],\n            [],\n        )\n        if len(invar_files) &gt; 0:\n            invar_ds = xr.open_mfdataset(invar_files, combine=\"by_coords\").squeeze(\n                drop=True\n            )\n            invar_ds = sel_era5(invar_ds, sel)\n            if len(files) &gt; 0:\n                for invar in invar_ds.data_vars:\n                    ds[invar] = invar_ds[invar]\n            else:\n                ds = invar_ds\n        if ds is None:\n            # If no data found\n            raise ValueError(f'No data found for ecmwf-era5{self.archive}, var={var} and date={date}.\\n'\n                             f'Model level variables = {self._ML_VARS}\\n'\n                             f'Surface variables = {self._SURF_VARS}\\n'\n                             f'Invariant variables = {self._INVARIANTS}')\n\n        if sp_info['in_var']:\n            ds = convert_lnsp_to_sp(ds, delete_lnsp=sp_info['delete_lnsp'])\n\n        return ds\n\n    def find_files(\n        self, var: str, dates: list[datetime], model: str = \"oper\"\n    ) -&gt; list[str]:\n        if var in self._INVARIANTS:\n            return self.find_invariant(var)\n        else:\n            return sum(\n                [self.find_single_file(var, date, model=model) for date in dates], []\n            )\n\n    def find_invariant(self, var: str) -&gt; list[str]:\n        if self.archive != '':\n            warnings.warn(f'Using base archive (ecmwf-era5), for invariant var={var} '\n                          f'despite requested archive of ecmwf-era5{self.archive}.')\n        date = self._INVARIANT_DATE\n        files = sorted(\n            list(\n                self.path.glob(\n                    f\"invariants/ecmwf-era5_oper_an_sfc_{date.year:04d}{date.month:02d}{date.day:02d}0000.{var}.inv.nc\"\n                )\n            )\n        )\n\n        return files\n\n    def find_single_file(\n        self, var: str, date: datetime, model: str = \"oper\"\n    ) -&gt; list[str]:\n        if model == \"enda\":\n            level_type = \"em_sfc\"\n        else:\n            level_type = \"*\"\n        files = sorted(\n            list(\n                self.path.glob(\n                    f\"{model}/{level_type}/{date.year:04d}/{date.month:02d}/{date.day:02d}/\"\n                    f\"ecmwf-era5{self.archive}_{model}_*_{date.year:04d}{date.month:02d}{date.day:02d}{date.hour:02d}*.{var}.nc\"\n                )\n            )\n        )\n        return files\n\n    def warn_missing_years(self, var: list[str], dates: list[datetime], model: str = \"oper\") -&gt; None:\n        \"\"\"\n        Warn about some years that might be missing model level data for ERA5 - print README in relevant directory\n\n        Args:\n            var: List of variables requested\n            dates: List of dates requested\n            model: Model requested\n\n        Returns:\n\n        \"\"\"\n        var_to_warn = [v for v in var if v in self._ML_VARS]\n        if self.archive=='' and (model == \"oper\") and (len(var_to_warn) &gt; 0):\n            years_to_warn = list({d.year for d in dates if d.year in self._ML_WARNING_YEARS})\n            if len(years_to_warn) &gt; 0:\n                dir_use = list(self.path.glob(f'oper/an_ml/{years_to_warn[0]}'))[0]\n                with open(f\"{dir_use}/00README\", \"r\") as f:\n                    contents = f.read()\n                warnings.warn(f\"README for year {years_to_warn[0]} (Can use ERA5.1 by setting `archive=1`):\\n{contents}\")\n</code></pre>"},{"location":"code/era5/get_jasmin_era5/core/#climdyn_tools.era5.get_jasmin_era5.core.Find_era5.__init__","title":"<code>__init__(archive=None)</code>","text":"<p>Initialise object to load ERA5 data from JASMIN.</p> <p>Parameters:</p> Name Type Description Default <code>archive</code> <code>Literal[None, 1, 't']</code> <p>There are three types of ERA5 archives:</p> <ul> <li><code>None</code> to use default ERA5 archive at <code>/badc/ecmwf-era5</code></li> <li><code>1</code> to use ERA5.1 at <code>/badc/ecmwf-era51</code>,     which is suggested for model level data in years 2000-2006 inclusive.</li> <li><code>t</code> to use Preliminary at <code>/badc/ecmwf-era5t</code>, near real-time data</li> </ul> <code>None</code> Source code in <code>climdyn_tools/era5/get_jasmin_era5/core.py</code> <pre><code>def __init__(self, archive: Literal[None, 1, 't'] = None):\n    \"\"\"\n    Initialise object to load ERA5 data from JASMIN.\n\n    Args:\n        archive: There are three types of ERA5 archives:\n\n            * `None` to use default ERA5 archive at `/badc/ecmwf-era5`\n            * `1` to use ERA5.1 at `/badc/ecmwf-era51`,\n                which is suggested for model level data in years 2000-2006 inclusive.\n            * `t` to use Preliminary at `/badc/ecmwf-era5t`, near real-time data\n    \"\"\"\n    self._init_vars(archive)\n    self.pl = Pressure_levels_era5(archive)\n    self.gz = Geopotential_levels_era5(archive)\n    self.enda = Ensemble_era5(archive)\n</code></pre>"},{"location":"code/era5/get_jasmin_era5/core/#climdyn_tools.era5.get_jasmin_era5.core.Find_era5.warn_missing_years","title":"<code>warn_missing_years(var, dates, model='oper')</code>","text":"<p>Warn about some years that might be missing model level data for ERA5 - print README in relevant directory</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>list[str]</code> <p>List of variables requested</p> required <code>dates</code> <code>list[datetime]</code> <p>List of dates requested</p> required <code>model</code> <code>str</code> <p>Model requested</p> <code>'oper'</code> <p>Returns:</p> Source code in <code>climdyn_tools/era5/get_jasmin_era5/core.py</code> <pre><code>def warn_missing_years(self, var: list[str], dates: list[datetime], model: str = \"oper\") -&gt; None:\n    \"\"\"\n    Warn about some years that might be missing model level data for ERA5 - print README in relevant directory\n\n    Args:\n        var: List of variables requested\n        dates: List of dates requested\n        model: Model requested\n\n    Returns:\n\n    \"\"\"\n    var_to_warn = [v for v in var if v in self._ML_VARS]\n    if self.archive=='' and (model == \"oper\") and (len(var_to_warn) &gt; 0):\n        years_to_warn = list({d.year for d in dates if d.year in self._ML_WARNING_YEARS})\n        if len(years_to_warn) &gt; 0:\n            dir_use = list(self.path.glob(f'oper/an_ml/{years_to_warn[0]}'))[0]\n            with open(f\"{dir_use}/00README\", \"r\") as f:\n                contents = f.read()\n            warnings.warn(f\"README for year {years_to_warn[0]} (Can use ERA5.1 by setting `archive=1`):\\n{contents}\")\n</code></pre>"},{"location":"code/era5/get_jasmin_era5/utils/","title":"Utils","text":""},{"location":"code/era5/get_jasmin_era5/utils/#climdyn_tools.era5.get_jasmin_era5.utils.convert_lnsp_to_sp","title":"<code>convert_lnsp_to_sp(ds, delete_lnsp=True)</code>","text":"<p>Function to convert logarithm of surface pressure, into surface pressure with units of Pa</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Dataset containing variable called <code>lnsp</code> to be converted.</p> required <code>delete_lnsp</code> <code>bool</code> <p>If True, delete the original <code>lnsp</code> variable.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset containing variable called <code>sp</code></p> Source code in <code>climdyn_tools/era5/get_jasmin_era5/utils.py</code> <pre><code>def convert_lnsp_to_sp(ds: xr.Dataset, delete_lnsp: bool = True) -&gt; xr.Dataset:\n    \"\"\"\n    Function to convert logarithm of surface pressure, into surface pressure with units of Pa\n\n    Args:\n        ds: Dataset containing variable called `lnsp` to be converted.\n        delete_lnsp: If True, delete the original `lnsp` variable.\n\n    Returns:\n        Dataset containing variable called `sp`\n    \"\"\"\n    if 'lnsp' not in ds:\n        print(\"Dataset does not contain variable called 'lnsp', returning original dataset.\")\n        return ds\n\n    # Compute surface pressure\n    sp = np.exp(ds['lnsp'])\n\n    # Rename and set attributes\n    sp.name = 'sp'\n    sp.attrs['long_name'] = 'surface pressure'\n    sp.attrs['units'] = 'Pa'\n\n    # Drop lnsp and add sp\n    if delete_lnsp:\n        ds = ds.drop_vars('lnsp')\n    ds['sp'] = sp\n\n    return ds\n</code></pre>"},{"location":"code/era5/get_jasmin_era5/utils/#climdyn_tools.era5.get_jasmin_era5.utils.get_ab","title":"<code>get_ab(n_levels)</code>","text":"<p>Returns ECMWF hybrid pressure level a and b coefficients for the specified level definition.</p> <p>Parameters:</p> Name Type Description Default <code>n_levels</code> <code>int</code> <p>Number of levels to provide coefficients for. Must be one of the following ECMWF level definitions: 137, 91, 62, 60, 50, 40, 31, 19, 16</p> required <p>Returns:</p> Name Type Description <code>a</code> <code>ndarray</code> <p>Floating point a coefficients in Pa (length = n_levels + 1)</p> <code>b</code> <code>ndarray</code> <p>Floating point b coefficients in Pa (length = n_levels + 1)</p> Source code in <code>climdyn_tools/era5/get_jasmin_era5/utils.py</code> <pre><code>def get_ab(n_levels: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns ECMWF hybrid pressure level a and b coefficients for the specified\n    level definition.\n\n    Args:\n        n_levels: Number of levels to provide coefficients for. Must be one of the\n            following ECMWF level definitions: 137, 91, 62, 60, 50, 40, 31, 19, 16\n\n    Returns:\n        a: Floating point a coefficients in Pa (length = n_levels + 1)\n        b: Floating point b coefficients in Pa (length = n_levels + 1)\n    \"\"\"\n    level_list = [137, 91, 62, 60, 50, 40, 31, 19, 16]\n    if n_levels not in level_list:\n        raise Exception(\n            \"\"\"Get_ab: Number of levels input not recognised as a\n                        valid ECMWF level definition. n_levels must be one of\n                        the following: \"\"\"\n            + str(level_list)\n        )\n    if n_levels == 137:\n        a = [\n            0.0,\n            2.000365,\n            3.102241,\n            4.666084,\n            6.827977,\n            9.746966,\n            13.605424,\n            18.608931,\n            24.985718,\n            32.98571,\n            42.879242,\n            54.955463,\n            69.520576,\n            86.895882,\n            107.415741,\n            131.425507,\n            159.279404,\n            191.338562,\n            227.968948,\n            269.539581,\n            316.420746,\n            368.982361,\n            427.592499,\n            492.616028,\n            564.413452,\n            643.339905,\n            729.744141,\n            823.967834,\n            926.34491,\n            1037.201172,\n            1156.853638,\n            1285.610352,\n            1423.770142,\n            1571.622925,\n            1729.448975,\n            1897.519287,\n            2076.095947,\n            2265.431641,\n            2465.770508,\n            2677.348145,\n            2900.391357,\n            3135.119385,\n            3381.743652,\n            3640.468262,\n            3911.490479,\n            4194.930664,\n            4490.817383,\n            4799.149414,\n            5119.89502,\n            5452.990723,\n            5798.344727,\n            6156.074219,\n            6526.946777,\n            6911.870605,\n            7311.869141,\n            7727.412109,\n            8159.354004,\n            8608.525391,\n            9076.400391,\n            9562.682617,\n            10065.97852,\n            10584.63184,\n            11116.66211,\n            11660.06738,\n            12211.54785,\n            12766.87305,\n            13324.66895,\n            13881.33106,\n            14432.13965,\n            14975.61523,\n            15508.25684,\n            16026.11523,\n            16527.32227,\n            17008.78906,\n            17467.61328,\n            17901.62109,\n            18308.43359,\n            18685.71875,\n            19031.28906,\n            19343.51172,\n            19620.04297,\n            19859.39063,\n            20059.93164,\n            20219.66406,\n            20337.86328,\n            20412.30859,\n            20442.07813,\n            20425.71875,\n            20361.81641,\n            20249.51172,\n            20087.08594,\n            19874.02539,\n            19608.57227,\n            19290.22656,\n            18917.46094,\n            18489.70703,\n            18006.92578,\n            17471.83984,\n            16888.6875,\n            16262.04688,\n            15596.69531,\n            14898.45313,\n            14173.32422,\n            13427.76953,\n            12668.25781,\n            11901.33984,\n            11133.30469,\n            10370.17578,\n            9617.515625,\n            8880.453125,\n            8163.375,\n            7470.34375,\n            6804.421875,\n            6168.53125,\n            5564.382813,\n            4993.796875,\n            4457.375,\n            3955.960938,\n            3489.234375,\n            3057.265625,\n            2659.140625,\n            2294.242188,\n            1961.5,\n            1659.476563,\n            1387.546875,\n            1143.25,\n            926.507813,\n            734.992188,\n            568.0625,\n            424.414063,\n            302.476563,\n            202.484375,\n            122.101563,\n            62.78125,\n            22.835938,\n            3.757813,\n            0,\n            0,\n        ]\n        b = [\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0.000007,\n            0.000024,\n            0.000059,\n            0.000112,\n            0.000199,\n            0.00034,\n            0.000562,\n            0.00089,\n            0.001353,\n            0.001992,\n            0.002857,\n            0.003971,\n            0.005378,\n            0.007133,\n            0.009261,\n            0.011806,\n            0.014816,\n            0.018318,\n            0.022355,\n            0.026964,\n            0.032176,\n            0.038026,\n            0.044548,\n            0.051773,\n            0.059728,\n            0.068448,\n            0.077958,\n            0.088286,\n            0.099462,\n            0.111505,\n            0.124448,\n            0.138313,\n            0.153125,\n            0.16891,\n            0.185689,\n            0.203491,\n            0.222333,\n            0.242244,\n            0.263242,\n            0.285354,\n            0.308598,\n            0.332939,\n            0.358254,\n            0.384363,\n            0.411125,\n            0.438391,\n            0.466003,\n            0.4938,\n            0.521619,\n            0.549301,\n            0.576692,\n            0.603648,\n            0.630036,\n            0.655736,\n            0.680643,\n            0.704669,\n            0.727739,\n            0.749797,\n            0.770798,\n            0.790717,\n            0.809536,\n            0.827256,\n            0.843881,\n            0.859432,\n            0.873929,\n            0.887408,\n            0.8999,\n            0.911448,\n            0.922096,\n            0.931881,\n            0.94086,\n            0.949064,\n            0.95655,\n            0.963352,\n            0.969513,\n            0.975078,\n            0.980072,\n            0.984542,\n            0.9885,\n            0.991984,\n            0.995003,\n            0.99763,\n            1,\n        ]\n    elif n_levels == 60:\n        a = [\n            0.0,\n            20.0,\n            38.425343,\n            63.647804,\n            95.636963,\n            134.483307,\n            180.584351,\n            234.779053,\n            298.495789,\n            373.971924,\n            464.618134,\n            575.651001,\n            713.218079,\n            883.660522,\n            1094.834717,\n            1356.474609,\n            1680.640259,\n            2082.273926,\n            2579.888672,\n            3196.421631,\n            3960.291504,\n            4906.708496,\n            6018.019531,\n            7306.631348,\n            8765.053711,\n            10376.12695,\n            12077.44629,\n            13775.3252,\n            15379.80566,\n            16819.47461,\n            18045.18359,\n            19027.69531,\n            19755.10938,\n            20222.20508,\n            20429.86328,\n            20384.48047,\n            20097.40234,\n            19584.33008,\n            18864.75,\n            17961.35742,\n            16899.46875,\n            15706.44727,\n            14411.12402,\n            13043.21875,\n            11632.75879,\n            10209.50098,\n            8802.356445,\n            7438.803223,\n            6144.314941,\n            4941.77832,\n            3850.91333,\n            2887.696533,\n            2063.779785,\n            1385.912598,\n            855.361755,\n            467.333588,\n            210.39389,\n            65.889244,\n            7.367743,\n            0.0,\n            0.0,\n        ]\n        b = [\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.0,\n            0.000076,\n            0.000461,\n            0.001815,\n            0.005081,\n            0.011143,\n            0.020678,\n            0.034121,\n            0.05169,\n            0.073534,\n            0.099675,\n            0.130023,\n            0.164384,\n            0.202476,\n            0.243933,\n            0.288323,\n            0.335155,\n            0.383892,\n            0.433963,\n            0.484772,\n            0.53571,\n            0.586168,\n            0.635547,\n            0.683269,\n            0.728786,\n            0.771597,\n            0.811253,\n            0.847375,\n            0.879657,\n            0.907884,\n            0.93194,\n            0.951822,\n            0.967645,\n            0.979663,\n            0.98827,\n            0.994019,\n            0.99763,\n            1.0,\n        ]\n    elif n_levels == 91:\n        a = [\n            2.00004,\n            3.980832,\n            7.387186,\n            12.908319,\n            21.413612,\n            33.952858,\n            51.746601,\n            76.167656,\n            108.715561,\n            150.986023,\n            204.637451,\n            271.356506,\n            352.824493,\n            450.685791,\n            566.519226,\n            701.813354,\n            857.945801,\n            1036.166504,\n            1237.585449,\n            1463.16394,\n            1713.709595,\n            1989.87439,\n            2292.155518,\n            2620.898438,\n            2976.302246,\n            3358.425781,\n            3767.196045,\n            4202.416504,\n            4663.776367,\n            5150.859863,\n            5663.15625,\n            6199.839355,\n            6759.727051,\n            7341.469727,\n            7942.92627,\n            8564.624023,\n            9208.305664,\n            9873.560547,\n            10558.88184,\n            11262.48438,\n            11982.66211,\n            12713.89746,\n            13453.22559,\n            14192.00977,\n            14922.68555,\n            15638.05371,\n            16329.56055,\n            16990.62305,\n            17613.28125,\n            18191.0293,\n            18716.96875,\n            19184.54492,\n            19587.51367,\n            19919.79688,\n            20175.39453,\n            20348.91602,\n            20434.1582,\n            20426.21875,\n            20319.01172,\n            20107.03125,\n            19785.35742,\n            19348.77539,\n            18798.82227,\n            18141.29688,\n            17385.5957,\n            16544.58594,\n            15633.56641,\n            14665.64551,\n            13653.21973,\n            12608.38379,\n            11543.16699,\n            10471.31055,\n            9405.222656,\n            8356.25293,\n            7335.164551,\n            6353.920898,\n            5422.802734,\n            4550.21582,\n            3743.464355,\n            3010.146973,\n            2356.202637,\n            1784.854614,\n            1297.656128,\n            895.193542,\n            576.314148,\n            336.772369,\n            162.043427,\n            54.208336,\n            6.575628,\n            0.00316,\n            0,\n        ]\n        b = [\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0.000014,\n            0.000055,\n            0.000131,\n            0.000279,\n            0.000548,\n            0.001,\n            0.001701,\n            0.002765,\n            0.004267,\n            0.006322,\n            0.009035,\n            0.012508,\n            0.01686,\n            0.022189,\n            0.02861,\n            0.036227,\n            0.045146,\n            0.055474,\n            0.067316,\n            0.080777,\n            0.095964,\n            0.112979,\n            0.131935,\n            0.152934,\n            0.176091,\n            0.20152,\n            0.229315,\n            0.259554,\n            0.291993,\n            0.326329,\n            0.362203,\n            0.399205,\n            0.436906,\n            0.475016,\n            0.51328,\n            0.551458,\n            0.589317,\n            0.626559,\n            0.662934,\n            0.698224,\n            0.732224,\n            0.764679,\n            0.795385,\n            0.824185,\n            0.85095,\n            0.875518,\n            0.897767,\n            0.917651,\n            0.935157,\n            0.950274,\n            0.963007,\n            0.973466,\n            0.982238,\n            0.989153,\n            0.994204,\n            0.99763,\n            1,\n        ]\n    elif n_levels == 62:\n        a = [\n            0,\n            988.835876,\n            1977.67627,\n            2966.516602,\n            3955.356934,\n            4944.197266,\n            5933.037598,\n            6921.870117,\n            7909.441406,\n            8890.707031,\n            9860.52832,\n            10807.7832,\n            11722.74902,\n            12595.00684,\n            13419.46387,\n            14192.00977,\n            14922.68555,\n            15638.05371,\n            16329.56055,\n            16990.62305,\n            17613.28125,\n            18191.0293,\n            18716.96875,\n            19184.54492,\n            19587.51367,\n            19919.79688,\n            20175.39453,\n            20348.91602,\n            20434.1582,\n            20426.21875,\n            20319.01172,\n            20107.03125,\n            19785.35742,\n            19348.77539,\n            18798.82227,\n            18141.29688,\n            17385.5957,\n            16544.58594,\n            15633.56641,\n            14665.64551,\n            13653.21973,\n            12608.38379,\n            11543.16699,\n            10471.31055,\n            9405.222656,\n            8356.25293,\n            7335.164551,\n            6353.920898,\n            5422.802734,\n            4550.21582,\n            3743.464355,\n            3010.146973,\n            2356.202637,\n            1784.854614,\n            1297.656128,\n            895.193542,\n            576.314148,\n            336.772369,\n            162.043427,\n            54.208336,\n            6.575628,\n            0.00316,\n            0,\n        ]\n        b = [\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0.000013,\n            0.000087,\n            0.000275,\n            0.000685,\n            0.001415,\n            0.002565,\n            0.004187,\n            0.006322,\n            0.009035,\n            0.012508,\n            0.01686,\n            0.022189,\n            0.02861,\n            0.036227,\n            0.045146,\n            0.055474,\n            0.067316,\n            0.080777,\n            0.095964,\n            0.112979,\n            0.131935,\n            0.152934,\n            0.176091,\n            0.20152,\n            0.229315,\n            0.259554,\n            0.291993,\n            0.326329,\n            0.362203,\n            0.399205,\n            0.436906,\n            0.475016,\n            0.51328,\n            0.551458,\n            0.589317,\n            0.626559,\n            0.662934,\n            0.698224,\n            0.732224,\n            0.764679,\n            0.795385,\n            0.824185,\n            0.85095,\n            0.875518,\n            0.897767,\n            0.917651,\n            0.935157,\n            0.950274,\n            0.963007,\n            0.973466,\n            0.982238,\n            0.989153,\n            0.994204,\n            0.99763,\n            1,\n        ]\n    elif n_levels == 50:\n        a = [\n            0,\n            20.006149,\n            43.29781,\n            75.34623,\n            115.082146,\n            161.897491,\n            215.896912,\n            278.005798,\n            350.138184,\n            435.562286,\n            539.651489,\n            668.61554,\n            828.398987,\n            1026.366943,\n            1271.644531,\n            1575.537842,\n            1952.054443,\n            2418.549805,\n            2996.526611,\n            3712.626221,\n            4599.856934,\n            5699.114746,\n            6998.388184,\n            8507.411133,\n            10181.70703,\n            11883.08984,\n            13442.91504,\n            14736.35449,\n            15689.20606,\n            16266.60938,\n            16465.00391,\n            16297.62012,\n            15791.59766,\n            14985.26953,\n            13925.51953,\n            12665.29492,\n            11261.23047,\n            9771.40625,\n            8253.210938,\n            6761.339844,\n            5345.917969,\n            4050.71875,\n            2911.570312,\n            1954.804688,\n            1195.890625,\n            638.148438,\n            271.625,\n            72.0625,\n            0,\n            0,\n            0,\n        ]\n        b = [\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0,\n            0.0001,\n            0.000673,\n            0.003163,\n            0.009292,\n            0.020319,\n            0.036975,\n            0.059488,\n            0.087895,\n            0.122004,\n            0.161442,\n            0.205703,\n            0.254189,\n            0.306235,\n            0.361145,\n            0.418202,\n            0.476688,\n            0.535887,\n            0.595084,\n            0.653565,\n            0.710594,\n            0.765405,\n            0.817167,\n            0.864956,\n            0.907716,\n            0.944213,\n            0.972985,\n            0.992281,\n            1,\n        ]\n    elif n_levels == 40:\n        a = [\n            0,\n            2000,\n            4000,\n            6000,\n            8000,\n            9988.882838,\n            11914.52447,\n            13722.94294,\n            15369.73086,\n            16819.47627,\n            18045.18359,\n            19027.69448,\n            19755.10876,\n            20222.20531,\n            20429.86297,\n            20384.48143,\n            20097.40215,\n            19584.32924,\n            18864.75039,\n            17961.35774,\n            16899.46879,\n            15706.44732,\n            14411.12426,\n            13043.21862,\n            11632.75836,\n            10209.50134,\n            8802.356155,\n            7438.803092,\n            6144.315003,\n            4941.778213,\n            3850.913422,\n            2887.696603,\n            2063.779905,\n            1385.912553,\n            855.36175,\n            467.333577,\n            210.393894,\n            65.889243,\n            7.367743,\n            0,\n            0,\n        ]\n        b = [\n            0,\n            0,\n            0,\n            0,\n            0,\n            0.000197,\n            0.001511,\n            0.004884,\n            0.011076,\n            0.020678,\n            0.034121,\n            0.05169,\n            0.073534,\n            0.099675,\n            0.130023,\n            0.164384,\n            0.202476,\n            0.243933,\n            0.288323,\n            0.335155,\n            0.383892,\n            0.433963,\n            0.484772,\n            0.53571,\n            0.586168,\n            0.635547,\n            0.683269,\n            0.728786,\n            0.771597,\n            0.811253,\n            0.847375,\n            0.879657,\n            0.907884,\n            0.93194,\n            0.951822,\n            0.967645,\n            0.979663,\n            0.98827,\n            0.994019,\n            0.99763,\n            1,\n        ]\n    elif n_levels == 31:\n        a = [\n            0,\n            2000,\n            4000,\n            6000,\n            8000,\n            9976.135361,\n            11820.53962,\n            13431.39393,\n            14736.35691,\n            15689.20746,\n            16266.6105,\n            16465.00573,\n            16297.61933,\n            15791.5986,\n            14985.26963,\n            13925.51786,\n            12665.29166,\n            11261.22888,\n            9771.40629,\n            8253.212096,\n            6761.341326,\n            5345.91424,\n            4050.717678,\n            2911.569385,\n            1954.805296,\n            1195.889791,\n            638.148911,\n            271.626545,\n            72.063577,\n            0,\n            0,\n            0,\n        ]\n        b = [\n            0,\n            0,\n            0,\n            0,\n            0,\n            0.000391,\n            0.00292,\n            0.009194,\n            0.020319,\n            0.036975,\n            0.059488,\n            0.087895,\n            0.122004,\n            0.161442,\n            0.205703,\n            0.254189,\n            0.306235,\n            0.361145,\n            0.418202,\n            0.476688,\n            0.535887,\n            0.595084,\n            0.653565,\n            0.710594,\n            0.765405,\n            0.817167,\n            0.864956,\n            0.907716,\n            0.944213,\n            0.972985,\n            0.992281,\n            1,\n        ]\n    elif n_levels == 19:\n        a = [\n            2000,\n            4000,\n            6046.110595,\n            8267.92756,\n            10609.51323,\n            12851.10017,\n            14698.49809,\n            15861.12518,\n            16116.23661,\n            15356.92412,\n            13621.4604,\n            11101.56199,\n            8127.144155,\n            5125.141747,\n            2549.969411,\n            783.195032,\n            0,\n            0,\n            0,\n        ]\n        b = [\n            0,\n            0,\n            0.000339,\n            0.003357,\n            0.01307,\n            0.034077,\n            0.07065,\n            0.125917,\n            0.201195,\n            0.29552,\n            0.405409,\n            0.524932,\n            0.646108,\n            0.759698,\n            0.856438,\n            0.928747,\n            0.972985,\n            0.992281,\n            1,\n        ]\n    elif n_levels == 16:\n        a = [\n            0,\n            5000,\n            9890.52,\n            14166.3,\n            17346.07,\n            19121.15,\n            19371.25,\n            18164.47,\n            15742.18,\n            12488.05,\n            8881.824,\n            5437.539,\n            2626.258,\n            783.2966,\n            0,\n            0,\n            0,\n        ]\n        b = [\n            0,\n            0,\n            0.001721,\n            0.013198,\n            0.042217,\n            0.093762,\n            0.169571,\n            0.268016,\n            0.384274,\n            0.510831,\n            0.638268,\n            0.756385,\n            0.855613,\n            0.928746,\n            0.972985,\n            0.992282,\n            1,\n        ]\n    a = np.array(a).astype(\"float\")\n    b = np.array(b).astype(\"float\")\n    return a, b\n</code></pre>"},{"location":"code/era5/get_jasmin_era5/utils/#climdyn_tools.era5.get_jasmin_era5.utils.get_dp","title":"<code>get_dp(ps, n_levels)</code>","text":"<p>Returns difference in pressure between levels for ECMWF hybrid pressure levels.</p> <p>Parameters:</p> Name Type Description Default <code>ps</code> <code>Union[ndarray, float]</code> <p>Surface pressure in Pa</p> required <code>n_levels</code> <code>int</code> <p>Number of levels to provide coefficients for. Must be one of the following ECMWF level definitions:     137, 91, 62, 60, 50, 40, 31, 19, 16</p> required <p>Returns:</p> Name Type Description <code>pl</code> <code>ndarray</code> <p>Floating point array of pressure differential between full levels in units Pa (shape = (n_levels, ps.shape))</p> Source code in <code>climdyn_tools/era5/get_jasmin_era5/utils.py</code> <pre><code>def get_dp(ps: Union[np.ndarray, float], n_levels: int) -&gt; np.ndarray:\n    \"\"\"\n    Returns difference in pressure between levels for ECMWF hybrid pressure\n    levels.\n\n    Args:\n        ps: Surface pressure in Pa\n        n_levels: Number of levels to provide coefficients for. Must be one of the\n            following ECMWF level definitions:\n                137, 91, 62, 60, 50, 40, 31, 19, 16\n\n    Returns:\n        pl: Floating point array of pressure differential between full levels in\n            units Pa (shape = (n_levels, ps.shape))\n\n    \"\"\"\n    ph = get_ph(ps, n_levels)\n    dp = ph[1:] - ph[:-1]\n    return dp\n</code></pre>"},{"location":"code/era5/get_jasmin_era5/utils/#climdyn_tools.era5.get_jasmin_era5.utils.get_gz","title":"<code>get_gz(ps, gzs, T, q, n_levels)</code>","text":"<p>Calculates the geopotential on model levels for ECMWF hybrid pressure levels.</p> <p>Parameters:</p> Name Type Description Default <code>ps</code> <code>Union[ndarray, float]</code> <p>Surface pressure in Pa</p> required <code>gzs</code> <code>Union[ndarray, float]</code> <p>Surface geopotential in m2 s-2</p> required <code>T</code> <code>ndarray</code> <p>Atmospheric temperature on levels in K. Must have shape (n_levels, ps.shape)</p> required <code>q</code> <code>ndarray</code> <p>Atmospheric specific humidity on levels in kg kg**-1. Must have shape (n_levels, ps.shape)</p> required <code>n_levels</code> <code>int</code> <p>Number of levels to provide coefficients for. Must be one of the following ECMWF level definitions:     137, 91, 62, 60, 50, 40, 31, 19, 16</p> required <p>Returns:</p> Name Type Description <code>gzf</code> <code>ndarray</code> <p>Floating point array of geopotential values on full levels in units m2 s-2. Shape = (n_levels, ps.shape)</p> Source code in <code>climdyn_tools/era5/get_jasmin_era5/utils.py</code> <pre><code>def get_gz(ps: Union[np.ndarray, float], gzs: Union[np.ndarray, float], T: np.ndarray, q: np.ndarray,\n           n_levels: int) -&gt; np.ndarray:\n    \"\"\"\n    Calculates the geopotential on model levels for ECMWF hybrid pressure\n    levels.\n\n    Args:\n        ps: Surface pressure in Pa\n        gzs: Surface geopotential in m**2 s**-2\n        T: Atmospheric temperature on levels in K. Must have shape (n_levels, ps.shape)\n        q: Atmospheric specific humidity on levels in kg kg**-1.\n            Must have shape (n_levels, ps.shape)\n        n_levels: Number of levels to provide coefficients for. Must be one of the\n            following ECMWF level definitions:\n                137, 91, 62, 60, 50, 40, 31, 19, 16\n\n    Returns:\n        gzf: Floating point array of geopotential values on full levels in units m**2 s**-2.\n            Shape = (n_levels, ps.shape)\n    \"\"\"\n    # Check that the dimensions of all inputs are consistent\n    try:\n        ps_shape = ps.shape\n    except:\n        if hasattr(ps, \"__iter__\"):\n            raise Exception(\n                \"\"\"get_gz: ps argument must be a numpy array or\n                            scalar value\"\"\"\n            )\n        ps_shape = ()\n    try:\n        gzs_shape = gzs.shape\n    except:\n        if hasattr(gzs, \"__iter__\"):\n            raise Exception(\n                \"\"\"get_gz: gzs argument must be a numpy array or\n                            scalar value\"\"\"\n            )\n        gzs_shape = ()\n    if len(gzs_shape) == len(ps_shape):\n        if gzs_shape != ps_shape:\n            if len(gzs_shape) != 0 and len(ps_shape) != 0:\n                raise Exception(\n                    \"\"\"get_gz: ps and gzs inputs must have the same\n                                shape or have scalar values\"\"\"\n                )\n    elif len(gzs_shape) == (len(ps_shape) - 1):\n        if gzs_shape != ps_shape[1:]:\n            if len(gzs_shape) != 0 and len(ps_shape) != 0:\n                raise Exception(\n                    \"\"\"get_gz: ps and gzs inputs must have the same\n                                shape or have scalar values\"\"\"\n                )\n    else:\n        raise Exception(\n            \"\"\"get_gz: ps and gzs inputs must have the same\n                                shape or have scalar values\"\"\"\n        )\n    if len(ps_shape) != 0:\n        out_shape = (n_levels,) + ps_shape\n    elif len(gzs_shape) != 0:\n        out_shape = (n_levels,) + gzs_shape\n    else:\n        out_shape = (n_levels,)\n    try:\n        if T.shape != q.shape:\n            raise Exception(\n                \"\"\"get_gz: T and q arguments must have the same\n                            shape\"\"\"\n            )\n    except:\n        raise Exception(\n            \"\"\"get_gz: Cannot get shape attribute of T and/or q\n                        arguments\"\"\"\n        )\n    else:\n        for dim in T.shape:\n            if dim not in out_shape:\n                raise Exception(\n                    \"\"\"get_gz: Shape of T not compatible with ps and\n                                and gzs arguments\"\"\"\n                )\n        for dim in q.shape:\n            if dim not in out_shape:\n                raise Exception(\n                    \"\"\"get_gz: Shape of T not compatible with ps and\n                                and gzs arguments\"\"\"\n                )\n\n    # Find axis index of height dimension\n    h_axis = [i for i, ax in enumerate(out_shape) if ax not in ps_shape][0]\n    ps_reshape = list(out_shape)\n    ps_reshape[h_axis] = 1\n    ps_reshape = tuple(ps_reshape)\n\n    # Get half level pressure, full level pressure and delta p\n    ph = get_ph(ps, n_levels)\n    pl = get_pl(ps, n_levels)\n    dp = get_dp(ps, n_levels)\n    # dlogP -- change in logP\n    dlogP = np.zeros(out_shape)\n    dlogP[1:] = np.log(ph[2:] / ph[1:-1])\n    dlogP[0] = np.log(ph[1] / pl[0])\n    # alpha factor for calculating geopotential\n    alpha = 1.0 - (ph[1:] / dp) * dlogP\n    alpha[0] = np.log(2)\n    # Get moist temperature at each level\n    Tm = T * (1.0 + 0.609133 * q)\n    TRd = Tm * 287.06\n    # Get geopotential half levels\n    gzh = np.cumsum((TRd * dlogP)[::-1], axis=h_axis)[::-1] + gzs\n    gzf = gzh + TRd * alpha\n    return gzf\n</code></pre>"},{"location":"code/era5/get_jasmin_era5/utils/#climdyn_tools.era5.get_jasmin_era5.utils.get_ph","title":"<code>get_ph(ps, n_levels)</code>","text":"<p>Returns pressure on half levels for ECMWF hybrid pressure levels.</p> <p>Parameters:</p> Name Type Description Default <code>ps</code> <code>Union[ndarray, float]</code> <p>Surface pressure in Pa</p> required <code>n_levels</code> <code>int</code> <p>Number of levels to provide coefficients for. Must be one of the following ECMWF level definitions:     137, 91, 62, 60, 50, 40, 31, 19, 16</p> required <p>Returns:</p> Name Type Description <code>ph</code> <code>ndarray</code> <p>Floating point array of pressure values on half levels in units Pa (shape = (n_levels + 1, ps.shape))</p> Source code in <code>climdyn_tools/era5/get_jasmin_era5/utils.py</code> <pre><code>def get_ph(ps: Union[np.ndarray, float], n_levels: int) -&gt; np.ndarray:\n    \"\"\"\n    Returns pressure on half levels for ECMWF hybrid pressure levels.\n\n    Args:\n        ps: Surface pressure in Pa\n        n_levels: Number of levels to provide coefficients for. Must be one of the\n            following ECMWF level definitions:\n                137, 91, 62, 60, 50, 40, 31, 19, 16\n\n    Returns:\n        ph: Floating point array of pressure values on half levels in units Pa (shape = (n_levels + 1, ps.shape))\n    \"\"\"\n    a, b = get_ab(n_levels)\n    try:\n        ps_shape = ps.shape\n    except:\n        if hasattr(ps, \"__iter__\"):\n            raise Exception(\n                \"\"\"get_ph: ps argument must be a numpy array or\n                            scalar value\"\"\"\n            )\n        ps_shape = ()\n    level_shape = np.ones(len(ps_shape) + 1).astype(\"int\")\n    level_shape[0] = n_levels + 1\n    level_shape = tuple(level_shape)\n    ph = a.reshape(level_shape) + b.reshape(level_shape) * ps\n    return ph\n</code></pre>"},{"location":"code/era5/get_jasmin_era5/utils/#climdyn_tools.era5.get_jasmin_era5.utils.get_pl","title":"<code>get_pl(ps, n_levels)</code>","text":"<p>Returns pressure on full levels for ECMWF hybrid pressure levels.</p> <p>Parameters:</p> Name Type Description Default <code>ps</code> <code>Union[ndarray, float]</code> <p>Surface pressure in Pa</p> required <code>n_levels</code> <code>int</code> <p>Number of levels to provide coefficients for. Must be one of the following ECMWF level definitions:     137, 91, 62, 60, 50, 40, 31, 19, 16</p> required <p>Returns:</p> Name Type Description <code>pl</code> <code>ndarray</code> <p>Floating point array of pressure on full levels in units Pa (shape = (n_levels, ps.shape))</p> Source code in <code>climdyn_tools/era5/get_jasmin_era5/utils.py</code> <pre><code>def get_pl(ps: Union[np.ndarray, float], n_levels: int) -&gt; np.ndarray:\n    \"\"\"\n    Returns pressure on full levels for ECMWF hybrid pressure levels.\n\n    Args:\n        ps: Surface pressure in Pa\n        n_levels: Number of levels to provide coefficients for. Must be one of the\n            following ECMWF level definitions:\n                137, 91, 62, 60, 50, 40, 31, 19, 16\n\n    Returns:\n        pl: Floating point array of pressure on full levels in units Pa (shape = (n_levels, ps.shape))\n    \"\"\"\n    ph = get_ph(ps, n_levels)\n    pl = (ph[1:] + ph[:-1]) * 0.5\n    return pl\n</code></pre>"},{"location":"code/utils/base/","title":"Base","text":""},{"location":"code/utils/base/#climdyn_tools.utils.base.parse_int_list","title":"<code>parse_int_list(value, format_func=lambda x: str(x), all_values=None)</code>","text":"<p>Takes in a value or list of values e.g. <code>[1, 2, 3]</code> and converts it into a list of strings where each string has the format given by <code>format_func</code> e.g. <code>['1', '2', '3']</code> for the default case.</p> <p>There are three string options for <code>value</code>:</p> <ul> <li><code>value='x:y'</code>, will return all integers between <code>x</code> and <code>y</code> inclusive.</li> <li><code>value='firstX'</code> will return first X values of <code>all_values</code>.</li> <li><code>value='firstY'</code> will return first Y values of <code>all_values</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int, List]</code> <p>Variable to convert into list of strings</p> required <code>format_func</code> <code>Callable</code> <p>How to format each integer within the string.</p> <code>lambda x: str(x)</code> <code>all_values</code> <code>Optional[List]</code> <p>List of all possible integers, must be provided if <code>value='firstX'</code> or <code>value='firstY'</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List</code> <p>List, where each integer in <code>value</code> is converted using <code>format_func</code>.</p> Source code in <code>climdyn_tools/utils/base.py</code> <pre><code>def parse_int_list(value: Union[str, int, List], format_func: Callable = lambda x: str(x),\n                   all_values: Optional[List] = None) -&gt; List:\n    \"\"\"\n    Takes in a value or list of values e.g. `[1, 2, 3]` and converts it into a list of strings where\n    each string has the format given by `format_func` e.g. `['1', '2', '3']` for the default case.\n\n    There are three string options for `value`:\n\n    * `value='x:y'`, will return all integers between `x` and `y` inclusive.\n    * `value='firstX'` will return first X values of `all_values`.\n    * `value='firstY'` will return first Y values of `all_values`.\n\n    Args:\n        value: Variable to convert into list of strings\n        format_func: How to format each integer within the string.\n        all_values: List of all possible integers, must be provided if `value='firstX'` or `value='firstY'`.\n\n    Returns:\n        List, where each integer in `value` is converted using `format_func`.\n    \"\"\"\n    if isinstance(value, list):\n        pass\n    elif isinstance(value, int):\n        value = [value]\n    elif isinstance(value, str):\n        value = value.strip()       # remove blank space\n        # Can specify just first or last n years\n        if re.search(r'^first(\\d+)', value):\n            if all_values is None:\n                raise ValueError(f'With value={value}, must provide all_values')\n            n_req = int(re.search(r'^first(\\d+)', value).group(1))\n            if n_req &gt; len(all_values):\n                warnings.warn(f\"Requested {value} but there are only \"\n                              f\"{len(all_values)} available:\\n{all_values}\")\n            value = all_values[:n_req]\n        elif re.search(r'^last(\\d+)', value):\n            if all_values is None:\n                raise ValueError(f'With value={value}, must provide all_values')\n            n_req = int(re.search(r'^last(\\d+)', all_values).group(1))\n            if n_req &gt; len(all_values):\n                warnings.warn(f\"Requested {value} but there are only \"\n                              f\"{len(all_values)} available:\\n{all_values}\")\n            value = all_values[-n_req:]\n        elif ':' in value:\n            # If '1979:2023' returns all integers from 1979 to 2023\n            start, end = map(int, value.split(':'))\n            value = list(range(start, end + 1))\n        else:\n            value = [int(value)]\n    else:\n        raise ValueError(f\"Unsupported format: {value}\")\n    return [format_func(i) for i in value]\n</code></pre>"},{"location":"code/utils/base/#climdyn_tools.utils.base.round_any","title":"<code>round_any(x, base, round_type='round')</code>","text":"<p>Rounds <code>x</code> to the nearest multiple of <code>base</code> with the rounding done according to <code>round_type</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[float, ndarray]</code> <p>Number or array to round.</p> required <code>base</code> <code>float</code> <p>Rounds <code>x</code> to nearest integer multiple of value of <code>base</code>.</p> required <code>round_type</code> <code>str</code> <p>One of the following, indicating how to round <code>x</code>:</p> <ul> <li><code>'round'</code></li> <li><code>'ceil'</code></li> <li><code>'float'</code></li> </ul> <code>'round'</code> <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Rounded version of <code>x</code>.</p> Example <pre><code>round_any(3, 5) = 5\nround_any(3, 5, 'floor') = 0\n</code></pre> Source code in <code>climdyn_tools/utils/base.py</code> <pre><code>def round_any(x: Union[float, np.ndarray], base: float, round_type: str = 'round') -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Rounds `x` to the nearest multiple of `base` with the rounding done according to `round_type`.\n\n    Args:\n        x: Number or array to round.\n        base: Rounds `x` to nearest integer multiple of value of `base`.\n        round_type: One of the following, indicating how to round `x`:\n\n            - `'round'`\n            - `'ceil'`\n            - `'float'`\n\n    Returns:\n        Rounded version of `x`.\n\n    Example:\n        ```\n        round_any(3, 5) = 5\n        round_any(3, 5, 'floor') = 0\n        ```\n    \"\"\"\n    if round_type == 'round':\n        return base * np.round(x / base)\n    elif round_type == 'ceil':\n        return base * np.ceil(x / base)\n    elif round_type == 'floor':\n        return base * np.floor(x / base)\n    else:\n        raise ValueError(f\"round_type specified was {round_type} but it should be one of the following:\\n\"\n                         f\"round, ceil, floor\")\n</code></pre>"},{"location":"code/utils/base/#climdyn_tools.utils.base.split_list_max_n","title":"<code>split_list_max_n(lst, n)</code>","text":"<p>Split <code>lst</code> into balanced chunks with at most <code>n</code> elements each.</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>Union[List, ndarray]</code> <p>List to split.</p> required <code>n</code> <code>int</code> <p>Maximum number of elements in each chunk of <code>lst</code>.</p> required <p>Returns:</p> Type Description <code>List</code> <p>List of <code>n</code> chunks of <code>lst</code></p> Source code in <code>climdyn_tools/utils/base.py</code> <pre><code>def split_list_max_n(lst: Union[List, np.ndarray], n: int) -&gt; List:\n    \"\"\"\n    Split `lst` into balanced chunks with at most `n` elements each.\n\n    Args:\n        lst: List to split.\n        n: Maximum number of elements in each chunk of `lst`.\n\n    Returns:\n        List of `n` chunks of `lst`\n    \"\"\"\n    k = int(np.ceil(len(lst) / n))  # Number of chunks needed\n    avg = int(np.ceil(len(lst) / k))\n    return [lst[i * avg : (i + 1) * avg] for i in range(k)]\n</code></pre>"},{"location":"code/utils/ds_slicing/","title":"Dataset Slicing","text":""},{"location":"code/utils/ds_slicing/#climdyn_tools.utils.ds_slicing.area_weight_mean_lat","title":"<code>area_weight_mean_lat(ds)</code>","text":"<p>For all variables in <code>ds</code>, an area-weighted mean is taken over all latitudes in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Dataset for a particular experiment.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset containing averaged variables with no latitude dependence.</p> Source code in <code>climdyn_tools/utils/ds_slicing.py</code> <pre><code>def area_weight_mean_lat(ds: Dataset) -&gt; Dataset:\n    \"\"\"\n    For all variables in `ds`, an area-weighted mean is taken over all latitudes in the dataset.\n\n    Args:\n        ds: Dataset for a particular experiment.\n\n    Returns:\n        Dataset containing averaged variables with no latitude dependence.\n    \"\"\"\n    var_averaged = []\n    for var in ds.keys():\n        if 'lat' in list(ds[var].coords):\n            ds[var] = area_weighting(ds[var]).mean(dim='lat')\n            var_averaged += [var]\n    print(f\"Variables Averaged: {var_averaged}\")\n    return ds\n</code></pre>"},{"location":"code/utils/ds_slicing/#climdyn_tools.utils.ds_slicing.area_weighting","title":"<code>area_weighting(var, weights=None)</code>","text":"<p>Apply area weighting to the variable <code>var</code> using the <code>cosine</code> of latitude: \\(\\cos (\\phi)\\).</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>DataArray</code> <p>Variable to weight e.g. <code>ds.t_surf</code> to weight the surface temperature, where <code>ds</code> is the dataset for the experiment which contains all variables.</p> required <code>weights</code> <code>Optional[DataArray]</code> <p>Weights to use as a function of latitude. If not given, will just take cosine of latitude.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataArrayWeighted</code> <p>Area weighted version of <code>var</code>.</p> Source code in <code>climdyn_tools/utils/ds_slicing.py</code> <pre><code>def area_weighting(var: xr.DataArray, weights: Optional[DataArray] = None) -&gt; DataArrayWeighted:\n    \"\"\"\n    Apply area weighting to the variable `var` using the `cosine` of latitude: $\\\\cos (\\\\phi)$.\n\n    Args:\n        var: Variable to weight e.g. `ds.t_surf` to weight the surface temperature, where\n            `ds` is the dataset for the experiment which contains all variables.\n        weights: Weights to use as a function of latitude. If not given, will just take cosine of latitude.\n\n    Returns:\n        Area weighted version of `var`.\n    \"\"\"\n    if weights is None:\n        weights = np.cos(np.deg2rad(var.lat))\n        weights.name = \"weights\"\n    return var.weighted(weights)\n</code></pre>"},{"location":"code/utils/ds_slicing/#climdyn_tools.utils.ds_slicing.lat_lon_coord_slice","title":"<code>lat_lon_coord_slice(ds, lat, lon)</code>","text":"<p>Returns dataset, <code>ds</code>, keeping only data at the coordinate indicated by <code>(lat[i], lon[i])</code> for all <code>i</code>.</p> <p>If <code>ds</code> contained <code>t_surf</code> then the returned dataset would contain <code>t_surf</code> as a function of the variables <code>time</code> and <code>location</code> with each value of <code>location</code> corresponding to a specific <code>(lat, lon)</code> combination. For the original <code>ds</code>, it would be a function of <code>time</code>, <code>lat</code> and <code>lon</code>.</p> <p>This is inspired by a stack overflow post.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Dataset for a particular experiment.</p> required <code>lat</code> <code>ndarray</code> <p><code>float [n_coords]</code> Latitude coordinates to keep.</p> required <code>lon</code> <code>ndarray</code> <p><code>float [n_coords]</code> Longitude coordinates to keep.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset only including the desired coordinates.</p> Source code in <code>climdyn_tools/utils/ds_slicing.py</code> <pre><code>def lat_lon_coord_slice(ds: Dataset, lat: np.ndarray, lon: np.ndarray) -&gt; Dataset:\n    \"\"\"\n    Returns dataset, `ds`, keeping only data at the coordinate indicated by `(lat[i], lon[i])` for all `i`.\n\n    If `ds` contained `t_surf` then the returned dataset would contain `t_surf` as a function of the variables\n    `time` and `location` with each value of `location` corresponding to a specific `(lat, lon)` combination.\n    For the original `ds`, it would be a function of `time`, `lat` and `lon`.\n\n    This is inspired by a\n    [stack overflow post](https://stackoverflow.com/questions/72179103/xarray-select-the-data-at-specific-x-and-y-coordinates).\n\n    Args:\n        ds: Dataset for a particular experiment.\n        lat: `float [n_coords]`\n            Latitude coordinates to keep.\n        lon: `float [n_coords]`\n            Longitude coordinates to keep.\n\n    Returns:\n        Dataset only including the desired coordinates.\n    \"\"\"\n    # To get dataset at specific coordinates, not all permutations, turn to xarray first\n    lat_xr = xr.DataArray(lat, dims=['location'])\n    lon_xr = xr.DataArray(lon, dims=['location'])\n    return ds.sel(lat=lat_xr, lon=lon_xr, method=\"nearest\")\n</code></pre>"},{"location":"code/utils/ds_slicing/#climdyn_tools.utils.ds_slicing.lat_lon_rolling","title":"<code>lat_lon_rolling(ds, window_lat, window_lon)</code>","text":"<p>This creates a rolling averaged version of the dataset or data-array in the spatial dimension. Returned data will have first <code>np.ceil((window_lat-1)/2)</code> and last <code>np.floor((window_lat-1)/2)</code> values as <code>nan</code> in latitude dimension. The averaging also does not take account of area weighting in latitude dimension.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Union[Dataset, DataArray]</code> <p>Dataset or DataArray to find rolling mean of.</p> required <code>window_lat</code> <code>int</code> <p>Size of window for rolling average in latitude dimension [number of grid points]</p> required <code>window_lon</code> <code>int</code> <p>Size of window for rolling average in longitude dimension [number of grid points].</p> required <p>Returns:</p> Type Description <code>Union[Dataset, DataArray]</code> <p>Rolling averaged dataset or DataArray.</p> Source code in <code>climdyn_tools/utils/ds_slicing.py</code> <pre><code>def lat_lon_rolling(ds: Union[Dataset, DataArray], window_lat: int, window_lon: int) -&gt; Union[Dataset, DataArray]:\n    \"\"\"\n    This creates a rolling averaged version of the dataset or data-array in the spatial dimension.\n    Returned data will have first `np.ceil((window_lat-1)/2)` and last `np.floor((window_lat-1)/2)`\n    values as `nan` in latitude dimension.\n    The averaging also does not take account of area weighting in latitude dimension.\n\n    Args:\n        ds: Dataset or DataArray to find rolling mean of.\n        window_lat: Size of window for rolling average in latitude dimension [number of grid points]\n        window_lon: Size of window for rolling average in longitude dimension [number of grid points].\n\n    Returns:\n        Rolling averaged dataset or DataArray.\n\n    \"\"\"\n    ds_roll = ds.pad(lon=window_lon, mode='wrap')       # first pad in lon so wraps around when doing rolling mean\n    ds_roll = ds_roll.rolling({'lon': window_lon, 'lat': window_lat}, center=True).mean()\n    return ds_roll.isel(lon=slice(window_lon, -window_lon))     # remove the padded longitude values\n</code></pre>"},{"location":"code/utils/ds_slicing/#climdyn_tools.utils.ds_slicing.time_rolling","title":"<code>time_rolling(ds, window_time, wrap=True)</code>","text":"<p>This creates a rolling-averaged version of the dataset or data-array in the time dimension. Useful for when you have an annual average dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Union[Dataset, DataArray]</code> <p>Dataset or DataArray to find rolling mean of.</p> required <code>window_time</code> <code>int</code> <p>Size of window for rolling average in time dimension [number of time units e.g. days]</p> required <code>wrap</code> <code>bool</code> <p>If the first time comes immediately after the last time i.e. for annual mean data</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[Dataset, DataArray]</code> <p>Rolling averaged dataset or DataArray.</p> Source code in <code>climdyn_tools/utils/ds_slicing.py</code> <pre><code>def time_rolling(ds: Union[Dataset, DataArray], window_time: int, wrap: bool = True) -&gt; Union[Dataset, DataArray]:\n    \"\"\"\n    This creates a rolling-averaged version of the dataset or data-array in the time dimension. Useful for when\n    you have an annual average dataset.\n\n    Args:\n        ds: Dataset or DataArray to find rolling mean of.\n        window_time: Size of window for rolling average in time dimension [number of time units e.g. days]\n        wrap: If the first time comes immediately after the last time i.e. for annual mean data\n\n    Returns:\n        Rolling averaged dataset or DataArray.\n    \"\"\"\n    if wrap:\n        ds_roll = ds.pad(time=window_time, mode='wrap')  # first pad in time so wraps around when doing rolling mean\n        ds_roll = ds_roll.rolling(time=window_time, center=True).mean()\n        return ds_roll.isel(time=slice(window_time, -window_time))  # remove the padded time values\n    else:\n        return ds.rolling(time=window_time, center=True).mean()\n</code></pre>"},{"location":"code/utils/xarray/","title":"Xarray","text":""},{"location":"code/utils/xarray/#climdyn_tools.utils.xarray.print_ds_var_list","title":"<code>print_ds_var_list(ds, phrase=None)</code>","text":"<p>Prints all variables in <code>ds</code> which contain <code>phrase</code> in the variable name or variable <code>long_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Dataset to investigate variables of.</p> required <code>phrase</code> <code>Optional[str]</code> <p>Key phrase to search for in variable info.</p> <code>None</code> Source code in <code>climdyn_tools/utils/xarray.py</code> <pre><code>def print_ds_var_list(ds: xr.Dataset, phrase: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Prints all variables in `ds` which contain `phrase` in the variable name or variable `long_name`.\n\n    Args:\n        ds: Dataset to investigate variables of.\n        phrase: Key phrase to search for in variable info.\n\n    \"\"\"\n    # All the exceptions to deal with case when var does not have a long_name\n    var_list = list(ds.keys())\n    if phrase is None:\n        for var in var_list:\n            try:\n                print(f'{var}: {ds[var].long_name}')\n            except AttributeError:\n                print(f'{var}')\n    else:\n        for var in var_list:\n            if phrase.lower() in var.lower():\n                try:\n                    print(f'{var}: {ds[var].long_name}')\n                except AttributeError:\n                    print(f'{var}')\n                continue\n            try:\n                if phrase.lower() in ds[var].long_name.lower():\n                    print(f'{var}: {ds[var].long_name}')\n                    continue\n            except AttributeError:\n                continue\n    return None\n</code></pre>"},{"location":"code/utils/xarray/#climdyn_tools.utils.xarray.set_attrs","title":"<code>set_attrs(var, overwrite=True, **kwargs)</code>","text":"<p>Set attributes of a given variable.</p> <p>Examples:</p> <p><code>set_attrs(ds.plev, long_name='pressure', units='Pa')</code></p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>DataArray</code> <p>Variable to set attributes of.</p> required <code>overwrite</code> <code>bool</code> <p>If <code>True</code>, overwrite existing attributes, otherwise leave unchanged.</p> <code>True</code> <code>**kwargs</code> <code>str</code> <p>Attributes to set. Common ones include <code>long_name</code> and <code>units</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>DataArray</code> <p><code>var</code> with attributes set.</p> Source code in <code>climdyn_tools/utils/xarray.py</code> <pre><code>def set_attrs(var: xr.DataArray, overwrite: bool = True, **kwargs: str) -&gt; xr.DataArray:\n    \"\"\"\n    Set attributes of a given variable.\n\n    Examples:\n        `set_attrs(ds.plev, long_name='pressure', units='Pa')`\n\n    Args:\n        var: Variable to set attributes of.\n        overwrite: If `True`, overwrite existing attributes, otherwise leave unchanged.\n        **kwargs: Attributes to set. Common ones include `long_name` and `units`\n\n    Returns:\n        `var` with attributes set.\n    \"\"\"\n    # Function to set main attributes of given variable\n    for key in kwargs:\n        if (key in var.attrs) and not overwrite:\n            continue\n        var.attrs[key] = kwargs[key]\n    return var\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>To contribute to the website, one can follow these steps:</p> <ol> <li>Ensure your personal Github profile is a member of the Climate Dynamics Lab organization.</li> <li>Clone the Github repository, and <code>cd</code> into it.</li> <li>Install the dependencies by running <code>pip install \".[docs]\"</code></li> <li>Within terminal in the directory of the repo, run <code>mkdocs serve</code> (or possibly <code>python -m mkdocs serve</code> on Windows) to see in live time the effect  of changes to the website. Press <code>Ctrl+C</code> to exit. </li> <li>Make changes to the relevant file within the <code>docs</code> directory, or add new files. </li> <li>If you add new files, update <code>mkdocs.yml</code> to ensure the correct structure.</li> <li>Push changes to Github, and the website should update automatically.</li> </ol> <p>The website was created using MkDocs, more information on which can be found here.</p>"},{"location":"contributing/#code","title":"Code","text":"<p>To contribute code to the <code>climdyn_tools</code> package, one can follow these steps:</p> <ul> <li>Add the code (e.g. <code>.py</code> file) to the <code>climdyn_tools</code> directory of the GitHub repository.</li> <li>Add a corresponding <code>.md</code> file to the <code>docs</code> directory of the repository. The comments from the functions can be added to the documentation by adding  a line of text to the <code>.md</code> file e.g. <code>::: climdyn_tools.ceda_esgf.base</code> for the CEDA/ESGF functions.</li> <li>For this to work, the comments of the functions need to be in the Google Docstring format. You should be able to set this automatically in your IDE.</li> </ul> <p>When updating comments in the code file, the documentation will not update in live time.You need to rerun <code>mkdocs serve</code> to see the changes.</p>"},{"location":"contributing/mkdocs/","title":"MkDocs","text":"<p>MkDocs is a package which produces a website based on a sequence of markdown pages. It also automatically produces documentation from code comments.</p>"},{"location":"contributing/mkdocs/#resources","title":"Resources","text":"<ul> <li>Material for MkDocs: Great website with instructions of how to make and publish website as well as how to make website look nice.</li> <li>MkDocs: Official website</li> <li>MkDocs - Jupyter: How to make a web page from a Jupyter notebook (<code>.ipynb</code>) or Python script (<code>.py</code>).</li> <li>Tutorial: Useful step by step instructions.</li> </ul>"},{"location":"contributing/mkdocs/#installation","title":"Installation","text":"<p>To install MkDocs, run: <pre><code>pip install mkdocs-material\npip install mkdocstrings-python\n</code></pre></p> Warning <p>In Windows, you need to preface this with <code>python -m</code></p> <p>This should all be done when installing the dependencies with <code>pip install \".[docs]\"</code></p>"},{"location":"contributing/mkdocs/#create-website","title":"Create website","text":"<p>In the repository, create a <code>docs</code> folder containing some Markdown documents corresponding to the website pages and a <code>mkdocs.yml</code> file.</p> <p>The structure should be something like: <pre><code>.\n\u251c\u2500 docs/\n\u2502  \u2514\u2500 index.md\n   \u2514\u2500 section1/\n      \u2514\u2500 page1.md\n      \u2514\u2500 page2.md\n\u2514\u2500 mkdocs.yml\n</code></pre></p> <p>There always needs to be a page called <code>index.md</code> as this is the starting page of the website.</p> <p>The <code>mkdocs.yml</code> file specifies the colour of the website and generally how it looks as well as how the pages of the website are arranged. It must contain the lines <pre><code>theme:\n  name: material\n</code></pre></p> <p>It also specifies the order of the pages in the website through the <code>nav</code> section.</p> Example <code>mkdocs.yml</code> file for the above <code>docs</code> folder structure <pre><code>site_name: test_website\nrepo_url: https://github.com/jduffield65/test_website  # provides link to github website\nrepo_name: jduffield65/test_website\n\ntheme:\n  name: material\n  # 404 page\n  static_templates:\n      - 404.html\n  palette:\n    primary: black   # specify colour of website\n  # Necessary for search to work properly\n  include_search_page: false\n  search_index_only: true\n\n  # Default values, taken from mkdocs_theme.yml\n  language: en\n  features:\n    - navigation.tabs\n    - navigation.tabs.sticky\n    - navigation.indexes\n    - navigation.expand\n    - content.tabs.link\n    - navigation.sections\n    # - toc.integrate  # This puts table of contents in right sidebar into left sidebar but makes left sidebar quite big\n    - navigation.top\n  font:\n      text: Roboto\n      code: Roboto Mono\n  icon:\n      logo: logo\n\nplugins:\n  - search\n    - mkdocstrings:\n          default_handler: python\n          handlers:\n            python:\n              options:\n                show_root_toc_entry: false # stops extra heading in contents of Code pages\n\nextra:\n  generator: false\n  social:\n    - icon: fontawesome/brands/github\n      link: https://github.com/jduffield65/test_website\n      name: Github Repository\n\nmarkdown_extensions:\n  - admonition\n    - pymdownx.details\n    - pymdownx.superfences\n    - tables\n    - attr_list\n    - md_in_html\n    - def_list\n    - pymdownx.tabbed:\n        alternate_style: true\n    - pymdownx.arithmatex:\n        generic: true\n\nextra_javascript:\n  - javascripts/mathjax.js   # allows you to put use latex maths equations.\n    - https://polyfill.io/v3/polyfill.min.js?features=es6\n    - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\n\nnav:\n    - Home: index.md\n    - Section 1:\n        - Page 1: section1/page1.md\n        - Page 2: section1/page2.md\n</code></pre> <p>To view what the website looks like, run <code>mkdocs serve</code> in terminal with the current directory being your repository.</p> Warning <p>Again, in Windows, you need to preface this with <code>python -m</code>.</p>"},{"location":"contributing/mkdocs/#publish-website","title":"Publish website","text":"<p>To publish the website, the repository needs to be on Github.</p> <p>Within the Github repository, create a file called <code>mkdocs_deploy.yml</code> in <code>.github/workflows</code>.  This is an implementation of Github actions meaning that whenever you push to this repository in Github,  it will run the instructions in this <code>mkdocs_deploy.yml</code> file to update the website.</p> Example <code>mkdocs_deploy.yml</code> file <pre><code>name: Publish docs via GitHub Pages\non:\n  push:\n    branches:\n      - main\njobs:\n  build:\n    name: Deploy docs\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout main\n        uses: actions/checkout@v2\n\n      - name: Set up Python 3.9\n        uses: actions/setup-python@v2\n        with:\n          python-version: 3.9\n\n      - name: Install dependencies\n        run: pip install \\\n          mkdocs-material\n          mkdocstrings[python]\n\n      - name: Deploy docs\n        run: mkdocs gh-deploy --force\n</code></pre> <p>The first time you push with <code>mkdocs_deploy.yml</code>, a new branch called <code>gh-pages</code> will be added to the repository.</p> <p>After this, you can go to <code>settings/Pages</code> of the GitHub repository and change source to <code>Deploy from a branch</code>, and select the branch <code>gh-pages/root</code>.</p> <p>The website should then appear at the indicated address.</p> <p>Once this is set up once, it should not need to be modified again, the website should automatically update everytime something is pushed to the main branch.</p>"},{"location":"data/","title":"Data","text":"<p>We have a group workspace on JASMIN at <code>/gws/nopw/j04/global_ex</code>.</p>"},{"location":"data/cesm/","title":"CESM","text":""},{"location":"data/cmip/","title":"CMIP","text":""},{"location":"data/cmip/CEDAESGF_Wrapper/","title":"CEDA/ESGF","text":"In\u00a0[1]: Copied! <pre>import os, intake_esgf\nimport glob\nimport xarray as xr\nimport numpy as np\nimport cftime\nimport pandas as pd\nfrom intake_esgf import ESGFCatalog\nfrom importlib import reload  # Python 3.4+\nimport matplotlib.pyplot as plt\nimport climdyn_tools.ceda_esgf.base as CEFunc\n# import CEDAESGF_Funcs as CEFunc\n\nreload(CEFunc)\n</pre> import os, intake_esgf import glob import xarray as xr import numpy as np import cftime import pandas as pd from intake_esgf import ESGFCatalog from importlib import reload  # Python 3.4+ import matplotlib.pyplot as plt import climdyn_tools.ceda_esgf.base as CEFunc # import CEDAESGF_Funcs as CEFunc  reload(CEFunc)  Out[1]: <pre>&lt;module 'climdyn_tools.ceda_esgf.base' from '/Users/joshduffield/Documents/StAndrews/Wiki/climdyn_tools/ceda_esgf/base.py'&gt;</pre> In\u00a0[250]: Copied! <pre>#### Config\nactivity_id = 'CMIP'\nexperiment_id = 'abrupt-4xCO2'\nsource_id = \"GFDL-CM4\"\ndo1member = True\n\n# CMIP variables of interest\nvariableList = ['tas', 'huss', 'rsds', 'rsus', 'rlds', 'rlus', 'hfls', 'hfss']\ntable_id = 'Amon'  ## Time step\n\n### Change to personal cache's\nintake_esgf.conf.set(local_cache=\"/gws/nopw/j04/global_ex/chingosa/cache\")\nintake_esgf.conf.set(indices={ ### Sets up which nodes it looks at - can be fiddly\n    \"esgf-node.llnl.gov\": False,\n    \"esgf-node.ornl.gov\": True,\n    \"esgf.ceda.ac.uk\": True,\n    \"anl-dev\": True,\n    \"ornl-dev\": True,\n    \"ESGF2-US-1.5-Catalog\": True,\n    \"esgf-data.dkrz.de\": True,\n    \"esgf-node.ipsl.upmc.fr\": True,\n    \"esg-dn1.nsc.liu.se\": True,\n    \"esgf.nci.org.au\": True,\n})\n\n### Meta Data\nCMIP6Meta = CEFunc.load_cmip6_source_id()                             #JSON of CMIP6 Meta Data\nsource_id_list = CEFunc.source_id_in_activity(activity_id, CMIP6Meta) # Source_id_list of models participatingin activity_id - use this for looping over source_ids\nM2I = CEFunc.getModel_to_inst(CMIP6Meta)                              # Dictionary linking institution name (for CEDA) to source_id\n\n# Figure out what CEDA/ESGF have\nCEDA_res = CEFunc.checkCEDA(source_id, activity_id, experiment_id, M2I, variableList, table_id, member_id = '*')\nESGF_res = CEFunc.checkESGF(source_id, activity_id, experiment_id, M2I, variableList, table_id, member_id = '*')\n\n# Find Overlapping Variants and variables that make sense to grab\npivot = CEFunc.compare_cat_res_pivot(ESGF_res.df, CEDA_res)\nranking = CEFunc.rank_members_with_vars(pivot)\nsensible_members = ranking[ranking.CEDA_CHOICE_count == ranking.CEDA_CHOICE_count.iloc[0]]\n\n# Which variables are we getting from each source\nESGF_vars = sensible_members.ESGF_vars.iloc[0]\n\nprint(f'There are like {len(sensible_members)} that would make sense to analyse but will have to download {len(ESGF_vars)} of {len(variableList)} variables from ESGF')\n\n# If looping through the sensible members heres where we decide which ones we are doing\nif do1member: member_ids = [sensible_members.member_id.iloc[0]]\nelse:         member_ids = sensible_members.member_id\n\n### Loop through Members\nfor member_id in member_ids:\n    print(f'Starting Analysis for {member_id}')\n    \n    row = sensible_members[sensible_members.member_id == member_id].reset_index(drop=True)\n    # Which variables are we getting from each source\n    CEDA_vars = row.CEDA_vars.item()\n    ESGF_vars = row.ESGF_vars.item()\n\n    # if there are multiple sources - finds and divies them up and combines into one thing...\n    ds = CEFunc.getCombinedData(source_id, activity_id, experiment_id, M2I,CEDA_vars, ESGF_vars, table_id, member_id, doReadOut = True)\n\n    ## do whatever you need to do with ds\n</pre> #### Config activity_id = 'CMIP' experiment_id = 'abrupt-4xCO2' source_id = \"GFDL-CM4\" do1member = True  # CMIP variables of interest variableList = ['tas', 'huss', 'rsds', 'rsus', 'rlds', 'rlus', 'hfls', 'hfss'] table_id = 'Amon'  ## Time step  ### Change to personal cache's intake_esgf.conf.set(local_cache=\"/gws/nopw/j04/global_ex/chingosa/cache\") intake_esgf.conf.set(indices={ ### Sets up which nodes it looks at - can be fiddly     \"esgf-node.llnl.gov\": False,     \"esgf-node.ornl.gov\": True,     \"esgf.ceda.ac.uk\": True,     \"anl-dev\": True,     \"ornl-dev\": True,     \"ESGF2-US-1.5-Catalog\": True,     \"esgf-data.dkrz.de\": True,     \"esgf-node.ipsl.upmc.fr\": True,     \"esg-dn1.nsc.liu.se\": True,     \"esgf.nci.org.au\": True, })  ### Meta Data CMIP6Meta = CEFunc.load_cmip6_source_id()                             #JSON of CMIP6 Meta Data source_id_list = CEFunc.source_id_in_activity(activity_id, CMIP6Meta) # Source_id_list of models participatingin activity_id - use this for looping over source_ids M2I = CEFunc.getModel_to_inst(CMIP6Meta)                              # Dictionary linking institution name (for CEDA) to source_id  # Figure out what CEDA/ESGF have CEDA_res = CEFunc.checkCEDA(source_id, activity_id, experiment_id, M2I, variableList, table_id, member_id = '*') ESGF_res = CEFunc.checkESGF(source_id, activity_id, experiment_id, M2I, variableList, table_id, member_id = '*')  # Find Overlapping Variants and variables that make sense to grab pivot = CEFunc.compare_cat_res_pivot(ESGF_res.df, CEDA_res) ranking = CEFunc.rank_members_with_vars(pivot) sensible_members = ranking[ranking.CEDA_CHOICE_count == ranking.CEDA_CHOICE_count.iloc[0]]  # Which variables are we getting from each source ESGF_vars = sensible_members.ESGF_vars.iloc[0]  print(f'There are like {len(sensible_members)} that would make sense to analyse but will have to download {len(ESGF_vars)} of {len(variableList)} variables from ESGF')  # If looping through the sensible members heres where we decide which ones we are doing if do1member: member_ids = [sensible_members.member_id.iloc[0]] else:         member_ids = sensible_members.member_id  ### Loop through Members for member_id in member_ids:     print(f'Starting Analysis for {member_id}')          row = sensible_members[sensible_members.member_id == member_id].reset_index(drop=True)     # Which variables are we getting from each source     CEDA_vars = row.CEDA_vars.item()     ESGF_vars = row.ESGF_vars.item()      # if there are multiple sources - finds and divies them up and combines into one thing...     ds = CEFunc.getCombinedData(source_id, activity_id, experiment_id, M2I,CEDA_vars, ESGF_vars, table_id, member_id, doReadOut = True)      ## do whatever you need to do with ds <pre>Attempt 1 to initialize ESGFCatalog...\nESGFCatalog successfully initialized.\n</pre> <pre>   Searching indices:   0%|          |0/9 [       ?index/s]</pre> <pre>/home/users/chingosa/.local/lib/python3.11/site-packages/intake_esgf/catalog.py:316: UserWarning: SolrESGFIndex('esgf-node.ornl.gov') failed to return a response, results may be incomplete\n  warnings.warn(\n</pre> <pre>There are like 1 that would make sense to analyse but will have to download 2 of 8 variables from ESGF\nStarting Analysis for r1i1p1f1\nAttempt 1 to initialize ESGFCatalog...\nESGFCatalog successfully initialized.\n</pre> <pre>   Searching indices:   0%|          |0/9 [       ?index/s]</pre> <pre>/home/users/chingosa/.local/lib/python3.11/site-packages/intake_esgf/catalog.py:316: UserWarning: SolrESGFIndex('esgf-node.ornl.gov') failed to return a response, results may be incomplete\n  warnings.warn(\n</pre> <pre>Get file information:   0%|          |0/9 [       ?index/s]</pre> <pre>/home/users/chingosa/.local/lib/python3.11/site-packages/intake_esgf/catalog.py:468: UserWarning: SolrESGFIndex('esgf-node.ornl.gov') failed to return a response, info may be incomplete\n  warnings.warn(\n</pre> <pre>Downloading 499.5 [Mb]...\n</pre> <pre>rlus_Amon_GFDL-CM4_abrupt-4xCO2_r1i1p...:   0%|          |0.00/160M [?B/s]</pre> <pre>rlus_Amon_GFDL-CM4_abrupt-4xCO2_r1i1p...:   0%|          |0.00/79.8M [?B/s]</pre> <pre>rsus_Amon_GFDL-CM4_abrupt-4xCO2_r1i1p...:   0%|          |0.00/173M [?B/s]</pre> <pre>rsus_Amon_GFDL-CM4_abrupt-4xCO2_r1i1p...:   0%|          |0.00/86.7M [?B/s]</pre> In\u00a0[251]: Copied! <pre>ds\n</pre> ds Out[251]: <pre>&lt;xarray.Dataset&gt; Size: 3GB\nDimensions:    (time: 1800, lat: 180, lon: 288, bnds: 2)\nCoordinates:\n  * bnds       (bnds) float64 16B 1.0 2.0\n  * lat        (lat) float64 1kB -89.5 -88.5 -87.5 -86.5 ... 86.5 87.5 88.5 89.5\n  * lon        (lon) float64 2kB 0.625 1.875 3.125 4.375 ... 356.9 358.1 359.4\n  * time       (time) object 14kB 0001-01-16 12:00:00 ... 0150-12-16 12:00:00\n    height     float64 8B 2.0\nData variables:\n    hfls       (time, lat, lon) float32 373MB dask.array&lt;chunksize=(1, 180, 288), meta=np.ndarray&gt;\n    lat_bnds   (time, lat, bnds) float64 5MB dask.array&lt;chunksize=(1200, 180, 2), meta=np.ndarray&gt;\n    lon_bnds   (time, lon, bnds) float64 8MB dask.array&lt;chunksize=(1200, 288, 2), meta=np.ndarray&gt;\n    time_bnds  (time, bnds) object 29kB dask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;\n    hfss       (time, lat, lon) float32 373MB dask.array&lt;chunksize=(1, 180, 288), meta=np.ndarray&gt;\n    huss       (time, lat, lon) float32 373MB dask.array&lt;chunksize=(1, 180, 288), meta=np.ndarray&gt;\n    rlds       (time, lat, lon) float32 373MB dask.array&lt;chunksize=(1, 180, 288), meta=np.ndarray&gt;\n    rsds       (time, lat, lon) float32 373MB dask.array&lt;chunksize=(1, 180, 288), meta=np.ndarray&gt;\n    tas        (time, lat, lon) float32 373MB dask.array&lt;chunksize=(1, 180, 288), meta=np.ndarray&gt;\n    rlus       (time, lat, lon) float32 373MB dask.array&lt;chunksize=(1, 180, 288), meta=np.ndarray&gt;\n    rsus       (time, lat, lon) float32 373MB dask.array&lt;chunksize=(1, 180, 288), meta=np.ndarray&gt;\nAttributes: (12/46)\n    external_variables:     areacella\n    history:                File was processed by fremetar (GFDL analog of CM...\n    table_id:               Amon\n    activity_id:            CMIP\n    branch_method:          standard\n    branch_time_in_child:   0.0\n    ...                     ...\n    variable_id:            hfls\n    variant_info:           N/A\n    references:             see further_info_url attribute\n    variant_label:          r1i1p1f1\n    branch_time_in_parent:  36500.0\n    parent_time_units:      days since 0001-1-1</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 1800</li><li>lat: 180</li><li>lon: 288</li><li>bnds: 2</li></ul></li><li>Coordinates: (5)<ul><li>bnds(bnds)float641.0 2.0long_name :vertex number<pre>array([1., 2.])</pre></li><li>lat(lat)float64-89.5 -88.5 -87.5 ... 88.5 89.5long_name :latitudeunits :degrees_northaxis :Ybounds :lat_bndsstandard_name :latitudecell_methods :time: point<pre>array([-89.5, -88.5, -87.5, -86.5, -85.5, -84.5, -83.5, -82.5, -81.5, -80.5,\n       -79.5, -78.5, -77.5, -76.5, -75.5, -74.5, -73.5, -72.5, -71.5, -70.5,\n       -69.5, -68.5, -67.5, -66.5, -65.5, -64.5, -63.5, -62.5, -61.5, -60.5,\n       -59.5, -58.5, -57.5, -56.5, -55.5, -54.5, -53.5, -52.5, -51.5, -50.5,\n       -49.5, -48.5, -47.5, -46.5, -45.5, -44.5, -43.5, -42.5, -41.5, -40.5,\n       -39.5, -38.5, -37.5, -36.5, -35.5, -34.5, -33.5, -32.5, -31.5, -30.5,\n       -29.5, -28.5, -27.5, -26.5, -25.5, -24.5, -23.5, -22.5, -21.5, -20.5,\n       -19.5, -18.5, -17.5, -16.5, -15.5, -14.5, -13.5, -12.5, -11.5, -10.5,\n        -9.5,  -8.5,  -7.5,  -6.5,  -5.5,  -4.5,  -3.5,  -2.5,  -1.5,  -0.5,\n         0.5,   1.5,   2.5,   3.5,   4.5,   5.5,   6.5,   7.5,   8.5,   9.5,\n        10.5,  11.5,  12.5,  13.5,  14.5,  15.5,  16.5,  17.5,  18.5,  19.5,\n        20.5,  21.5,  22.5,  23.5,  24.5,  25.5,  26.5,  27.5,  28.5,  29.5,\n        30.5,  31.5,  32.5,  33.5,  34.5,  35.5,  36.5,  37.5,  38.5,  39.5,\n        40.5,  41.5,  42.5,  43.5,  44.5,  45.5,  46.5,  47.5,  48.5,  49.5,\n        50.5,  51.5,  52.5,  53.5,  54.5,  55.5,  56.5,  57.5,  58.5,  59.5,\n        60.5,  61.5,  62.5,  63.5,  64.5,  65.5,  66.5,  67.5,  68.5,  69.5,\n        70.5,  71.5,  72.5,  73.5,  74.5,  75.5,  76.5,  77.5,  78.5,  79.5,\n        80.5,  81.5,  82.5,  83.5,  84.5,  85.5,  86.5,  87.5,  88.5,  89.5])</pre></li><li>lon(lon)float640.625 1.875 3.125 ... 358.1 359.4long_name :longitudeunits :degrees_eastaxis :Xbounds :lon_bndsstandard_name :longitudecell_methods :time: point<pre>array([  0.625,   1.875,   3.125, ..., 356.875, 358.125, 359.375])</pre></li><li>time(time)object0001-01-16 12:00:00 ... 0150-12-...long_name :timeaxis :Tcalendar_type :noleapbounds :time_bndsstandard_name :timedescription :Temporal mean<pre>array([cftime.DatetimeNoLeap(1, 1, 16, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(1, 2, 15, 0, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(1, 3, 16, 12, 0, 0, 0, has_year_zero=True), ...,\n       cftime.DatetimeNoLeap(150, 10, 16, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(150, 11, 16, 0, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeNoLeap(150, 12, 16, 12, 0, 0, 0, has_year_zero=True)],\n      dtype=object)</pre></li><li>height()float642.0long_name :heightunits :mcell_methods :time: pointaxis :Zpositive :upstandard_name :heightdescription :~2 m standard surface air temperature and surface humidity  height<pre>array(2.)</pre></li></ul></li><li>Data variables: (11)<ul><li>hfls(time, lat, lon)float32dask.array&lt;chunksize=(1, 180, 288), meta=np.ndarray&gt;long_name :Surface Upward Latent Heat Fluxunits :W m-2cell_methods :area: time: meancell_measures :area: areacellacomment :Lv*evapstandard_name :surface_upward_latent_heat_fluxinterp_method :conserve_order2original_name :hfls  Array   Chunk   Bytes   355.96 MiB   202.50 kiB   Shape   (1800, 180, 288)   (1, 180, 288)   Dask graph   1800 chunks in 5 graph layers   Data type   float32 numpy.ndarray  288 180 1800 </li><li>lat_bnds(time, lat, bnds)float64dask.array&lt;chunksize=(1200, 180, 2), meta=np.ndarray&gt;long_name :latitude boundsunits :degrees_northaxis :Y  Array   Chunk   Bytes   4.94 MiB   3.30 MiB   Shape   (1800, 180, 2)   (1200, 180, 2)   Dask graph   2 chunks in 52 graph layers   Data type   float64 numpy.ndarray  2 180 1800 </li><li>lon_bnds(time, lon, bnds)float64dask.array&lt;chunksize=(1200, 288, 2), meta=np.ndarray&gt;long_name :longitude boundsunits :degrees_eastaxis :X  Array   Chunk   Bytes   7.91 MiB   5.27 MiB   Shape   (1800, 288, 2)   (1200, 288, 2)   Dask graph   2 chunks in 52 graph layers   Data type   float64 numpy.ndarray  2 288 1800 </li><li>time_bnds(time, bnds)objectdask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;long_name :time axis boundaries  Array   Chunk   Bytes   28.12 kiB   16 B   Shape   (1800, 2)   (1, 2)   Dask graph   1800 chunks in 42 graph layers   Data type   object numpy.ndarray  2 1800 </li><li>hfss(time, lat, lon)float32dask.array&lt;chunksize=(1, 180, 288), meta=np.ndarray&gt;long_name :Surface Upward Sensible Heat Fluxunits :W m-2cell_methods :area: time: meancell_measures :area: areacellastandard_name :surface_upward_sensible_heat_fluxinterp_method :conserve_order2original_name :hfss  Array   Chunk   Bytes   355.96 MiB   202.50 kiB   Shape   (1800, 180, 288)   (1, 180, 288)   Dask graph   1800 chunks in 5 graph layers   Data type   float32 numpy.ndarray  288 180 1800 </li><li>huss(time, lat, lon)float32dask.array&lt;chunksize=(1, 180, 288), meta=np.ndarray&gt;long_name :Near-Surface Specific Humidityunits :1cell_methods :area: time: meancell_measures :area: areacellastandard_name :specific_humidityinterp_method :conserve_order2original_name :huss  Array   Chunk   Bytes   355.96 MiB   202.50 kiB   Shape   (1800, 180, 288)   (1, 180, 288)   Dask graph   1800 chunks in 5 graph layers   Data type   float32 numpy.ndarray  288 180 1800 </li><li>rlds(time, lat, lon)float32dask.array&lt;chunksize=(1, 180, 288), meta=np.ndarray&gt;long_name :Surface Downwelling Longwave Radiationunits :W m-2cell_methods :area: time: meancell_measures :area: areacellastandard_name :surface_downwelling_longwave_flux_in_airinterp_method :conserve_order2original_name :rlds  Array   Chunk   Bytes   355.96 MiB   202.50 kiB   Shape   (1800, 180, 288)   (1, 180, 288)   Dask graph   1800 chunks in 5 graph layers   Data type   float32 numpy.ndarray  288 180 1800 </li><li>rsds(time, lat, lon)float32dask.array&lt;chunksize=(1, 180, 288), meta=np.ndarray&gt;long_name :Surface Downwelling Shortwave Radiationunits :W m-2cell_methods :area: time: meancell_measures :area: areacellastandard_name :surface_downwelling_shortwave_flux_in_airinterp_method :conserve_order1original_name :rsds  Array   Chunk   Bytes   355.96 MiB   202.50 kiB   Shape   (1800, 180, 288)   (1, 180, 288)   Dask graph   1800 chunks in 5 graph layers   Data type   float32 numpy.ndarray  288 180 1800 </li><li>tas(time, lat, lon)float32dask.array&lt;chunksize=(1, 180, 288), meta=np.ndarray&gt;long_name :Near-Surface Air Temperatureunits :Kcell_methods :area: time: meancell_measures :area: areacellastandard_name :air_temperatureinterp_method :conserve_order2original_name :tas  Array   Chunk   Bytes   355.96 MiB   202.50 kiB   Shape   (1800, 180, 288)   (1, 180, 288)   Dask graph   1800 chunks in 5 graph layers   Data type   float32 numpy.ndarray  288 180 1800 </li><li>rlus(time, lat, lon)float32dask.array&lt;chunksize=(1, 180, 288), meta=np.ndarray&gt;long_name :Surface Upwelling Longwave Radiationunits :W m-2cell_methods :area: time: meancell_measures :area: areacellastandard_name :surface_upwelling_longwave_flux_in_airinterp_method :conserve_order2original_name :rlus  Array   Chunk   Bytes   355.96 MiB   202.50 kiB   Shape   (1800, 180, 288)   (1, 180, 288)   Dask graph   1800 chunks in 5 graph layers   Data type   float32 numpy.ndarray  288 180 1800 </li><li>rsus(time, lat, lon)float32dask.array&lt;chunksize=(1, 180, 288), meta=np.ndarray&gt;long_name :Surface Upwelling Shortwave Radiationunits :W m-2cell_methods :area: time: meancell_measures :area: areacellastandard_name :surface_upwelling_shortwave_flux_in_airinterp_method :conserve_order1original_name :rsus  Array   Chunk   Bytes   355.96 MiB   202.50 kiB   Shape   (1800, 180, 288)   (1, 180, 288)   Dask graph   1800 chunks in 5 graph layers   Data type   float32 numpy.ndarray  288 180 1800 </li></ul></li><li>Indexes: (4)<ul><li>bndsPandasIndex<pre>PandasIndex(Index([1.0, 2.0], dtype='float64', name='bnds'))</pre></li><li>latPandasIndex<pre>PandasIndex(Index([-89.5, -88.5, -87.5, -86.5, -85.5, -84.5, -83.5, -82.5, -81.5, -80.5,\n       ...\n        80.5,  81.5,  82.5,  83.5,  84.5,  85.5,  86.5,  87.5,  88.5,  89.5],\n      dtype='float64', name='lat', length=180))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([             0.625, 1.8749999999999998,              3.125,\n                    4.375,              5.625,              6.875,\n                    8.125,              9.375,             10.625,\n                   11.875,\n       ...\n                  348.125,            349.375,            350.625,\n                  351.875,            353.125,            354.375,\n                  355.625,            356.875,            358.125,\n                  359.375],\n      dtype='float64', name='lon', length=288))</pre></li><li>timePandasIndex<pre>PandasIndex(CFTimeIndex([0001-01-16 12:00:00, 0001-02-15 00:00:00, 0001-03-16 12:00:00,\n             0001-04-16 00:00:00, 0001-05-16 12:00:00, 0001-06-16 00:00:00,\n             0001-07-16 12:00:00, 0001-08-16 12:00:00, 0001-09-16 00:00:00,\n             0001-10-16 12:00:00,\n             ...\n             0150-03-16 12:00:00, 0150-04-16 00:00:00, 0150-05-16 12:00:00,\n             0150-06-16 00:00:00, 0150-07-16 12:00:00, 0150-08-16 12:00:00,\n             0150-09-16 00:00:00, 0150-10-16 12:00:00, 0150-11-16 00:00:00,\n             0150-12-16 12:00:00],\n            dtype='object', length=1800, calendar='noleap', freq=None))</pre></li></ul></li><li>Attributes: (46)external_variables :areacellahistory :File was processed by fremetar (GFDL analog of CMOR). TripleID: [exper_id_qVYgCix5ML,realiz_id_c2oVUWxeGl,run_id_aUlk27asiH]table_id :Amonactivity_id :CMIPbranch_method :standardbranch_time_in_child :0.0comment :&lt;null ref&gt;contact :gfdl.climate.model.info@noaa.govConventions :CF-1.7 CMIP-6.0 UGRID-1.0creation_date :2019-03-09T21:37:53Zdata_specs_version :01.00.27experiment :abrupt quadrupling of CO2experiment_id :abrupt-4xCO2forcing_index :1frequency :monCfurther_info_url :https://furtherinfo.es-doc.org/CMIP6.NOAA-GFDL.GFDL-CM4.abrupt-4xCO2.none.r1i1p1f1grid :atmos data regridded from Cubed-sphere (c96) to 180,288; interpolation method: conserve_order2grid_label :gr1initialization_index :1institution :National Oceanic and Atmospheric Administration, Geophysical Fluid Dynamics Laboratory, Princeton, NJ 08540, USAinstitution_id :NOAA-GFDLlicense :CMIP6 model data produced by NOAA-GFDL is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License (https://creativecommons.org/licenses/). Consult https://pcmdi.llnl.gov/CMIP6/TermsOfUse for terms of use governing CMIP6 output, including citation requirements and proper acknowledgment. Further information about this data, including some limitations, can be found via the further_info_url (recorded as a global attribute in this file). The data producers and data providers make no warranty, either express or implied, including, but not limited to, warranties of merchantability and fitness for a particular purpose. All liabilities arising from the supply of the information (including any liability arising in negligence) are excluded to the fullest extent permitted by law.mip_era :CMIP6nominal_resolution :100 kmparent_activity_id :CMIPparent_experiment_id :piControlparent_mip_era :CMIP6parent_source_id :GFDL-CM4parent_variant_label :r1i1p1f1physics_index :1product :model-outputrealization_index :1realm :atmossource :GFDL-CM4 (2018):  aerosol: interactive atmos: GFDL-AM4.0.1 (Cubed-sphere (c96) - 1 degree nominal horizontal resolution; 360 x 180 longitude/latitude; 33 levels; top level 1 hPa) atmosChem: fast chemistry, aerosol only land: GFDL-LM4.0.1 (1 degree nominal horizontal resolution; 360 x 180 longitude/latitude; 20 levels; bot level 10m); land:Veg:unnamed (dynamic vegetation, dynamic land use); land:Hydro:unnamed (soil water and ice, multi-layer snow, rivers and lakes) landIce: GFDL-LM4.0.1 ocean: GFDL-OM4p25 (GFDL-MOM6, tripolar - nominal 0.25 deg; 1440 x 1080 longitude/latitude; 75 levels; top grid cell 0-2 m) ocnBgchem: GFDL-BLINGv2 seaIce: GFDL-SIM4p25 (GFDL-SIS2.0, tripolar - nominal 0.25 deg; 1440 x 1080 longitude/latitude; 5 layers; 5 thickness categories) (GFDL ID: 2019_0193)source_id :GFDL-CM4source_type :AOGCMsub_experiment :nonesub_experiment_id :nonetitle :NOAA GFDL GFDL-CM4 model output prepared for CMIP6 abrupt quadrupling of CO2tracking_id :hdl:21.14100/336b54c7-e1ae-408d-a1d8-1c53a0eebabevariable_id :hflsvariant_info :N/Areferences :see further_info_url attributevariant_label :r1i1p1f1branch_time_in_parent :36500.0parent_time_units :days since 0001-1-1</li></ul>"},{"location":"data/cmip/CEDAESGF_Wrapper/#cedaesgf","title":"CEDA/ESGF\u00b6","text":""},{"location":"data/cmip/CEDAESGF_Wrapper/#cedaesgf-an-easy-cmip6-wrapper","title":"CEDA+ESGF - an easy CMIP6 wrapper\u00b6","text":""},{"location":"data/cmip/CEDAESGF_Wrapper/#after-the-lab-meeting","title":"After the lab meeting:\u00b6","text":"<ul> <li>We discussed everyones different ways of getting at CMIP6 data</li> <li>The two prevailing camps are CEDA and intake ESGF</li> <li>CEDA is incomplete however does contain useful datasets</li> <li>intake ESGF methods are easy to use but relies on unlimited storage capacity</li> <li>Bridging the gap between these two/ identifiying where CEDA resouces start and end may lead to better data management practices</li> </ul>"},{"location":"data/cmip/CEDAESGF_Wrapper/#goals","title":"Goals:\u00b6","text":"<ul> <li>ESGF Web browser like search interface (auto complete?)</li> <li>CEDA first comprehensive search</li> <li>Indentification of missing variables</li> <li>Estimation of data download requirements</li> <li>Easy download to cache/ repeatable access</li> <li>Storage statistics</li> </ul>"},{"location":"data/cmip/CEDAESGF_Wrapper/#potential-issues-planned-fixes","title":"Potential Issues/ Planned fixes:\u00b6","text":"<ul> <li>Different grids (gn, gr, gr1...) - could mess with things</li> <li>No way of combining timelines at the moment - ie historical into SSPXXX</li> <li>No way of grabbing land masks - relies on importing external ones</li> <li>Would like a storage demand estimator/ way of managing storage</li> <li>View of pivot table to visualize data avaliability</li> </ul> <p>Note that this notebook uses functions from the ceda_esgf section of the <code>climdyn_tools</code> package.</p>"},{"location":"data/era5/","title":"ERA5","text":""},{"location":"hpc_basics/","title":"High Performance Computing","text":"<p>The lab frequently uses supercomputers for data analysis and performing simulations.</p> <p>A familiarity with shell scripting and Slurm  would be useful in general for using these. </p> <p>There is also specific information on getting started on the three main HPCs we use in the lab:</p> <ul> <li>Hypatia: University of St Andrews supercomputer, useful for less intensive tasks such  as running Isca simulations.</li> <li>Archer2: UK national supercomputer, useful for more intensive tasks such as  running CESM simulations.</li> <li>JASMIN: UKs environmental data analysis platform, useful for data analysis with large datasets such as CESM or ERA5.</li> </ul>"},{"location":"hpc_basics/archer2/","title":"Archer2","text":"<p>Archer2 is the UK supercomputer, we mainly use it for  running CESM2 simulations.</p> <p>To use it, you need to setup an ARCHER2 account and  then connect using ssh.</p>"},{"location":"hpc_basics/archer2/#resources","title":"Resources","text":"<ul> <li>The Archer2 documentation is good in general.</li> <li>SAFE: Used to manage account and monitor usage.</li> </ul>"},{"location":"hpc_basics/archer2/#getting-started","title":"Getting started","text":"<p>To use Archer2, one needs to obtain a SAFE account, followed by an ARCHER2 login account.</p> Login Tip <p>After the first login, to make it easier, I have config file on my  local machine at <code>/Users/user/.ssh/config</code> with the following (the same file can have multiple hosts for different supercomputers): <pre><code>Host archer2\nHostName login.archer2.ac.uk\nUser jamd\nIdentityFile ~/.ssh/id_rsa_archer\n</code></pre></p> <p>This means I can just type <code>ssh archer2</code> to login directly.</p>"},{"location":"hpc_basics/archer2/#file-transfer","title":"File Transfer","text":"<p>I find it easiest to transfer files with globus. Archer2 gives good instructions on how to do this.</p>"},{"location":"hpc_basics/hypatia/","title":"Hypatia","text":"<p>The following gives some instructions on how to get an account on  the University of St Andrews HPC called hypatia, and then login (based on Mac OS).</p>"},{"location":"hpc_basics/hypatia/#resources","title":"Resources","text":"<ul> <li> <p>There is a website for hypatia. </p> </li> <li> <p>There is also a recorded  lecture for the previous incarnation (kennedy).</p> </li> <li> <p>Corresponding  lecture notes.</p> </li> </ul> <p>These may be useful, especially if using an operating system other than Mac.</p>"},{"location":"hpc_basics/hypatia/#getting-an-account","title":"Getting an account","text":"<ul> <li>Register for an account \ud83d\udd17.</li> <li>Create a public ssh key.<ul> <li>Follow these instructions.</li> <li>I would not enter a password and leave the folder as the default.</li> <li>You should have created a private key (id_rsa) and a public key (id_rsa.pub). For me, these were saved to  /Users/user/.ssh/.</li> </ul> </li> <li>Email the public key to Herbert (herbert.fruchtl@st-andrews.ac.uk) so he can make you an account.<ul> <li>He should then email you a username and password.</li> </ul> </li> </ul>"},{"location":"hpc_basics/hypatia/#login","title":"Login","text":"<p>Username</p> <p>Your hypatia username will probably be the same as your normal St Andrews one. In the following, I have left  my username, jamd1, so just replace this wherever it appears with your username.</p> <ul> <li>Unless you are in a University building or a student hall, you need to connect to the St Andrews VPN.</li> <li>To login, run the following in terminal (the IP address may be replaced by a host name in the future). <pre><code>ssh jamd1@138.251.14.231\n</code></pre></li> <li>You may get the following message: <pre><code>The authenticity of host can't be established.\nED25519 key fingerprint is ...\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\n</code></pre> If so, just say <code>yes</code></li> <li>Then it will call for the password:       <pre><code>jamd1@138.251.14.231's password:\n</code></pre></li> </ul> If logged in successfully, the following should appear <pre><code>  H H Y Y PPP   A   TTTTT I   A\n   HHH  Y  PPP  A A    T   I  A A\n   H H  Y  P   A   A   T   I A   A\n\nCurrent Queue setup:\n  small-short: 1 node, 24 cores,  2 days\n  small-long:  1 node, 24 cores, 30 days\n  large-short: multiple nodes, more cores (up to 168 per node),  2 days\n  large-long:  multiple nodes, more cores (up to 168 per node), 30 days\n  gpu.L40S: \\\n  gpu.A100:  ) GPU nodes: 4 days. Request GPUs separately (see example job)\n  gpu.A30:  /             \n\nExample batch jobs in\n  /software/examples\n\nMore information (including introductory lectures) at\n  https://www.st-andrews.ac.uk/high-performance-computing/\n\n\n             Welcome to the St Andrews HPC Resource\n\n   H H Y Y PPP   A   TTTTT I   A\n   HHH  Y  PPP  A A    T   I  A A\n   H H  Y  P   A   A   T   I A   A\n\nCurrent Queue setup:\n  small-short: 1 node, 24 cores,  2 days\n  small-long:  1 node, 24 cores, 30 days\n  large-short: multiple nodes, more cores (up to 168 per node),  2 days\n  large-long:  multiple nodes, more cores (up to 168 per node), 30 days\n  gpu.L40S: \\\n  gpu.A100:  ) GPU nodes: 4 days. Request GPUs separately (see example job)\n  gpu.A30:  /             \n\nExample batch jobs in\n  /software/examples\n\nMore information (including introductory lectures) at\n  https://www.st-andrews.ac.uk/high-performance-computing/\n\n\ncd null\nSLURM: Your account, jamd1, is ready to submit Slurm jobs.\nInfo: Loaded slurm into the modular environment.\nInfo: Loaded lmod into the modular environment.\n</code></pre>"},{"location":"hpc_basics/hypatia/#change-password","title":"Change Password","text":"<p>On first login, the password should be changed using the command <code>passwd</code>: <pre><code>[jamd1@login01(standrews) ~]$ passwd\nChanging password for user jamd1.\nCurrent Password: \nNew password: \nRetype new password: \npasswd: all authentication tokens updated successfully.\n[jamd1@login01(standrews) ~]$ \n</code></pre></p>"},{"location":"hpc_basics/hypatia/#file-transfer","title":"File Transfer","text":"Using Terminal <p>You can exchange files between your local computer and hypatia using terminal: <pre><code>sftp jamd1@138.251.14.231\njamd1@138.251.14.231's password: \nConnected to 138.251.14.231.\nsftp&gt; \n</code></pre></p> <ul> <li>Then use <code>cd &lt;dir&gt;</code> to change directory</li> <li><code>put &lt;file&gt;</code> to move file from local computer to hypatia.</li> <li><code>get &lt;file&gt;</code> to move file from hypatia to local computer.</li> </ul> <p>I find it easiest to exchange files using FileZilla which has a graphical interface.</p> <ul> <li>Download it from this website</li> <li> <p>Next, you need to specify the private key which was created earlier.</p> <ul> <li>In FileZilla, in the top bar, click Edit and then Settings.</li> <li>Then click Connection/SFTP and then Add key file.</li> <li>Select the private key created earlier i.e. it should be called id_rsa.  </li> <li>Then press OK </li> <li>This comes from the On a Mac section on the wiki. </li> </ul> </li> <li> <p>Now, in the top bar, click File and then Site Manager</p> <ul> <li>Create a New Site for hypatia.</li> <li>Protocol: SFTP</li> <li>Host: 138.251.14.231</li> <li>Username and password is the same as used to login.  </li> <li>Then click Connect and it should hopefully work with a screen like the following appearing.  </li> </ul> </li> <li>You should now be able to drag and drop files from the local computer (left) to hypatia (right) and create new  directories etc.</li> </ul>"},{"location":"hpc_basics/hypatia/#conda","title":"CONDA","text":""},{"location":"hpc_basics/hypatia/#installation","title":"Installation","text":"<p>To install CONDA, login and then run <code>install-conda</code> in terminal. This should then produce some files in the location /gpfs01/software/conda/jamd1/conda:  </p>"},{"location":"hpc_basics/hypatia/#create-environment","title":"Create Environment","text":"<p>To create a <code>python 3.9</code> conda environment called <code>test_env</code> run: <pre><code>conda create -n test_env python=3.9\n</code></pre> Then to activate it, run: <pre><code>conda activate test_env\n</code></pre> Terminal should then look something like this: <pre><code>(test_env) [jamd1@login01(standrews) ~]$\n</code></pre></p>"},{"location":"hpc_basics/hypatia/#error-wrong-python-version","title":"Error - Wrong Python Version","text":"Error <p>If you now run <code>python -V</code> to check the python version, it will print <code>Python 2.7.5</code> even though the conda  environment is <code>python 3.9</code>.</p> <p>This is because it is using the wrong python. If you run <code>which python</code>, it will print <code>/usr/bin/python</code> which  has nothing to do with the <code>test_env</code> CONDA environment.</p> <p>The problem is that the python installed using CONDA does not have execution permissions, so it reverts to  a python version which does. To give execution permissions, you can run the following line (The <code>$USER</code> will  automatically be your username so you don't need to change it): <pre><code>chmod u+x /gpfs01/software/conda/$USER/conda/envs/*/bin/*\n</code></pre></p> <p>If you now run <code>conda deactivate</code> and then <code>conda activate test_env</code> to log out and then back into the CONDA  environment, <code>python -V</code> should now print <code>Python 3.9.13</code> and <code>which python</code> should print  <code>/gpfs01/software/conda/jamd1/conda/envs/test_env/bin/python</code>.</p> <p>In general, whenever you hit a <code>Permission Denied</code> error when using a CONDA environment, I would run  <code>chmod u+x /gpfs01/software/conda/$USER/conda/envs/*/bin/*</code> as a first attempt at fixing it.</p>"},{"location":"hpc_basics/jasmin/","title":"JASMIN","text":"<p>JASMIN is the UKs environmental data analysis platform. The lab stores its data on JASMIN at <code>/gws/nopw/j04/global_ex</code>.</p>"},{"location":"hpc_basics/jasmin/#resources","title":"Resources","text":"<ul> <li>Official documentation \ud83d\udd17</li> <li>Jupyter notebook service \ud83d\udd17</li> <li>List of Scientific Servers \ud83d\udd17</li> <li>Slurm Queue information \ud83d\udd17</li> <li>Using Jupyter notebooks on JASMIN without their own service \ud83d\udd17:  This has the advantage that you can stay within your chosen IDE,  use more memory, and keep working when the notebook service is down.</li> </ul>"},{"location":"hpc_basics/jasmin/#getting-started","title":"Getting started","text":"<p>The official documentation is very good for setting up an account and logging in for the first time.</p> Login Tip <p>To make the login easier, I have config file on my local machine at <code>/Users/user/.ssh/config</code> (the same file can have multiple hosts for different supercomputers): <pre><code>Host jasmin\nHostName sci-ph-01.jasmin.ac.uk\nUser jamd1\nIdentityFile ~/.ssh/id_rsa_jasmin\nProxyJump login-02.jasmin.ac.uk\n</code></pre></p> <p>This means I can just type <code>ssh jasmin</code> to log in directly to the sci server in one go.</p>"},{"location":"hpc_basics/jasmin/#file-transfer","title":"File Transfer","text":"<p>As with Archer2, I find it easiest to use globus to transfer files to/from JASMIN.  Instructions for JASMIN are given here.</p>"},{"location":"hpc_basics/shell_scripting/","title":"Shell Scripting","text":"<p>A shell script (<code>.sh</code>) can be used to submit a sequence of commands to terminal.</p>"},{"location":"hpc_basics/shell_scripting/#video","title":"Video","text":"<p>The following video outlines the basics of shell scripting. </p> <p>08:32: The first line of a shell script must be the shebang line, telling system which shell interpreter to use. The  default shell interpreter can be found  by running <code>echo $SHELL</code> in terminal. This will return something like <code>/bin/bash</code>.</p> <p>10:36: Before running a shell script, you may need to give it execution permissions (this is similar to the CONDA python issue with hypatia).</p> <p>13:00: A shell script can accept any number of parameters.</p>"},{"location":"hpc_basics/shell_scripting/#example-script","title":"Example Script","text":"<p>Let's create an example script which accepts 3 parameters and prints them, as well as the file name, called example.sh (execution permission can be given through <code>chmod u+x example.sh</code>):</p> <pre><code>#!/bin/bash\n\n#This program accepts 3 parameters and prints them\necho File Name: $0\necho Param 1  : $1\necho Param 2  : $2\necho Param 3  : $3\n</code></pre> <p>Running this script through <code>./example.sh p1 p2 p3</code> prints: <pre><code>Exec Name: ./example.sh\nParam 1  : p1\nParam 2  : p2\nParam 3  : p3\n</code></pre></p>"},{"location":"hpc_basics/shell_scripting/#useful-commands","title":"Useful Commands","text":"<ul> <li><code>echo</code> - Use to display the value of a variable e.g. <code>echo $SHELL</code> will display the value of <code>SHELL</code>. </li> <li><code>export var=5</code> - this will mean that an environmental variable called <code>var</code> will be created and given the value <code>5</code>. It can then be accessed in terminal e.g. it can be printed through <code>echo $var</code>. </li> <li><code>printenv</code> - This displays all the environmental variables e.g. if the parameter <code>SHELL</code> shows up in the list,  then its value can be accessed through <code>$SHELL</code> </li> <li><code>source shell_script.sh</code> - This will make all variables defined through <code>export</code> in the shell script shell_script.sh  be available in the current terminal. </li> </ul>"},{"location":"hpc_basics/slurm/","title":"Slurm","text":"<p>To run a job on an HPC e.g. hypatia, it must be submitted to a Slurm queue. To do this, you basically modify a .sh script with some headers indicating which node to submit the job to.</p>"},{"location":"hpc_basics/slurm/#useful-commands","title":"Useful commands \ud83d\udd17","text":""},{"location":"hpc_basics/slurm/#monitoring-jobs","title":"Monitoring jobs","text":"<p>These can be used to monitor the current state of the queues and your jobs.</p> <ul> <li><code>squeue -u jamd1</code> - shows all jobs submitted by the username <code>jamd1</code> </li> <li><code>scancel 158329</code> - cancels job with id <code>158329</code>. Job ID is shown with the above command or <code>squeue</code> </li> <li><code>scancel -u jamd1</code> - cancels all jobs by the username <code>jamd1</code> </li> <li><code>squeue -t RUNNING</code> - shows all jobs that are currently running  </li> <li><code>squeue -p debug</code> - shows all jobs queued and running for the partition called <code>debug</code> </li> </ul>"},{"location":"hpc_basics/slurm/#sbatch","title":"SBATCH","text":"<p>The slurm commands must be added to top of .sh script, just below the shebang line. Some key commands are listed below:</p> <ul> <li><code>#SBATCH --job-name=</code> - name of job e.g. <code>test</code> </li> <li><code>#SBATCH --output=</code> - file where things printed to console are saved e.g. <code>output.txt</code> </li> <li><code>#SBATCH --error=</code> - file where any errors are saved e.g. <code>error.txt</code> </li> <li><code>#SBATCH --time=</code> - maximum walltime for the job e.g. <code>01:00:00</code> for 1 hour or <code>01:00</code> for 1 minute.  </li> <li><code>#SBATCH --nodes=</code> - number of nodes used e.g. <code>1</code> </li> <li><code>#SBATCH --ntasks-per-node=</code> - Number of processors per node e.g. <code>16</code>.</li> <li><code>#SBATCH --partition=</code> - the queue to submit a job to. </li> </ul> <p>Options for hypatia can be found here.</p> <ul> <li><code>#SBATCH --mail-type=</code> - indicates when you would like to receive email notifications, <code>NONE</code>, <code>FAIL</code>, <code>ALL</code> or <code>END</code>.  </li> <li><code>#SBATCH --mail-user=</code> - email address e.g. <code>jamd1@st-andrews.ac.uk</code> </li> <li><code>#SBATCH --dependency=afterok:158329</code> - defers start of a submitted job until job with id <code>158329</code>  has finished successfully \ud83d\udd17</li> </ul>"},{"location":"hpc_basics/slurm/#submitting-a-job","title":"Submitting a job","text":"<p>To submit a script which prints 'hello world' to the <code>debug</code> queue with 16 tasks per node with the output saved to the file <code>example_output.txt</code>, the <code>example.sh</code> script would look like this:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=example\n#SBATCH --output=\"example_output.txt\"\n#SBATCH --error=\"example_error.txt\"\n#SBATCH --time=01:00 # maximum walltime for the job\n#SBATCH --nodes=1 # specify number of nodes\n#SBATCH --ntasks-per-node=16 # specify number of processors per node\n#SBATCH --mail-type=END # send email at job completion\n#SBATCH --mail-user=$USER@st-andrews.ac.uk # email address\n#SBATCH --partition=debug # queue to run on\n\necho hello world\n</code></pre> <p>To submit this job, you then need to login to hypatia and  transfer <code>example.sh</code> from your local computer to hypatia.</p> <p>Then make sure your current directory contains the <code>example.sh</code> file and run <code>sbatch example.sh</code>.  This should produce the files <code>example_output.txt</code> and <code>example_error.txt</code> in your current directory when it is finished running (use <code>squeue -p debug</code> to see its place in the queue).</p>"},{"location":"hpc_basics/slurm/#multiple-jobs-with-environmental-variables","title":"Multiple jobs with environmental variables","text":"<p>The submission of multiple instances of the same job but using different numbers of cores can be achieved  through an overarching python script which then runs the shell (<code>.sh</code>) script with different input  parameters.</p> <p>For example, if the three files below are all in the current directory then running <code>python example_run.py</code> will send two jobs to the <code>debug</code> queue, both with job name <code>example</code> but one on 8 cores per node and one on 16. </p> <p>Dependency: The job on 16 cores will also wait for the 8 core job to finish. To do this,  we need the <code>example_depend.sh</code> script including the dependency slurm command.</p> Using CONDA <p>Because this uses python, you probably want to run it with a relatively modern version of python. To do this, activate a CONDA environment before running <code>python example_run.py</code>.</p> example_run.pyexample.shexample_depend.shexample_print.py <pre><code>import subprocess\njob_name = 'example'\nshell_script = 'example.sh'\nn_nodes = 1\noutput_text = 'hello world'\ndependent_job_id = ''           # first job has no dependency so leave blank\nfor n_cores in [8, 16]:\n    submit_string = (f'bash {shell_script if dependent_job_id == '' else shell_script.replace('.sh', '_depend.sh')} '\n                     f'{job_name} {n_nodes} {n_cores} {output_text} {dependent_job_id}')\n    output = subprocess.check_output(submit_string, shell=True).decode(\"utf-8\").strip()  # get job just submitted info\n    dependent_job_id = output.split()[-1]  # Save this job id (last word) for the next submission\n</code></pre> <pre><code>#!/bin/bash\nsbatch &lt;&lt;EOT\n#!/bin/bash\n#SBATCH --job-name=$1\n#SBATCH --output=\"outFile\"$3\".txt\"\n#SBATCH --error=\"errFile\"$3\".txt\"\n#SBATCH --time=01:00 # maximum walltime for the job\n#SBATCH --nodes=$2 # specify number of nodes\n#SBATCH --ntasks-per-node=$3 # specify number of processors per node\n#SBATCH --mail-type=END # send email at job completion\n#SBATCH --mail-user=$USER@st-andrews.ac.uk # email address\n#SBATCH --partition=debug # queue to run on\n\nexport OUTPUT_TEXT=$4   # export so can be used by python script\npython example_print.py\nexit 0\nEOT\n</code></pre> <pre><code>#!/bin/bash\nsbatch &lt;&lt;EOT\n#!/bin/bash\n#SBATCH --job-name=$1\n#SBATCH --output=\"outFile\"$3\".txt\"\n#SBATCH --error=\"errFile\"$3\".txt\"\n#SBATCH --time=01:00 # maximum walltime for the job\n#SBATCH --nodes=$2 # specify number of nodes\n#SBATCH --ntasks-per-node=$3 # specify number of processors per node\n#SBATCH --mail-type=END # send email at job completion\n#SBATCH --mail-user=$USER@st-andrews.ac.uk # email address\n#SBATCH --partition=debug # queue to run on\n#SBATCH --dependency=afterok:$5           # Only run after job submitted\n\nexport OUTPUT_TEXT=$4   # export so can be used by python script\npython example_print.py\nexit 0\nEOT\n</code></pre> <pre><code>import os\nprint(f'Job Name: {os.environ['SLURM_JOB_NAME']}')\nprint(f'Number of nodes: {int(os.environ['SLURM_NNODES'])}')\nprint(f'Number of tasks per node: {int(os.environ['SLURM_NTASKS_PER_NODE'])}')\nprint(f'Output text: {os.environ['OUTPUT_TEXT']}')\n</code></pre> Wrapper in <code>.sh</code> script <p>The <code>example.sh</code> script is slightly different when it is called from a python script. It needs a wrapper which is what the <code>EOT</code> stuff is.</p> <p>When both jobs have been completed, this will then produce 4 files in the current directory,  <code>outFile8.txt</code>, <code>outFile16.txt</code>, <code>errFile8.txt</code> and <code>errFile16.txt</code>.</p> outFile8.txtoutFile16.txt <pre><code>Job Name: example\nNumber of nodes: 1\nNumber of tasks per node: 8\nOutput text: hello world\n</code></pre> <pre><code>Job Name: example\nNumber of nodes: 1\nNumber of tasks per node: 16\nOutput text: hello world\n</code></pre> <p>This example shows that the <code>#SBATCH</code> commands in the <code>example.sh</code> script produces  environmental variables with a <code>SLURM</code> prefix which can then be accessed. These variables cannot be accessed from the <code>example.sh</code> script itself though e.g. <code>echo $SLURM_SLURM_JOB_NAME</code> would not print anything if it was included in <code>example.sh</code>.</p>"},{"location":"hpc_basics/slurm/#debugging","title":"Debugging","text":"<p>It can be annoying if you are running a small job, to submit it to a <code>SLURM</code> queue and then wait for  it to get to the front.</p> <p>As an alternative, you can just login to hypatia and run the <code>example_print.py</code> script (after activating the relevant CONDA environment). To do this, you first need to set the environmental variables that the script uses by running <code>source example_params.sh</code> where the <code>example_params.sh</code> script is given below:</p> <pre><code>export SLURM_JOB_NAME=example\nexport SLURM_NTASKS_PER_NODE=8\nexport OUTPUT_TEXT=hello world\n</code></pre> <p>Doing this, you are limited to one node and 8 tasks per node, so you may get problems  with Isca if you try to run a simulation with more than 8 tasks per node.</p>"},{"location":"software/","title":"Software","text":"<p>Information on software that is useful for the lab.</p>"},{"location":"software/#integrated-development-environment","title":"Integrated Development Environment","text":"<p>In the lab, we do a lot of data analysis using Python,  both locally and on JASMIN. To streamline this process, it is useful to have an IDE where you can do everything. Some options are listed below:</p> <ul> <li>VS Code: Probably the most popular choice</li> <li>Pycharm: Pretty good alternative, they have a free student pack as well.</li> <li>Cursor: Like VS Code but more focused on AI. Students get one year free. Not sure if it has support for Jupyter notebooks  yet, though.</li> </ul>"},{"location":"software/#python","title":"Python","text":"<p>Here we provide some useful libraries to use for Python.</p>"},{"location":"software/#climdyn_tools","title":"<code>climdyn_tools</code>","text":"<p>This is a package containing code created by members of the lab.</p>"},{"location":"software/#xarray","title":"<code>xarray</code>","text":"<p>Most of our data analysis is performed by manipulating xarray <code>Datasets</code> in Jupyter notebooks.</p>"},{"location":"software/#organisation","title":"Organisation","text":"<p>Here are some useful applications for writing as well as keeping track of research, literature.</p>"},{"location":"software/#microsoft-teams","title":"Microsoft Teams","text":"<p>This is where we organize group meetings and share interesting results.</p>"},{"location":"software/#zotero","title":"Zotero","text":"<p>Zotero is a free, open-source reference manager that helps you collect, organize, and cite research materials. They have a useful web extension.  We also have a shared lab library to keep track of papers we have discussed as a group.</p>"},{"location":"software/#overleaf","title":"Overleaf","text":"<p>Overleaf is useful for writing up papers.</p>"},{"location":"software/#diary","title":"Diary","text":"<p>It is useful to have a diary to keep notes day to day. </p> <p>I use logseq as it is free, open source, and you write in Markdown which is easy to use. It is also easy to embed equations in LaTeX, as well as images. It is probably easiest to use this as a diary but allows for much more complexity with tags and links.</p> <p>Alternatives to this include Notion and Obsidian. I think all three of these support Zotero integration as well.</p>"}]}